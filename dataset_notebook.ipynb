{"cells":[{"cell_type":"markdown","metadata":{"id":"btuUXLNIlkst"},"source":["## Note\n","\n","Include notes here about notebook. Mention running \"Build Datasets from Colab\""]},{"cell_type":"markdown","metadata":{"id":"xoNd1Rb3zmng"},"source":["# Imports"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60826,"status":"ok","timestamp":1760307500331,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"},"user_tz":300},"id":"odmouzHe6txY","outputId":"38970b0c-4a0f-4d36-8912-3cad71c3170f","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting git+https://github.com/trent-b/iterative-stratification.git\n","  Cloning https://github.com/trent-b/iterative-stratification.git to /tmp/pip-req-build-a4pij46q\n","  Running command git clone --filter=blob:none --quiet https://github.com/trent-b/iterative-stratification.git /tmp/pip-req-build-a4pij46q\n","  Resolved https://github.com/trent-b/iterative-stratification.git to commit c763bd5440e5f03c6447471305897c1046fc4838\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from iterative-stratification==0.1.9) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from iterative-stratification==0.1.9) (1.16.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from iterative-stratification==0.1.9) (1.6.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->iterative-stratification==0.1.9) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->iterative-stratification==0.1.9) (3.6.0)\n","Building wheels for collected packages: iterative-stratification\n","  Building wheel for iterative-stratification (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iterative-stratification: filename=iterative_stratification-0.1.9-py3-none-any.whl size=8444 sha256=5f64993ada93659540e96db8373ec7331ba9d8508b4f56cafa2efccd48e3c851\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-bchp0s8b/wheels/1a/53/b5/f5f9836cdfff718e36f6af0df976157168f38b5ce6bf53a49d\n","Successfully built iterative-stratification\n","Installing collected packages: iterative-stratification\n","Successfully installed iterative-stratification-0.1.9\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.8.0+cu126)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision) (3.0.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.0.2)\n","Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (1.16.2)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.5)\n","Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (11.3.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.10.4)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (25.0)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n","Device:  cpu\n"]}],"source":["run_imports=True\n","Google_Colab=True  # Running on Google Colab?\n","\n","if run_imports:\n","    if Google_Colab==True:\n","        ## Mount Google Drive ##L\n","        from google.colab import drive\n","        drive.mount('/content/drive') # Mount Google Drive\n","\n","        ## Iterstrat\n","        !pip install git+https://github.com/trent-b/iterative-stratification.git\n","        from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n","\n","    ## Python ##\n","    import os\n","    import time\n","    import sys\n","    from pathlib import Path\n","    import re\n","    #from IPython.display import display, clear_output\n","\n","    ## Progress Bar\n","    !pip install tqdm\n","    from tqdm import tqdm\n","\n","    ## Pytorch ##\n","    !pip install torch\n","    import torch\n","    from torch import nn\n","    from torch.utils.data import DataLoader\n","    from torch.utils.data import Dataset\n","\n","    ## Torchvision ##\n","    !pip install torchvision\n","    from torchvision.utils import make_grid\n","    from torchvision import transforms\n","\n","    ## Numpy/MatPlotLib ##\n","    !pip install numpy\n","    import numpy as np\n","    import re\n","\n","    !pip install matplotlib\n","    import matplotlib.pyplot as plt\n","    from matplotlib.colors import Normalize\n","    from matplotlib.pyplot import savefig\n","    import matplotlib.gridspec as gridspec\n","\n","    ## Pandas ##\n","    !pip install pandas\n","    import pandas as pd\n","\n","    ## SciKit #\n","    !pip install scikit-image\n","    from skimage import metrics\n","    from skimage.metrics import structural_similarity\n","    from skimage.transform import radon, iradon\n","    from skimage.transform import iradon\n","    from skimage import morphology\n","    from skimage.morphology import opening, erosion\n","    #from skimage.restoration import denoise_bilateral, denoise_tv_chambolle, denoise_wavelet\n","\n","    ## SciPy ##\n","    #from scipy.stats import moment as compute_moment\n","\n","    ## UpRoot\n","    #import uproot\n","\n","    ## Determine what Hardware we have ##\n","    device = ('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    print('Device: ', device)"]},{"cell_type":"markdown","metadata":{"id":"yDdfXS_b90J7"},"source":["# Misc. Functions"]},{"cell_type":"markdown","metadata":{"id":"fc9w1e_ACjmb"},"source":["## Cropping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IbC9Sgg9RcT"},"outputs":[],"source":["def crop_single_image_by_size(image, crop_size=-1):\n","    '''\n","    Function to crop a single image to a square shape, with even margins around the edges.\n","\n","    image:       Input image tensor of shape [height, width]\n","    crop_size:   Edge size of (square) image to keep. The edges are discarded.\n","    '''\n","    x_size = image.shape[1]\n","\n","    margin_low = int((x_size-crop_size)/2.0)  # (90-71)/2 = 19/2 = 9.5 -->9\n","    margin_high = x_size-crop_size-margin_low # 90-71-9 = 10\n","\n","    pix_min = 0 + margin_low\n","    pix_max = x_size - margin_high\n","\n","    image = image[pix_min : pix_max , pix_min : pix_max]\n","\n","    return image\n","\n","def crop_single_image_by_factor(image, crop_factor=1):\n","    '''\n","    Function to crop a single image for a factor.\n","\n","    image:       Input image tensor of shape [height, width]\n","    crop_factor: Fraction of image to keep. The image is trimmed so the edges are discarded.\n","    '''\n","    x_size = image.shape[1]\n","    y_size = image.shape[0]\n","\n","    x_margin = int(x_size*(1-crop_factor)/2)\n","    y_margin = int(y_size*(1-crop_factor)/2)\n","\n","    x_min = 0 + x_margin\n","    x_max = x_size - x_margin\n","    y_min = 0 + y_margin\n","    y_max = y_size - y_margin\n","\n","    return image[y_min:y_max , x_min:x_max]\n","\n","\n","def crop_image_tensor_with_corner(image_tensor, crop_size, corner=(0,0)):\n","    '''\n","    Function which returns a smaller, cropped version of a tensor (multiple images)\n","\n","    image_tensor:       a batch of images with dimensions: (num_images, channel, y_dimension, x_dimension)\n","    corner:      upper-left corner of window\n","    crop_size:   size of cropping window\n","    '''\n","\n","    y_min = corner[0]\n","    y_max = corner[0]+crop_size\n","    x_min = corner[1]\n","    x_max = corner[1]+crop_size\n","\n","    return image_tensor[:, :, y_min:y_max , x_min:x_max ]\n","\n","\n","def crop_image_tensor_by_size(image_tensor, crop_size):\n","    '''\n","    Function which returns a smaller, cropped version of a tensor (multiple images)\n","\n","    batch:       a batch of images with dimensions: (num_images, channel, y_dimension, x_dimension)\n","    crop_size:   size of cropping window\n","    '''\n","    x_size = image_tensor.shape[3]\n","    y_size = image_tensor.shape[2]\n","\n","    x_margin = int((x_size-crop_size)/2)\n","    y_margin = int((y_size-crop_size)/2)\n","\n","    x_min = 0 + x_margin\n","    x_max = x_size - x_margin\n","    y_min = 0 + y_margin\n","    y_max = y_size - y_margin\n","\n","    return image_tensor[:,:, y_min:y_max , x_min:x_max ]\n","\n","\n","def crop_image_tensor_by_factor(image_tensor, crop_factor=1):\n","    '''\n","    Function to crop an image tensor.\n","\n","    image_tensor:   Input image tensor of shape [image number, channel, height, width]\n","    crop_factor:    Fraction of image to keep. The images are trimmed so the edges are discarded.\n","    '''\n","    x_size = image_tensor.shape[3]\n","    y_size = image_tensor.shape[2]\n","\n","    x_margin = int(x_size*(1-crop_factor)/2)\n","    y_margin = int(y_size*(1-crop_factor)/2)\n","\n","    x_min = 0 + x_margin\n","    x_max = x_size - x_margin\n","    y_min = 0 + y_margin\n","    y_max = y_size - y_margin\n","\n","    return image_tensor[:,:, y_min:y_max , x_min:x_max ]"]},{"cell_type":"markdown","metadata":{"id":"SjQTwQeHF6BO"},"source":["## Reconstructions & Projection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WD7RY-uF13-","scrolled":true},"outputs":[],"source":["def iradon_MLEM(sino_ground, azi_angles=None, max_iter=15, circle=True, crop_factor=2**0.5/2):\n","    '''\n","    Function to reconstruct a single PET image from a single sinogram using ML-EM.\n","\n","    sino_ground:    sinogram (photopeak). This is a numpy array with minimum values of 0.\n","    azi_angles:     list of azimuthal angles for sinogram. If set to None, angles are assumed to span [0,180)\n","    max_iter:       Maximum number of iterations for ML-EM algorithm.\n","    circle:         circle=True: The projection data spans the width (or height) of the activity distribution, and the reconstructed image is circular.\n","                    circle=False: The projection data (sinograms) spans the corner-to-corner line of the activity distribution, and the reconstructed image is square.\n","    crop_size:      Size to crop the image to after performing ML-EM. For ML-EM performed on a 90x90 sinogram, the\n","                    output image will be 90x90. However, it is necessary to crop this to 64x64 to get the same FOV\n","                    as the dataset. This means the image must cropped by a factor of sqrt(2)/2.\n","    '''\n","    if azi_angles==None:\n","        num_angles = sino_ground.shape[1] # Width\n","        azi_angles=np.linspace(0, 180, num_angles, endpoint=False)\n","\n","    ## Create Sensitivity Image ##\n","    sino_ones = np.ones(sino_ground.shape)\n","    sens_image = iradon(sino_ones, azi_angles, circle=circle, filter_name=None)\n","\n","    if circle==False:\n","        def modify_sens(image, const_factor=0.9, slope=0.03):\n","            '''\n","            Modifies an image so that the area in the central FOV remains constant, but values at edges are attenuated.\n","            image               image to modify\n","            constant_factor     fraction of the image to leave alone\n","            slope               increase this to attenuate images at the edges more\n","            '''\n","            def shape_piecewise(r, const_value, slope):\n","                if r <= const_value:\n","                    return 1\n","                else:\n","                    return 1+slope*(r-const_value)\n","\n","            y_max = image.shape[0]\n","            x_max = image.shape[1]\n","\n","            const_dist = const_factor*x_max/2 # radius over which image remains constant\n","\n","            x_center = (x_max-1)/2.0 # the -1 comes from the fact that the coordinates of a pixel start at 0, not 1\n","            y_center = (y_max-1)/2.0\n","\n","            for y in range(0, y_max):\n","                for x in range(0, x_max):\n","                    r = ((x-x_center)**2 + (y-y_center)**2)**0.5\n","\n","                    total_factor = shape_piecewise(r, const_dist, slope) # creates a circular shaped piece-wise\n","                    #total_factor = shape_piecewise(abs(x-x_center), const_dist, slope) * shape_piecewise(abs(y-y_center), const_dist, slope) # square-shaped piecewise\n","                    #total_factor = shape_piecewise(abs(y-y_center), const_dist, slope) # vertical only\n","\n","                    image[y,x] = image[y,x]*total_factor\n","\n","            return image\n","        sens_image = modify_sens(sens_image)\n","\n","    ## Create blank reconstruction ##\n","    image_recon  = np.ones(sens_image.shape)\n","\n","    for iter in range(max_iter):\n","\n","        if circle==True:\n","            sens_image = sens_image + 0.001 # Guarantees the denominator is >0\n","\n","        sino_recon = radon(image_recon, azi_angles, circle=circle) #\n","        sino_recon[sino_recon==0]=1000 # Set a limit on the denominator (next line)\n","        sino_ratio = sino_ground / (sino_recon) #\n","        image_ratio = iradon(sino_ratio, azi_angles, circle=circle, filter_name=None) / sens_image\n","        image_ratio[image_ratio>1.5]=1.5 # Sets limit on backprojected ratio, on how fast image can grow. Threshold and set value should equal each other (good value=1.5)\n","        image_recon = image_recon * image_ratio\n","        image_recon[image_recon<0]=0 # Sets floor on image pixels. No need to adjust.\n","\n","        #footprint = morphology.disk(1)\n","        #image_recon = opening(image_recon, footprint)\n","\n","    image_cropped = crop_single_image_by_factor(image_recon, crop_factor=crop_factor)\n","    #image_cropped = crop_single_image_by_size(image_recon, crop_size=crop_size)\n","\n","    return image_cropped\n","\n","def reconstruct(sinogram_tensor, config, image_size=90, recon_type='FBP', circle=True):\n","    '''\n","    Function for calculating a reconstructed PET image tensor, given a sinogram_tensor. One image is reconstructed for\n","    each sinogram in the sinogram_tensor.\n","\n","    sinogram_tensor:    Tensor of sinograms of size (number of images)x(channels)x(height)x(width).\n","                        Only the first channel (photopeak) is used for recontruction here.\n","    config:             configuration dictionary\n","    image_size:         size of output (images are resized to this shape)\n","    recon_type:         Can be set to 'MLEM' for maximum-likelihood expectation maximization, or 'FBP' for\n","                        filtered back-projection.\n","    circle              circle=True: The projection data spans the width (or height) of the activity distribution, and the reconstructed image is circular.\n","                        circle=False: The projection data (sinograms) spans the corner-to-corner line of the activity distribution, and the reconstructed image is square.\n","\n","    Function returns a tensor of reconstructed images. Returned images are resized, and optionall normalized and scaled (according to the keys in the configuration dictionary)\n","    '''\n","    normalize = config[\"SI_normalize\"]\n","    scale = config['SI_scale']\n","\n","    photopeak_array = torch.clamp(sinogram_tensor[:,0,:,:], min=0).detach().cpu().numpy()\n","    # Note: there really should be no need to clamp the sinogram as it should contain no negative values.\n","\n","    ## Reconstruct Individual Sinograms ##\n","    first=True\n","    for sino in photopeak_array[0:,]:\n","        if recon_type == 'FBP':\n","            image = iradon(sino.squeeze(),\n","                        circle=False, # For an unknown reason, circle=False gives better reconstructions here. Maybe due to errors introduced in interpolation.\n","                        preserve_range=True,\n","                        filter_name='cosine' # Options: 'ramp', 'shepp-logan', 'cosine', 'hamming', 'hann'\n","                        )\n","        else:\n","            image = iradon_MLEM(sino, circle=circle)\n","\n","        ## Morphologic Opening - removes outlier pixels than can cause problems with image normalization\n","        #footprint = morphology.disk(1)\n","        #image = opening(image, footprint)\n","\n","        ## Concatenate Images ##\n","        image = np.expand_dims(image, axis=0) # Add a dimension to the beginning of the reconstructed image\n","        if first==True:\n","            image_array = image\n","            first=False\n","        else:\n","            image_array = np.append(image_array, image, axis=0)\n","\n","    ## For All Images: create resized/dimensioned Torch tensor ##\n","    image_array = np.expand_dims(image_array, axis=1)        # Creates channels dimension\n","    a = torch.from_numpy(image_array)                        # Converts to Torch tensor\n","    a = torch.clamp(a, min=0)                                # You HAVE to clamp before normalizing or the negative values throw it off.\n","    a = transforms.Resize(size = (image_size, image_size), antialias=True)(a) # Resize tensor\n","\n","    ## Normalize Entire Tensor ##\n","    if normalize:\n","        batch_size = len(a)\n","        a = torch.reshape(a,(batch_size, 1, image_size**2)) # Flattens each image\n","        a = nn.functional.normalize(a, p=1, dim = 2)\n","        a = torch.reshape(a,(batch_size, 1 , image_size, image_size)) # Reshapes images back into square matrices\n","        a = scale*a\n","\n","    return a.to(device)\n","\n","def project(image_tensor, circle=False, theta=-1):\n","    '''\n","    Perform the forward radon transform to calculate projections from images. Returns an array of sinograms.\n","\n","    image_tensor:   tensor of PET images\n","    theta:          numpy array of projection angles. Default is [0,180)\n","    '''\n","    image_collapsed = torch.clamp(image_tensor[:,0,:,:], min=0).detach().squeeze().cpu().numpy()\n","\n","    if theta==-1:\n","        theta = np.arange(0,180)\n","\n","    first=True\n","    for image in image_collapsed[0:,]:\n","        sino = radon(image,\n","                    circle=circle,\n","                    preserve_range=True,\n","                    theta=theta,\n","                    )\n","        sino = np.moveaxis(np.atleast_3d(sino), 2, 0) # Adds a blank axis and moves it to the beginning\n","        if first==True:\n","            sino_array=sino\n","            first=False\n","        else:\n","            sino_array = np.append(sino_array, sino, axis=0)\n","\n","    return torch.from_numpy(sino_array)"]},{"cell_type":"markdown","metadata":{"id":"Doag6oulGKVo"},"source":["## Display Images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bV8lRESBGS52"},"outputs":[],"source":["def show_single_unmatched_tensor(image_tensor, grid=False, cmap='inferno', fig_size=1):\n","    '''\n","    Function for visualizing images. The images are displayed, each with their own colormap scaling, so quantitative comparisons are not possible.\n","    Send only the images you want plotted to this function. Works with both single channel and multi-channel images.\n","    If using the single-channel grid option, it plots 120 images in a 15x8 grid.\n","\n","    image_tensor:   image tensor of shape [num, chan, height, width]\n","    grid:           If True, displays images in a 15x8 grid (120 images in total). If false, images are displayed in a horizontal line.\n","    cmap:           Matplotlib color map\n","    fig_size:       figure size\n","    '''\n","    print(f'Shape: {image_tensor.shape} // Min: {torch.min(image_tensor)} // Max: {torch.max(image_tensor)} \\\n","    //Mean Sum (per image): {torch.sum(image_tensor).item()/(image_tensor.shape[0]*image_tensor.shape[1])} // Sum (a single image): {torch.sum(image_tensor[0,0,:])}')\n","\n","    #image_tensor = image_tensor.detach().squeeze(.cpu()\n","    image_tensor = image_tensor.detach().cpu()\n","    image_tensor = torch.clamp(image_tensor, min=0)\n","\n","    num = image_tensor.size(dim=0)\n","    chan = image_tensor.size(dim=1)\n","\n","    ## Plot 3-Channel Images ##\n","    #image_np = image_grid.mean(dim=0).squeeze().numpy() # This also works!\n","\n","    ## Plot Multi-Channel Images ##\n","    if chan!=1:\n","        #123\n","        print(f'Mean (Ch 0): {torch.mean(image_tensor[:,0,:,:])} // Mean (Ch 1): {torch.mean(image_tensor[:,1,:,:])} // Mean (Ch 2): {torch.mean(image_tensor[:,2,:,:])}')\n","\n","        # Plot Grid #\n","        if grid:\n","            fig, ax = plt.subplots(num, chan, figsize=(fig_size*num, fig_size*chan), constrained_layout=True)\n","            for N in range(0, num): # Iterate over image number\n","                for C in range(0, chan): # Iterate over channels\n","                    img = image_tensor[N,C,:,:]\n","                    ax[N,C].axis('off')\n","                    ax[N,C].imshow(img.squeeze(), cmap=cmap)\n","\n","        # Plot in-Line #\n","        else:\n","            fig, ax = plt.subplots(1, num*(chan+1), figsize=(fig_size, fig_size*num*(chan+1)), constrained_layout=True)\n","            i=0\n","            for N in range(0, num): # Iterate over image number\n","                for C in range(0, chan): # Iterate over channels\n","                    img = image_tensor[N,C,:,:]\n","                    ax[i].axis('off')\n","                    ax[i].imshow(img.squeeze(), cmap=cmap)\n","                    i+=1\n","                blank = torch.ones_like(img)\n","                ax[i].axis('off')\n","                ax[i].imshow(blank.squeeze())\n","                i+=1\n","\n","    ## Plot 1 Channel Images ##\n","    else:\n","        # Plot Grid #\n","        # Note: This plots 120 images at a time!\n","        if grid:\n","            cols, rows = 15, 8\n","        # Plot in-Line #\n","        else:\n","            rows = 1\n","            cols = image_tensor.shape[0]\n","\n","        figure=plt.figure(figsize=(cols*fig_size,rows*fig_size))\n","\n","        for i in range(0, cols*rows):\n","            img = image_tensor[i]             # Shape: torch.Size([3, 1, 180, 180]) /\n","            figure.add_subplot(rows,cols,i+1) # MatplotLib indeces start at 1\n","            plt.axis(\"off\")\n","            plt.imshow(img.squeeze(), cmap=cmap)\n","\n","    plt.show()\n","\n","\n","def show_multiple_matched_tensors(*image_tensors, cmap='inferno', fig_size=0.8):\n","    '''\n","    Function for visualizing images from multiple tensors. Each image is \"matched\" with images from the other tensors,\n","    and each matched set of images (one from each tensor) is plotted with the same colormap in a column.\n","    Send only the images you want plotted to this function. Works with both single channel and multi-channel images.\n","\n","    image_tensors:  list of tensors, each of which may contain multiple images.\n","    '''\n","    for tensor in image_tensors:\n","        # Begin by printing statistics for each tensor\n","        print(f'Shape: {tensor.shape} // Min: {torch.min(tensor)} // Max: {torch.max(tensor)} \\\n","        // Mean: {torch.mean(tensor)} // Mean Sum (per image): {torch.sum(tensor).item()/(tensor.shape[0]*tensor.shape[1])} // Sum (a single image): {torch.sum(tensor[0,0,:])}')\n","\n","    combined_tensor = torch.cat(image_tensors, dim=0).detach().cpu()\n","    combined_tensor = torch.clamp(combined_tensor, min=0)\n","\n","    num_rows = len(image_tensors)           # The number of rows equals the number of tensors (images to match)\n","    num_cols = len(image_tensors[0])        # The length of the zeroth element (of the list) is the number of images in a tensor.\n","    num_chan = image_tensors[0].size(dim=1) # Equivalent to: image_tensors[0].shape(1)\n","\n","    ## Plot 1 Channel Images ##\n","    if num_chan==1:\n","        fig, ax = plt.subplots(num_rows, num_cols, squeeze=False, figsize=(fig_size*num_cols, fig_size*num_rows), constrained_layout=True)\n","        #fig, ax = plt.subplots(num_rows, num_cols, constrained_layout=True)\n","\n","        i=0 # i = column number\n","        for col in range(0, num_cols): # Iterate over column number. All images in a column will have the same colormap.\n","            img_list=[]\n","            min_list=[]\n","            max_list=[]\n","\n","            # Construct image list and normalization object for matched images in a column (iterating over rows) #\n","            for row in range(0,num_rows):                               # We iterate over rows in orcer\n","                img = combined_tensor[row*num_cols+col, 0 ,:,:]         # Grab the correct image (zeroth channel for 1-D images)\n","                img_list.append(img)                                    # We construct a new image list for each row\n","                min_list.append(torch.min(img).item())                  # Create list of image minimums\n","                max_list.append(torch.max(img).item())                  # Create list of image maximums\n","            norm = Normalize(vmin=min(min_list), vmax=max(max_list))    # We construct a normalization object with min/max = min/max pixel value for all images in list\n","\n","            # Plot normalized images in a single column (iterating over rows) #\n","            for row in range(0,num_rows):\n","                ax[row, i].axis('off')\n","                ax[row, i].imshow(img_list[row].squeeze(), cmap=cmap, norm=norm) # Squeeze gets rid of extra channel dimension\n","            i+=1\n","\n","    ## Plot Multi-Channel Images ##\n","    else:\n","        print(f'Mean (Ch 0): {torch.mean(combined_tensor[:,0,:,:])} // Mean (Ch 1): {torch.mean(combined_tensor[:,1,:,:])} // Mean (Ch 2): {torch.mean(combined_tensor[:,2,:,:])}')\n","\n","        #if num_cols>3:  # Restricts to 3-channels. You could get rid of this without an issue.\n","        #    num_cols=3\n","\n","        # Construct figure and axes. Note: 'num_chan+1' arises from the divider blank image btw. each multi-channel image\n","        fig, ax = plt.subplots(num_rows, num_cols*(num_chan+1), squeeze=False, figsize=(fig_size*num_cols*(num_chan+1), fig_size*num_rows), constrained_layout=True)\n","\n","        i=0\n","        for col in range(0, num_cols):      # Iterate over column number\n","            for chan in range(0, num_chan): # Iterate over channels\n","                img_list=[]\n","                min_list=[]\n","                max_list=[]\n","\n","                # Iterates over rows (one row per tensor) to construct an image list and normalization object a single column. All matched images have the same channel. #\n","                for row in range(0,num_rows):\n","                    img = combined_tensor[row*num_cols+col, chan ,:,:] # Constructs an image list where each row has the same channel #\n","                    img_list.append(img)\n","                    min_list.append(torch.min(img).item())\n","                    max_list.append(torch.max(img).item())\n","                norm = Normalize(vmin=min(min_list), vmax=max(max_list))\n","\n","                # Iterates over rows to plot matched images in a single column. These share the same channel. #\n","                for row in range(0,num_rows):\n","                    ax[row, i].axis('off')\n","                    ax[row, i].imshow(img_list[row].squeeze(), cmap=cmap, norm=norm) # Squeeze gets rid of extra channel dimension\n","                i+=1\n","\n","            # After all channels have been iterated, the complete multi-channel image has been plotted. Now we plot a divider before the next image #\n","            for row in range(0,num_rows):\n","                blank = torch.ones_like(img)\n","                ax[row, i].axis('off')\n","                ax[row, i].imshow(blank.squeeze())\n","            i+=1\n","\n","    plt.show()\n","\n","def show_single_commonmap_tensor(image_tensor, nrow=15, figsize=(27,18), cmap='inferno'):\n","    '''\n","    Function for visualizing images from one tensor, all of which will be plotted with the same scaled colormap. Only works with single-channel image tensors.\n","\n","    image_tensor:  image tensor. nrow should go into this evenly.\n","    nrow:          number of rows for the image grid\n","    figsize:       figure size\n","    cmap:          color map\n","    '''\n","    tensor = torch.clamp(image_tensor, min=0).detach().cpu()\n","    image_grid = make_grid(tensor, nrow=nrow)  # from torchvision.utils import make_grid\n","\n","    #print(f'Shape: {tensor.shape} // Min: {torch.min(tensor)} // Max: {torch.max(tensor)} \\\n","    #// Mean: {torch.mean(tensor)} // Mean Sum (per image): {torch.sum(tensor).item()/(tensor.shape[0]*tensor.shape[1])} // Sum (a single image): {torch.sum(tensor[0,0,:])}')\n","\n","    fig, ax = plt.subplots(1,1, figsize=figsize)\n","    ax.axis('off')\n","\n","    image_grid = image_grid[0,:].squeeze()\n","    #plt.imshow(image_grid, cmap=cmap)\n","    im = ax.imshow(image_grid, cmap=cmap)\n","    #fig.colorbar(im, ax=ax)\n","    plt.show()\n","\n","def show_multiple_commonmap_tensors(*image_tensors, cmap='inferno'):\n","    '''\n","    Function for visualizing images from multiple tensors, all of which will be plotted with the same scaled colormap. Only works with single-channel image tensors.\n","\n","    *image_tensors: list of image tensors, all of which should contain the same number of images. Only send the number of images you want to plot to this function.\n","    '''\n","    # Print tensor statistics #\n","    for tensor in image_tensors:\n","        print(f'Shape: {tensor.shape} // Min: {torch.min(tensor)} // Max: {torch.max(tensor)} \\\n","        // Mean: {torch.mean(tensor)} // Mean Sum (per image): {torch.sum(tensor).item()/(tensor.shape[0]*tensor.shape[1])} // Sum (a single image): {torch.sum(tensor[0,0,:])}')\n","\n","    num_rows = len(image_tensors)\n","    num_columns = len(image_tensors[0])\n","    # Combine tensors into one & clamp #\n","    combined_tensor = torch.cat(image_tensors, dim=0).detach().cpu()\n","    combined_tensor = torch.clamp(combined_tensor, min=0)\n","    # Make a grid of the tensors #\n","    image_grid = make_grid(combined_tensor, nrow=num_columns) # Note: nrow is the number of images displayed in each row (i.e., the number of columns)\n","\n","    # Determine figure size #\n","    print('num_rows:', num_rows)\n","    fig, ax = plt.subplots(1,1, figsize=(30,1*num_rows))\n","    #fig, ax = plt.subplots(1,1, figsize=(30,7))\n","\n","    ax.axis('off')\n","\n","    image_grid = image_grid[0,:].squeeze()\n","    im = ax.imshow(image_grid, cmap=cmap)\n","    fig.colorbar(im, ax=ax)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ajsf7Iok0X2I"},"source":["## Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NzzJJ4-70XE5"},"outputs":[],"source":["##################################################\n","## Functions for Calculating Metrics Dataframes ##\n","##################################################\n","\n","## Calculate Arbitrary Metric ##\n","def calculate_metric(batch_A, batch_B, img_metric_function, return_dataframe=False, label='default', crop_factor=1):\n","    '''\n","    Function which calculates metric values for two batches of images.\n","    Returns either the average metric value for the batch or a dataframe of individual image metric values.\n","\n","    batch_A:                tensor of images to compare [num, chan, height, width]\n","    batch_B:                tensor of images to compare [num, chan, height, width]\n","    img_metric_function:    a function which calculates a metric (MSE, SSIM, etc.) from two INDIVIDUAL images\n","    return_dataframe:       If False, then the average is returned.\n","                            Otherwise both the average, and a dataframe containing the metric values of the images in the batches, are returned.\n","    label:                  what to call dataframe, if it is created\n","    crop_factor:            factor by which to crop both batches of images. 1 = whole image retained.\n","    '''\n","\n","    if crop_factor != 1:\n","        A = crop_image_tensor_by_factor(batch_A, crop_factor=crop_factor)\n","        B = crop_image_tensor_by_factor(batch_B, crop_factor=crop_factor)\n","\n","    length = len(batch_A)\n","    metric_avg = 0\n","    metric_list = []\n","\n","    for i in range(length):\n","        image_A = batch_A[i:i+1,:,:,:] # Using i:i+1 instead of just i preserves the dimensionality of the array\n","        image_B = batch_B[i:i+1,:,:,:]\n","\n","        metric_value = img_metric_function(image_A, image_B)\n","        metric_avg += metric_value/length\n","        if return_dataframe==True:\n","            metric_list.append(metric_value)\n","\n","    metric_frame = pd.DataFrame({label : metric_list})\n","\n","    if return_dataframe==False:\n","        return metric_avg\n","    else:\n","        return metric_frame, metric_avg\n","\n","\n","def update_tune_dataframe(tune_dataframe, model, config, mean_CNN_MSE, mean_CNN_SSIM, mean_CNN_CUSTOM):\n","    '''\n","    Function to update the tune_dataframe for each trial run that makes it partway through the tuning process.\n","    '''\n","    # Extract values from config dictionary\n","    SI_dropout =        config['SI_dropout']\n","    SI_exp_kernel =     config['SI_exp_kernel']\n","    SI_gen_fill =       config['SI_gen_fill']\n","    SI_gen_hidden_dim = config['SI_gen_hidden_dim']\n","    SI_gen_neck =       config['SI_gen_neck']\n","    SI_layer_norm =     config['SI_layer_norm']\n","    SI_normalize =      config['SI_normalize']\n","    SI_pad_mode =       config['SI_pad_mode']\n","    batch_size =        config['batch_size']\n","    gen_lr =            config['gen_lr']\n","\n","    # Calculate number of trainable weights in CNN\n","    num_params = sum(map(torch.numel, model.parameters()))\n","\n","    # Concatenate Dataframe\n","    add_frame = pd.DataFrame({'SI_dropout': SI_dropout, 'SI_exp_kernel': SI_exp_kernel, 'SI_gen_fill': SI_gen_fill, 'SI_gen_hidden_dim': SI_gen_hidden_dim,\n","                            'SI_gen_neck': SI_gen_neck, 'SI_layer_norm': SI_layer_norm, 'SI_normalize': SI_normalize, 'SI_pad_mode': SI_pad_mode, 'batch_size': batch_size,\n","                            'gen_lr': gen_lr, 'num_params': num_params, 'mean_CNN_MSE': mean_CNN_MSE, 'mean_CNN_SSIM': mean_CNN_SSIM, 'mean_CNN_CUSTOM': mean_CNN_CUSTOM}, index=[0])\n","\n","    tune_dataframe = pd.concat([tune_dataframe, add_frame], axis=0)\n","\n","    # Save Dataframe to File\n","    tune_dataframe.to_csv(tune_dataframe_path, index=False)\n","\n","    return tune_dataframe\n","\n","\n","def update_test_dataframe_and_return_outputs(sino_tensor, image_size, CNN_output, ground_image, test_dataframe, config):\n","    '''\n","    Function which: A) performs reconstructions (FBP and possibly ML-EM)\n","                    B) constructs a dataframe of metric values (MSE & SSIM) for these reconstructions, and also for the CNN output, with respect to the ground truth image.\n","                    C) concatenates this with the test dataframe passed to this function\n","                    D) returns the concatenated dataframe, mean metric values, and reconstructions\n","\n","    sino_tensor:    sinogram tensor of shape [num, chan, height, width]\n","    image_size:     image_size\n","    CNN_output:     CNN reconstruction\n","    ground_image:   ground truth image\n","    test_dataframe: dataframe to append metric values to\n","    config:         general config dictionary\n","    '''\n","    # Construct Outputs #\n","    FBP_output = reconstruct(sino_tensor, config, image_size=image_size, recon_type='FBP')\n","    if compute_MLEM==True:\n","        MLEM_output = reconstruct(sino_tensor, config, image_size=image_size, recon_type='MLEM')\n","    else: # If not looking at ML-EM, don't waste time computing the MLEM images, which can take awhile.\n","        MLEM_output = FBP_output\n","\n","    # Dataframes: build dataframes for every reconstruction technique/metric combination #\n","    batch_CNN_MSE,  mean_CNN_MSE   = calculate_metric(ground_image, CNN_output, MSE,  return_dataframe=True, label='MSE (Network)')\n","    batch_CNN_SSIM,  mean_CNN_SSIM = calculate_metric(ground_image, CNN_output, SSIM, return_dataframe=True, label='SSIM (Network)')\n","    batch_FBP_MSE,  mean_FBP_MSE   = calculate_metric(ground_image, FBP_output, MSE,  return_dataframe=True, label='MSE (FBP)')\n","    batch_FBP_SSIM,  mean_FBP_SSIM = calculate_metric(ground_image, FBP_output, SSIM, return_dataframe=True, label='SSIM (FBP)')\n","    batch_MLEM_MSE, mean_MLEM_MSE  = calculate_metric(ground_image, MLEM_output, MSE, return_dataframe=True, label='MSE (ML-EM)')\n","    batch_MLEM_SSIM, mean_MLEM_SSIM= calculate_metric(ground_image, MLEM_output, SSIM,return_dataframe=True, label='SSIM (ML-EM)')\n","\n","    # Concatenate batch dataframes and larger running test dataframe\n","    add_frame = pd.concat([batch_CNN_MSE, batch_FBP_MSE, batch_MLEM_MSE, batch_CNN_SSIM, batch_FBP_SSIM, batch_MLEM_SSIM], axis=1)\n","    test_dataframe = pd.concat([test_dataframe, add_frame], axis=0)\n","\n","    # Return a whole lot of stuff\n","    return test_dataframe, mean_CNN_MSE, mean_CNN_SSIM, mean_FBP_MSE, mean_FBP_SSIM, mean_MLEM_MSE, mean_MLEM_SSIM, FBP_output, MLEM_output\n","\n","######################\n","## Metric Functions ##\n","######################\n","\n","## Metrics which take only single images as inputs ##\n","## ----------------------------------------------- ##\n","def SSIM(image_A, image_B, win_size=-1):\n","    '''\n","    Function to return the SSIM for two 2D images.\n","\n","    image_A:        pytorch tensor for a single image\n","    image_B:        pytorch tensor for a single image\n","    win_size:       window size to use when computing the SSIM. If =-1, the full size of the image is used.\n","    '''\n","\n","    if win_size == -1:   # The default shape of the window size is the same size as the image.\n","        x = image_A.shape[2]\n","        win_size = (x if x % 2 == 1 else x-1) # Guarantees the window size is odd.\n","\n","    image_A_npy = image_A.detach().squeeze().cpu().numpy()\n","    image_B_npy = image_B.detach().squeeze().cpu().numpy()\n","\n","    max_value = max([np.amax(image_A_npy, axis=(0,1)), np.amax(image_B_npy, axis=(0,1))])   # Find maximum among the images\n","    min_value = min([np.amin(image_A_npy, axis=(0,1)), np.amin(image_B_npy, axis=(0,1))])   # Find minimum among the images\n","    data_range = max_value-min_value\n","\n","    SSIM_image = structural_similarity(image_A_npy, image_B_npy, data_range=data_range, gaussian_weights=False, use_sample_covariance=False, win_size=win_size)\n","\n","    return SSIM_image\n","\n","## Metrics which take either batches or images as inputs ##\n","## ----------------------------------------------------- ##\n","def MSE(image_A, image_B):\n","    '''\n","    Function to return the mean square error for two 2D images (or two batches of images).\n","\n","    image_A:        pytorch tensor for a single image\n","    image_B:        pytorch tensor for a single image\n","    '''\n","    image_A_npy = image_A.detach().squeeze().cpu().numpy()\n","    image_B_npy = image_B.detach().squeeze().cpu().numpy()\n","\n","    return torch.mean((image_A-image_B)**2).item()\n","\n","def NMSE(image_A, image_B):\n","    '''\n","    Function to return the normalized mean square error for two 2D images (or two batches of images).\n","\n","    image_A:        pytorch tensor for a single image (reference image)\n","    image_B:        pytorch tensor for a single image\n","    '''\n","    image_A_npy = image_A.detach().squeeze().cpu().numpy()\n","    image_B_npy = image_B.detach().squeeze().cpu().numpy()\n","\n","    return (torch.mean((image_A-image_B)**2)/torch.mean(image_A**2)).item()\n","\n","def MAE(image_A, image_B):\n","    '''\n","    Function to return the mean absolute error for two 2D images (or two batches of images).\n","\n","    image_A:        pytorch tensor for a single image\n","    image_B:        pytorch tensor for a single image\n","    '''\n","    image_A_npy = image_A.detach().squeeze().cpu().numpy()\n","    image_B_npy = image_B.detach().squeeze().cpu().numpy()\n","\n","    return torch.mean(torch.abs(image_A-image_B)).item()\n","\n","def calculate_moments(batch_A, batch_B, window_size = 10, stride=10, dataframe=False):\n","    '''\n","    Function to return the three statistical moment scores for two image tensors.\n","    '''\n","    ## Nested Functions ##\n","\n","    def compare_moments(win_A, win_B, moment):\n","        def compute_moment(win, moment, axis=1):\n","            mean_value = np.mean(win, axis=axis)\n","            if moment == 1:\n","                return mean_value\n","            else:\n","                mean_array = np.array([mean_value] * win.shape[1]).T  # The square brackets in win.shape[1] mean the value is repeated spatially\n","                moment = np.mean((win - mean_array)**moment, axis=1)\n","                return moment\n","\n","        batch_size = win_A.shape[0]\n","\n","\n","        reshape_A = (torch.reshape(win_A, (batch_size, -1))).detach().cpu().numpy()\n","        reshape_B = (torch.reshape(win_B, (batch_size, -1))).detach().cpu().numpy()\n","\n","        moment_A = compute_moment(reshape_A, moment=moment)\n","        moment_B = compute_moment(reshape_B, moment=moment)\n","        moment_score = np.mean(np.absolute(moment_A-moment_B)/(np.absolute(moment_A)+0.1))\n","\n","        '''\n","        print('===============================')\n","        print('MOMENT: ', moment)\n","        print('moment_A shape: ', moment_A.shape)\n","        print('moment_A mean: ', np.mean(moment_A))\n","        print('moment_B shape: ', moment_B.shape)\n","        print('moment_B mean: ', np.mean(moment_B))\n","        print('moment_score, |moment_A-moment_B|/(moment_A+0.1) : ', moment_score)\n","        '''\n","        return moment_score\n","\n","    ## Code ##\n","    image_size = batch_A.shape[2]\n","\n","    num_windows = int((image_size)/stride) # Maximum number of windows occurs when: stride = window_size.\n","    while (num_windows-1)*stride + window_size > image_size: # Solve for the number of windows (crops)\n","        num_windows += -1\n","\n","    moment_1_running_score = 0\n","    moment_2_running_score = 0\n","    moment_3_running_score = 0\n","\n","    for i in range(0, num_windows):\n","        for j in range(0, num_windows):\n","            corner = (i*stride, j*stride)\n","\n","            win_A = crop_image_tensor_with_corner(batch_A, window_size, corner)\n","            win_B = crop_image_tensor_with_corner(batch_B, window_size, corner)\n","\n","            moment_1_score = compare_moments(win_A, win_B, moment=1)\n","            moment_2_score = compare_moments(win_A, win_B, moment=2)\n","            moment_3_score = compare_moments(win_A, win_B, moment=3)\n","\n","            moment_1_running_score += moment_1_score\n","            moment_2_running_score += moment_2_score\n","            moment_3_running_score += moment_3_score\n","\n","    return moment_1_running_score, moment_2_running_score, moment_3_running_score\n","\n","def LDM(batch_A, batch_B):\n","    '''\n","    Calculate the local distributions metric (LDM) for two batches of images\n","    '''\n","\n","    score_1, score_2, score_3 = calculate_moments(batch_A, batch_B, window_size=5, stride=5)\n","\n","    score_1 = score_1*1\n","    score_2 = score_2*1\n","    score_3 = score_3*1\n","\n","    '''\n","    print('Scores')\n","    print('====================')\n","    print(score_1)\n","    print(score_2)\n","    print(score_3)\n","    '''\n","\n","    return score_1+score_2+score_3\n","\n","def custom_metric(batch_A, batch_B):\n","    return 0\n","    #return MSE(batch_A, batch_B)\n","\n","\n","\n","###############################################\n","## Average or a Batch Metrics: Good for GANs ##\n","###############################################\n","\n","# Range #\n","def range_metric(real, fake):\n","    '''\n","    Computes a simple metric which penalizes \"fake\" images in a batch for having a range different than the \"real\" images in a batch.\n","    Only a single metric number is returned.\n","    '''\n","    range_real = torch.max(real).item()-torch.min(real).item()\n","    range_fake = torch.max(fake).item()-torch.min(fake).item()\n","\n","    return abs(range_real-range_fake)/(range_real+.1)\n","\n","# Average #\n","def avg_metric(real, fake):\n","    '''\n","    Computes a simple metric which penalizes \"fake\" images in a batch for having an average value different than the \"real\" images in a batch.\n","    Only a single metric number is returned.\n","    '''\n","    avg_metric = abs((torch.mean(real).item()-torch.mean(fake).item())/(torch.mean(real)+.1).item())\n","    return avg_metric\n","\n","# Pixel Variation #\n","def pixel_dist_metric(real, fake):\n","    '''\n","    Computes a metric which penalizes \"fake\" images for having a pixel distance different than the \"real\" images.\n","\n","    real: real image tensor\n","    fake: fake image tensor\n","    '''\n","    def pixel_dist(image_tensor):\n","        '''\n","        Function for computing the pixel distance (standard deviation from mean) for a batch of images.\n","        For simplicity, it only looks at the 0th channel.\n","        '''\n","        array = image_tensor[:,0,:,:].detach().cpu().numpy().squeeze()\n","        sd = np.std(array, axis=0)\n","        avg=np.mean(sd)\n","        return(avg)\n","\n","    pix_dist_fake = pixel_dist(fake)\n","    pix_dist_real = pixel_dist(real)\n","\n","    return abs((pix_dist_real-pix_dist_fake)/(pix_dist_real+.1)) # The +0.1 in the denominators guarantees we don't divide by zero\n","\n","###################\n","## Old Functions ##\n","###################\n","\n","\n","def LDM_OLD(real, fake, crop_size = 10, stride=10):\n","    '''\n","    Function to return the local distributions metric for two images.\n","\n","    image_A:        pytorch tensor for a single image\n","    image_B:        pytorch tensor for a single image\n","    '''\n","    image_size = real.shape[2]\n","\n","    i_max = int((image_size)/stride) # Maximum number of windows occurs when the stride equals the crop_size\n","    while (i_max-1)*stride + crop_size > image_size: # If stride < crop_size, we need fewer need to solve for the number of crops\n","        i_max += -1\n","\n","    def crop_image_tensor_with_corner(A, corner=(0,0), crop_size=1):\n","        '''\n","        Function which returns a small, cropped version of an image.\n","\n","        A           a batch of images with dimensions: (num_images, channel, height, width)\n","        corner      upper-left corner of window\n","        crop_size   size of croppiong window\n","        '''\n","        x_min = corner[1]\n","        x_max = corner[1]+crop_size\n","        y_min = corner[0]\n","        y_max = corner[0]+crop_size\n","        return A[:,:, y_min:y_max , x_min:x_max ]\n","\n","    running_dist_score = 0\n","    running_avg_score = 0\n","\n","    for i in range(0, i_max):\n","        for j in range(0, j_max):\n","            corner = (i*crop_size, j*crop_size)\n","            win_real = crop_image_tensor_with_corner(real, corner, crop_size)\n","            win_fake = crop_image_tensor_with_corner(fake, corner, crop_size)\n","\n","            #range_score = range_metric(win_real, win_fake)\n","            avg_score = avg_metric(win_real, win_fake)\n","            pixel_dist_score = pixel_dist_metric(win_real, win_fake)\n","\n","            running_dist_score += pixel_dist_score\n","            running_avg_score += avg_score\n","\n","    combined_score = running_dist_score + running_avg_score\n","\n","    return combined_score\n"]},{"cell_type":"markdown","source":["## ROI Analysiss"],"metadata":{"id":"Ri6DKrFlFZDC"}},{"cell_type":"code","source":["def ROI_NEMA_hot(ground_truth_tensor, reconstruction_tensor, background_mask, hot_mask):\n","\n","    background_truth =          np.sum(np.multiply(background_mask.numpy(), ground_truth_tensor.numpy())) # You must convert tensors to numpy arrays before multiplication\n","    background_reconstruction = np.sum(np.multiply(background_mask.numpy(), reconstruction_tensor.numpy()))\n","\n","    hot_truth          = np.sum(np.multiply(hot_mask.numpy(), ground_truth_tensor.numpy()))\n","    hot_reconstruction = np.sum(np.multiply(hot_mask.numpy(), reconstruction_tensor.numpy()))\n","\n","    print('background_reconstruction: ', background_reconstruction)\n","    print('background_truth: ', background_truth)\n","    print('hot_reconstruction: ', hot_reconstruction)\n","    print('hot_truth: ', hot_truth)\n","\n","    x = 100*(hot_reconstruction/background_reconstruction-1)/(hot_truth/background_truth-1) # This measure is invariant to absolute pixel values. It only measure relative contrast.\n","\n","    return x\n","\n","\n","def ROI_NEMA_cold(reconstruction_tensor, background_mask, cold_mask):\n","\n","    background_reconstruction = np.sum(np.multiply(background_mask.numpy(), reconstruction_tensor.numpy()))\n","    cold_reconstruction = np.sum(np.multiply(cold_mask.numpy(), reconstruction_tensor.numpy()))\n","\n","    print('background_reconstruction: ', background_reconstruction)\n","    print('cold_reconstruction: ', cold_reconstruction)\n","    x = 100*(1-cold_reconstruction/background_reconstruction) # This measure does not take into account ground truths because the cold/background should ideally always be zero.\n","\n","    return x"],"metadata":{"id":"v5-4x2ORFYbe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YIuoH75uhqMI"},"source":["# Find Train/Val/Test Splits"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1750,"status":"ok","timestamp":1759891264951,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"},"user_tz":300},"id":"Sd9bepAdhnD7","outputId":"9832b6ed-cb50-4202-fd2e-7310d6a847a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Codes:  ['male_pt96', 'male_pt108', 'male_pt118', 'male_pt139', 'male_pt141', 'male_pt144', 'male_pt145', 'male_pt155', 'male_pt157', 'male_pt163', 'male_pt164', 'male_pt167', 'male_pt169', 'male_pt171', 'male_pt178', 'male_pt180', 'male_pt184', 'male_pt196', 'male_pt200', 'male_pt168', 'female_pt71', 'female_pt76', 'female_pt86', 'female_pt89', 'female_pt98', 'female_pt99', 'female_pt117', 'female_pt152', 'female_pt153', 'female_pt166', 'female_pt175', 'female_pt176', 'female_pt182', 'female_pt170']\n","Validation Codes  ['male_pt93', 'male_pt128', 'male_pt159', 'male_pt201', 'female_pt106', 'female_pt147', 'female_pt149']\n","Test Codes:  ['male_pt77', 'male_pt80', 'male_pt92', 'male_pt150', 'male_pt154', 'male_pt173', 'female_pt140', 'female_pt142', 'female_pt143', 'female_pt151', 'female_pt162']\n","34\n","7\n","11\n","52\n"]}],"source":["def stratified_split_from_csv(csv_path, stratify_cols, code_col,\n","                              proportions=(0.6, 0.2, 0.2),\n","                              output_txt=\"phantom_splits.txt\",\n","                              random_state=42):\n","    \"\"\"\n","    Splits a phantom dataset into train/validation/test sets using stratified sampling.\n","\n","    Parameters\n","    ----------\n","    csv_path : str\n","        Path to the input CSV file containing phantom characteristics.\n","    stratify_cols : list of str\n","        Column names to use for stratified splitting (e.g. [\"age\", \"gender\", \"BMI\"]).\n","    code_col : str\n","        Column name that uniquely identifies each phantom.\n","    proportions : tuple of floats\n","        Fractions for (train, val, test) sets. Must sum to 1.\n","    output_txt : str\n","        Path for the output text file listing phantom codes per set.\n","    random_state : int\n","        Seed for reproducibility.\n","\n","    Returns\n","    -------\n","    train_codes, val_codes, test_codes : list of str\n","        Lists of phantom codes for each split.\n","    \"\"\"\n","    # --- 1. Read CSV ---\n","    df = pd.read_csv(csv_path)\n","\n","    # --- 2. Automatically bin numeric columns for stratification ---\n","    df_binned = df.copy()\n","    for col in stratify_cols:\n","        if np.issubdtype(df[col].dtype, np.number):\n","            # Automatically bin into 4 roughly equal-frequency bins. Column entries are replace with strings like '(19.0, 32.0]'. These labels repeat for numbers in the same range.\n","            df_binned[col] = pd.qcut(df[col], q=4, duplicates='drop').astype(str)\n","        else:\n","            df_binned[col] = df[col].astype(str) # Makes sure all labels are strings.\n","\n","    # --- 3. Convert stratification columns into a multilabel one-hot matrix ---\n","    strat_matrix = pd.get_dummies(df_binned[stratify_cols]) # strat_matrix is still a datafram.\n","\n","    # --- 4. Prepare for stratified splitting ---\n","    X = np.zeros((len(df), 1))  # dummy feature matrix (not used)\n","    y = strat_matrix.values     # y = multilabel targets for stratification. The .values attribute means that y is a numpy matrix now, not a dataframe.\n","\n","    msss = MultilabelStratifiedShuffleSplit(n_splits=1,  # Create an msss object with method that splits phantoms into (training) set and [(validation + test) set].\n","                                            test_size=proportions[1] + proportions[2],\n","                                            random_state=random_state)\n","    train_idx, temp_idx = next(msss.split(X, y)) # These are the absolute indices identifying the (training set) and [(validation + test) set]\n","\n","    # Split remaining into validation and test\n","    temp_y = y[temp_idx]  # temp_y = matrix of one hot vectors for remaining sets (validation + test)\n","    test_fraction = proportions[2] / (proportions[1] + proportions[2])  # Proportion of remaining (validation + test) phantoms that are composed of validation phantoms\n","    msss_val = MultilabelStratifiedShuffleSplit(n_splits=1,            # We set n_splits=1 because if we try doing multiple splits at once, we'll just get multiple instances of partitioning the set in two.\n","                                                test_size=test_fraction,\n","                                                random_state=random_state)\n","    val_idx_rel, test_idx_rel = next(msss_val.split(np.zeros((len(temp_y), 1)), temp_y))\n","\n","    val_idx = temp_idx[val_idx_rel]  # Converts from relative index back to absolute\n","    test_idx = temp_idx[test_idx_rel]\n","\n","    # --- 5. Extract phantom codes ---\n","    codes = df[code_col].astype(str)\n","    train_codes = list(codes.iloc[train_idx])\n","    val_codes = list(codes.iloc[val_idx])\n","    test_codes = list(codes.iloc[test_idx])\n","\n","    # --- 6. Write results to a text file ---\n","    with open(output_txt, \"w\") as f:\n","        f.write(\"TRAIN SET:\\n\")\n","        f.write(\", \".join(train_codes) + \"\\n\\n\")\n","        f.write(\"VALIDATION SET:\\n\")\n","        f.write(\", \".join(val_codes) + \"\\n\\n\")\n","        f.write(\"TEST SET:\\n\")\n","        f.write(\", \".join(test_codes) + \"\\n\")\n","\n","    return train_codes, val_codes, test_codes\n","\n","\n","## 52 Patients total ##\n","# 35 train\n","# 10 test\n","# 7  cross-validation\n","\n","csv_path = '/content/drive/MyDrive/Colab/Working/dataset/XCAT_sortingInfo.csv'\n","stratify_cols = ['Gender', 'Age', 'BMI']\n","code_col = 'Code'\n","proportions = (35/52.0,7/52,10/52)\n","output_txt = '/content/drive/MyDrive/Colab/Working/dataset/phantom_splits.txt'\n","random_state=33\n","\n","train_codes, val_codes, test_codes = stratified_split_from_csv(csv_path, stratify_cols, code_col, proportions=proportions, output_txt=output_txt, random_state=random_state)\n","\n","print('Train Codes: ', train_codes)\n","print('Validation Codes ', val_codes)\n","print('Test Codes: ', test_codes)\n","\n","print(len(train_codes))\n","print(len(val_codes))\n","print(len(test_codes))\n","print(len(train_codes)+len(val_codes)+len(test_codes))\n"]},{"cell_type":"markdown","metadata":{"id":"RggsFzKSlks_"},"source":["# Create Dataset from Interfiles"]},{"cell_type":"markdown","metadata":{"id":"HzR48joSlks_"},"source":["## Read Interfile"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MwurwS-llks_"},"outputs":[],"source":["def read_interfile_data(load_dir_path, header_filename, exclude_slices=(0,0), numpy=False, shift_x=0, shift_y=0, crop_size=-1, resize_size=-1, blur=-1, normalize_scale=-1):\n","    '''\n","    Function for reading interfile images or sinograms. Either may be in interfile 3.3 or STIR format.\n","        The function will figure which of the four possibilities (2 formats x 2 file types) are being read.\n","        Returns a pytorch tensor.\n","\n","    load_dir_path:   path to directory containing the data\n","    header_file:     header file (of interfile). Can use any extension.\n","    exclude_slices:  tuple. Exclude this number of slices at the beginning and end of stack, keeping only the middle slices.\n","    numpy:           return a numpy array instead of a pytorch tensor.\n","    shift_x:         translate the image this many pixels to the right\n","    shift_y:         translate the image this many pixels up\n","\n","\n","    ## The following parameters may be set to -1 to disable cropping, resizing, blurring, or normalization.\n","\n","    crop_size:       crop the square tensor to be of shape: [ #images, 1, crop_size, crop_size]\n","    resize_size:     set to a positive scalar <=1 (resize by a scale factor), or tuple (resize to a specific size)\n","    blur:            set to a tuple for gaussian blur (kernel size, s.d.)\n","    normalize_scale: normalize pixel values so that they add to this parameter\n","\n","    '''\n","\n","    def read_interfile_header(header_path):\n","        \"\"\"Read and parse the Interfile header.\"\"\"\n","        header_info = {}\n","        with open(header_path, 'r') as file:\n","            for line in file:\n","                #if line.startswith('!'):\n","                #    continue\n","                key_value = re.split(r':=', line, maxsplit=1)\n","                if len(key_value) == 2:\n","                    key = key_value[0].strip()\n","                    value = key_value[1].strip()\n","                    header_info[key] = value\n","        return header_info\n","\n","\n","    header_path=os.path.join(load_dir_path, header_filename)\n","    header_info=read_interfile_header(header_path)\n","\n","    ## Determine file type ##\n","    if int(header_info['!matrix size [1]'])>380: # Sinograms have first dimension = 381\n","        if os.path.splitext(header_path)[1]=='.hs':\n","            type='sino_hs'  # Sinogram in interfile 3.33 format\n","        else:\n","            type='sino_v'   # Sinogram in STIR format\n","    else:\n","        type='image'\n","\n","    ## If Image ##\n","    if type=='image':\n","        x_size = int(header_info['!matrix size [1]'])\n","        y_size = int(header_info['!matrix size [2]'])\n","        if '!matrix size [3]' in header_info:\n","            axial_size = int(header_info['!matrix size [3]'])\n","        else:\n","            axial_size = int(header_info['!total number of images'])\n","        matrix_size = np.array([axial_size,y_size,x_size])\n","\n","    ## If Sinogram ##\n","    else:\n","        tangential_size = int(header_info['!matrix size [1]'])\n","        if type=='sino_hs':\n","            view_size = int(header_info['!matrix size [3]'])    # view_size = number of projections\n","            axial_size_str = header_info['!matrix size [2]']    # In this case, the axial coordinate is surrounded by {}\n","            match = re.search(r'\\d+', axial_size_str)\n","            axial_size = int(match.group())\n","        elif type=='sino_v':\n","            view_size = int(header_info['!matrix size [2]'])\n","            axial_size = int(header_info['!matrix size [3]'])\n","        matrix_size = np.array([axial_size, view_size, tangential_size])     # Here, the angles are in the vertical dimension. That is altered below.\n","\n","    ## Read the binary data ##\n","    if 'name of data file' in header_info:\n","        bin_filename = header_info['name of data file']\n","    else:\n","        bin_filename = header_info['!name of data file']\n","    bin_path=os.path.join(load_dir_path, bin_filename)\n","    total_elements = np.prod(matrix_size)\n","    bin_array = np.fromfile(bin_path, dtype=np.float32, count=total_elements)\n","\n","    ## Manipulate & Convert 3D Stack ##\n","    bin_array = bin_array.reshape(matrix_size)\n","    bin_array = np.expand_dims(bin_array, axis=1)  # Creates channels dimension\n","    bin_tensor = torch.from_numpy(bin_array)       # Converts to PyTorch tensor\n","    bin_tensor = torch.clamp(bin_tensor, min=0)    # You HAVE to clamp before normalizing or the negative values throw it off.\n","\n","    ## If Sinogram, rotate & flip tensor ##\n","    if type=='sino_hs' or type=='sino_v':\n","        bin_tensor = transforms.functional.rotate(bin_tensor, 90, fill=0, expand=True) # Rotate image (now the angles are on the horizontal dimension)\n","        bin_tensor = torch.flip(bin_tensor, dims=(3,)) # Flip horizontally\n","        #bin_tensor = torch.flip(bin_tensor, dims=(1,)) # Flip horizontally\n","\n","    axial_start = exclude_slices[0]\n","    axial_end = axial_size - exclude_slices[1]\n","    bin_tensor = bin_tensor[axial_start:axial_end,:,:,:]\n","\n","    ## Perform Optional Transformations ##\n","    if shift_x!=0:\n","        bin_tensor = torch.roll(bin_tensor, shift_x, dims=(3,)) # Cycle (or 'Roll') tensor\n","    if shift_y!=0:\n","        bin_tensor = torch.roll(bin_tensor, -shift_y, dims=(2,)) # Cycle (or 'Roll') tensor\n","\n","    if crop_size!=-1: # Crop before resize\n","        bin_tensor = crop_image_tensor_by_size(bin_tensor, crop_size=crop_size)\n","\n","    if resize_size!=-1:\n","        bin_tensor = transforms.Resize(size = (resize_size, resize_size), antialias=True)(bin_tensor) # Resize tensor\n","\n","    if blur!=-1:\n","        bin_tensor = transforms.GaussianBlur(blur[0], sigma=(blur[1]))(bin_tensor)\n","\n","    if normalize_scale!=-1:\n","        batch_size = len(bin_tensor)\n","        image_size_y = bin_tensor.shape[2]\n","        image_size_x = bin_tensor.shape[3]\n","        a = torch.reshape(bin_tensor,(batch_size, 1, image_size_x*image_size_y)) # Flattens each image\n","        a = nn.functional.normalize(a, p=1, dim = 2)\n","        a = torch.reshape(a,(batch_size, 1 , image_size_y, image_size_x)) # Reshapes images back into square matrices\n","        bin_tensor = normalize_scale*a\n","\n","    if numpy==True:\n","        output = bin_tensor.numpy()\n","    else:\n","        output = bin_tensor.to(device)\n","\n","    return header_info, output"]},{"cell_type":"markdown","metadata":{"id":"BjVP_vAglktA"},"source":["## Compile Single Data Type"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1759891264968,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"},"user_tz":300},"id":"rydefDcQlktA","outputId":"44541ed4-0570-424c-f8bf-1e792d5ef8f1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n## Changeable Variables ##\\nload_dir_path='./dataset-males/XCAT_male_pt77'  # end_section = 16\\nmap_stem='act_map_toGATE_.h33'\\nstart_section_num=0\\nend_section_num=16\\noverlap=12    # For our dataset, the overlap is 12\\nnum_output_channels=1\\n\\n#load_dir_path='./dataset-males/XCAT_male_pt80' # end_section=15\\n#load_dir_path='./dataset-males/XCAT_male_pt92' # end_section=15\\n\\nmap_array = compile_single_data_type(load_dir_path, map_stem, start_section_num, end_section_num, overlap=overlap, num_output_channels=num_output_channels, shift_x=0, shift_y=0, crop_size=-1, resize_size=-1,\\n                             normalize_scale=-1, visualize=False)\\nvis_tensor=torch.from_numpy(map_array[0:8])\\nshow_single_unmatched_tensor(vis_tensor, fig_size=15)\\n\\n\\n\\nmap_array = compile_single_data_type(load_dir_path, map_stem, start_section_num, end_section_num, overlap=overlap, num_output_channels=3, shift_x=0, shift_y=0, crop_size=-1, resize_size=-1,\\n                             normalize_scale=-1, visualize=False)\\nvis_tensor=torch.from_numpy(map_array[0:3])\\nshow_single_unmatched_tensor(vis_tensor, fig_size=15)\\n\\n#show_single_commonmap_tensor(torch.from_numpy(map_array), nrow=35, figsize=(35,47), cmap='inferno')\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["def compile_single_data_type(load_dir_path, data_filename_stem, start_section_num, end_section_num, overlap=12, num_output_channels=1, shift_x=0, shift_y=0, crop_size=-1, resize_size=-1,\n","                             normalize_scale=-1, visualize=False):\n","    '''\n","    This function loads a single file type for each section of the dataset. This type can be images (reconstructed activity, attenuation map, etc.) or sinograms.\n","    The function then contatenates each of the sections for this single file type to create a larger numpy array.\n","\n","    load_dir_path:         Directory from which to load data\n","    data_filename_stem:    These will be filenames but without the numbers (indicating section numbers). It does include the extension.\n","                           For example, if rebin_sino_stem='rebinned_sino.hs', the section names will go from \"rebinned_sino{start_section_num}.hs\" to 'rebinned_sino{end_sections_num}.hs'\n","    start_section_num:     Start compilation with this section number.\n","    end_section_num:       End compilation with this section number.\n","    overlap:               The overlap between adjacent sections. This was chosen when creating the dataset. For our dataset, overlap=12.\n","    num_output_channels:          Number of channels. If >1, a sliding window is used to place adjacent in each example. For example, if num_output_channels=3, the Nth training example will have 3 channels.\n","                               The channel 0 will contain the N-1 slice, channel 1 the Nth slice, and channel 2 the N+1 slice. This is usually done for sinograms.\n","    shift_x:               Shift data left or right. Only used on images to align reconstructions with ground truths.\n","    shift_y:               Shift data up or down. Only used on images\n","    crop_size:             Crop data. Used on images to ensure ground truths & reconstructions are the same size. Set to -1 to not crop.\n","    resize_size:           Resize data. Used to resize reconstructions so that they are the same size as the ground truths maps.\n","    '''\n","\n","    for section_num in range(start_section_num, end_section_num+1): # loop over the phantom sections. Remember, Python ends a for loop at one less than (end_section+1).\n","\n","        # Construct header filename\n","        data_section_header_filename =os.path.splitext(data_filename_stem)[0] +str(section_num) +os.path.splitext(data_filename_stem)[1] # filename = stem + section_num + extension\n","\n","        # Grab the data sections. Returned header dictionaries are discarded.\n","        h, section_arr = read_interfile_data(load_dir_path, data_section_header_filename, exclude_slices=(0,0), numpy=True, shift_x=shift_x, shift_y=shift_y,\n","                                             crop_size=crop_size, resize_size=resize_size)\n","\n","        # If necessary, add channels by stacking slices (usually done for sinograms)\n","        start_idx = int(overlap/2)+1\n","        end_idx = section_arr.shape[0]-int(overlap/2)\n","\n","        if num_output_channels==1:\n","            section_arr = section_arr[start_idx:end_idx+1,:]\n","        else:\n","            section_arr = make_slice_stack(section_arr, start_idx, end_idx, num_output_channels)\n","\n","        if section_num==start_section_num:\n","            phantom_array = section_arr\n","        else:\n","            phantom_array=np.append(phantom_array, section_arr, axis=0)\n","\n","        if visualize==True:\n","            section_tensor = torch.from_numpy(section_arr)\n","            print('data_filename_stem : ', data_filename_stem)\n","            print('section_num: ', section_num)\n","            print('section shape: ', section_tensor.shape)\n","            show_single_unmatched_tensor(section_tensor)\n","\n","    return phantom_array\n","\n","'''\n","## Changeable Variables ##\n","load_dir_path='./dataset-males/XCAT_male_pt77'  # end_section = 16\n","map_stem='act_map_toGATE_.h33'\n","start_section_num=0\n","end_section_num=16\n","overlap=12    # For our dataset, the overlap is 12\n","num_output_channels=1\n","\n","#load_dir_path='./dataset-males/XCAT_male_pt80' # end_section=15\n","#load_dir_path='./dataset-males/XCAT_male_pt92' # end_section=15\n","\n","map_array = compile_single_data_type(load_dir_path, map_stem, start_section_num, end_section_num, overlap=overlap, num_output_channels=num_output_channels, shift_x=0, shift_y=0, crop_size=-1, resize_size=-1,\n","                             normalize_scale=-1, visualize=False)\n","vis_tensor=torch.from_numpy(map_array[0:8])\n","show_single_unmatched_tensor(vis_tensor, fig_size=15)\n","\n","\n","\n","map_array = compile_single_data_type(load_dir_path, map_stem, start_section_num, end_section_num, overlap=overlap, num_output_channels=3, shift_x=0, shift_y=0, crop_size=-1, resize_size=-1,\n","                             normalize_scale=-1, visualize=False)\n","vis_tensor=torch.from_numpy(map_array[0:3])\n","show_single_unmatched_tensor(vis_tensor, fig_size=15)\n","\n","#show_single_commonmap_tensor(torch.from_numpy(map_array), nrow=35, figsize=(35,47), cmap='inferno')\n","'''"]},{"cell_type":"markdown","metadata":{"id":"0uwkooxCPuIO"},"source":["## Make Single Phantom Files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YL3kP2_OPrKG","outputId":"dc9d704f-53f7-4316-c25f-b167f51de95c","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1759891265209,"user_tz":300,"elapsed":240,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n## Changeable Variables ##\\nload_dir_path='./dataset-males/XCAT_male_pt77'  # end_section = 16\\n#load_dir_path='./dataset-males/XCAT_male_pt80' # end_section=15\\n#load_dir_path='./dataset-males/XCAT_male_pt92' # end_section=15\\nstart_section_num=0\\nend_section_num=16\\noverlap=12    # For our dataset, the overlap is 12\\nnum_sino_channels=5\\nnormalize_scale=-1\\nvisualize=False\\nsave_dir_path='./compiled'\\nsave_prefix='male_pt77'\\n\\nlowCountSino, medCountSino, highCountSino, lowCountImage, medCountImage, highCountImage, obliqueImage, anniMap, attenMap, actMap = make_single_phantom_files(\\n                                load_dir_path, start_section_num, end_section_num, overlap, num_sino_channels,\\n                                normalize_scale=normalize_scale, visualize=visualize, save_dir_path=save_dir_path, save_prefix=save_prefix)\\n\\nprint('low/med/high count sinograms')\\nprint(lowCountSino.shape)\\nprint(medCountSino.shape)\\nprint(highCountSino.shape)\\n#show_single_commonmap_tensor(torch.from_numpy(highCountSino), nrow=47, figsize=(47,47), cmap='inferno')\\n#show_single_unmatched_tensor(torch.from_numpy(highCountSino[500:502]), fig_size=40)\\n\\nprint('low/med/high count images')\\nprint(lowCountImage.shape)\\nprint(medCountImage.shape)\\nprint(highCountImage.shape)\\n#show_single_commonmap_tensor(torch.from_numpy(highCountImage), nrow=47, figsize=(47,47), cmap='inferno')\\n#show_single_unmatched_tensor(torch.from_numpy(highCountImage[200:205]), fig_size=3)\\n\\nprint('oblique image')\\nprint(obliqueImage.shape)\\n#show_single_commonmap_tensor(torch.from_numpy(obliqueImage), nrow=47, figsize=(47,47), cmap='inferno')\\n#show_single_unmatched_tensor(torch.from_numpy(obliqueImage[200:205]), fig_size=3)\\n\\nprint('anni/atten/act maps')\\nprint(anniMap.shape)\\nprint(attenMap.shape)\\nprint(actMap.shape)\\n\\n#show_single_commonmap_tensor(torch.from_numpy(attenMap), nrow=40, figsize=(47,47), cmap='inferno')\\n#show_single_commonmap_tensor(torch.from_numpy(actMap), nrow=47, figsize=(35,47), cmap='inferno')\\n#show_single_unmatched_tensor(torch.from_numpy(actMap[200:205]), fig_size=3)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["def make_single_phantom_files(load_dir_path, start_section_num, end_section_num, overlap, num_sino_channels=5, normalize_scale=-1, visualize=False,\n","            rebin_lowCountSino_stem='rebin_lowCountSino_.hv', rebin_medCountSino_stem='rebin_medCountSino_.hv', rebin_highCountSino_stem='rebin_highCountSino_.hv',\n","            rebin_lowCountImage_stem='rebin_lowCountImage_.hv', rebin_medCountImage_stem='rebin_medCountImage_.hv', rebin_highCountImage_stem='rebin_highCountImage_.hv',\n","            oblique_image_stem='oblique_image_.hv', anni_map_stem='anni_map_fromGATE_.h33', atten_map_stem='atten_map_fromGATE_.h33', act_map_stem='act_map_toGATE_.h33',\n","            save_types=[False,False,True,False,False,True,True,True,True,True], save_dir_path='./', phantom_label=''):\n","    '''\n","    This function loads and compiles each section of a phantom for each datatype (maps, sinograms, etc.).\n","    By altering the code below, one can alter the shift_x, shift_y, crop_size & resize_size for each data type. This allows one to make sure\n","    that the reconstructed images line up perfectly on top of the attenuation map, annihilation map, and activity maps. Values that we hardcoded\n","    in this function work for our dataset.\n","\n","    load_dir_path:             path to the directory with the data files.\n","    start_section_num:         the lowest numbered section that you will be concatenating.\n","    end_section_num:           the highest numbered section that you will be concatenating.\n","    overlap:                   overlap between sections. Note: slices are left alone on the first and last section because there is no overlap at the ends of the phantom.\n","    num_sino_channels:         number of channels for the stacked sinogram arrays.\n","    normalize_scale:           normalize each slice to this number. Set to -1 to not normalize.\n","    visualize:                 Set to True to visualize the phantom sections as they compile.\n","    rebin_lowCountSino_stem:   filename stem for the low-count rebinned sinograms. The sinogram section names will\n","                               consist of this stem with a number added to the end of the filename (but before\n","                               the extension). For example, if rebin_sino_stem='rebinned_sino.hs', the\n","                               section names will go from \"rebinned_sino{start_section_num}.hs\" to 'rebinned_sino{end_sections_num}.hs'\n","    rebin_medCountSino_stem    filename stem for the medium-count rebinned sinograms.\n","    rebin_highCountSino_stem   filename stem for the high-count rebinned sinograms.\n","    rebin_lowCountImage_stem:  filename stem for PET image reconstructed from the low-count rebinned sinogram\n","    rebin_medCountImage_stem:  filename stem for PET image reconstructed from the medium-count rebinned sinogrammax_axial_index\n","    rebin_highCountImage_stem: filename stem for PET image reconstructed from the high-count rebinned sinogram\n","    oblique_image_stem:        filename stem for images reconstructed from the (4-D) oblique sinograms\n","    anni_map_stem:             filename stem for the annihilation maps\n","    atten_map_stem:            filename stem for the attenuation maps\n","    act_map_stem:              filename stem for the activity maps\n","    save_types:                a list of boolean values corresponding to which data types to save. Order starts with low count sinograms and ends with activity maps (as above).\n","    save_dir_path              path to the save directory\n","    phantom_label:             a prefix to add to the front of each compiled section. This will usually be something like \"male_pt77\" or \"female_pt80\", etc.\n","    '''\n","\n","    lowCountImage =compile_single_data_type(load_dir_path, rebin_lowCountImage_stem, start_section_num, end_section_num, overlap, 1,\n","                                            shift_x=0, shift_y=0, crop_size=251, resize_size=180,\n","                                            normalize_scale=normalize_scale, visualize=visualize)\n","    medCountImage =compile_single_data_type(load_dir_path, rebin_medCountImage_stem, start_section_num, end_section_num, overlap, 1,\n","                                            shift_x=0, shift_y=0, crop_size=251, resize_size=180,\n","                                            normalize_scale=normalize_scale, visualize=visualize)\n","    highCountImage=compile_single_data_type(load_dir_path, rebin_highCountImage_stem,start_section_num, end_section_num, overlap, 1,\n","                                            shift_x=0, shift_y=0, crop_size=251, resize_size=180,\n","                                            normalize_scale=normalize_scale, visualize=visualize)\n","    obliqueImage  =compile_single_data_type(load_dir_path, oblique_image_stem,       start_section_num, end_section_num, overlap, 1,\n","                                            shift_x=0, shift_y=0, crop_size=251, resize_size=180,\n","                                            normalize_scale=normalize_scale, visualize=visualize)\n","\n","    lowCountSino  =compile_single_data_type(load_dir_path, rebin_lowCountSino_stem,  start_section_num, end_section_num, overlap, num_sino_channels,\n","                                            shift_x=0, shift_y=0, crop_size=-1, resize_size=-1,\n","                                            normalize_scale=normalize_scale, visualize=visualize)\n","    medCountSino  =compile_single_data_type(load_dir_path, rebin_medCountSino_stem,  start_section_num, end_section_num, overlap, num_sino_channels,\n","                                            shift_x=0, shift_y=0, crop_size=-1, resize_size=-1,\n","                                            normalize_scale=normalize_scale, visualize=visualize)\n","    highCountSino =compile_single_data_type(load_dir_path, rebin_highCountSino_stem, start_section_num, end_section_num, overlap, num_sino_channels,\n","                                            shift_x=0, shift_y=0, crop_size=-1, resize_size=-1,\n","                                            normalize_scale=normalize_scale, visualize=visualize)\n","\n","    anniMap       =compile_single_data_type(load_dir_path, anni_map_stem,  start_section_num, end_section_num, overlap, 1,\n","                                            shift_x=0, shift_y=0, crop_size=-1, resize_size=-1,\n","                                            normalize_scale=normalize_scale, visualize=visualize)\n","    attenMap      =compile_single_data_type(load_dir_path, atten_map_stem, start_section_num, end_section_num, overlap, 1,\n","                                            shift_x=0, shift_y=0, crop_size=-1, resize_size=-1,\n","                                            normalize_scale=normalize_scale,visualize=visualize)\n","    # The activity map must be aligned with atten_map and anni_map, because while the latter are output by GATE the former is output by STIR. The two do not exactly align.\n","    actMap        =compile_single_data_type(load_dir_path, act_map_stem,   start_section_num, end_section_num, overlap, 1,\n","                                            shift_x=-4, shift_y=5, crop_size=-1, resize_size=-1,\n","                                            normalize_scale=normalize_scale, visualize=visualize)\n","\n","    # Determine first and last indexes which have nonzero ground truth activity\n","    start_idx, end_idx = first_last_nonzero(actMap)\n","\n","    # Slice off ends of arrays with zero ground truth activity.\n","    print('Slicing off ends of with zero activity')\n","    lowCountSino = lowCountSino[start_idx:end_idx+1,:]\n","    medCountSino = medCountSino[start_idx:end_idx+1,:]\n","    highCountSino = highCountSino[start_idx:end_idx+1,:]\n","\n","    lowCountImage = lowCountImage[start_idx:end_idx+1,:]\n","    medCountImage = medCountImage[start_idx:end_idx+1,:]\n","    highCountImage= highCountImage[start_idx:end_idx+1,:]\n","    obliqueImage  = obliqueImage[start_idx:end_idx+1,:]\n","\n","    anniMap =       anniMap[start_idx:end_idx+1,:]\n","    attenMap =      attenMap[start_idx:end_idx+1,:]\n","    actMap =        actMap[start_idx:end_idx+1,:]\n","\n","    print('Saving files')\n","    # Construct the beginning of the path, including the file prefix\n","    save_begin=save_dir_path+'/'+phantom_label+'-'\n","\n","    if save_types[0]==True:\n","        np.save(save_begin+'lowCountSino', lowCountSino)\n","    if save_types[1]==True:\n","        np.save(save_begin+'medCountSino', medCountSino)\n","    if save_types[2]==True:\n","        np.save(save_begin+'highCountSino', highCountSino)\n","    if save_types[3]==True:\n","        np.save(save_begin+'lowCountImage', lowCountImage)\n","    if save_types[4]==True:\n","        np.save(save_begin+'medCountImage', medCountImage)\n","    if save_types[5]==True:\n","        np.save(save_begin+'highCountImage', highCountImage)\n","    if save_types[6]==True:\n","        np.save(save_begin+'obliqueImage', obliqueImage)\n","    if save_types[7]==True:\n","        np.save(save_begin+'anniMap', anniMap)\n","    if save_types[8]==True:\n","        np.save(save_begin+'attenMap', attenMap)\n","    if save_types[9]==True:\n","        np.save(save_begin+'actMap', actMap)\n","\n","    return lowCountSino, medCountSino, highCountSino, lowCountImage, medCountImage, highCountImage, obliqueImage, anniMap, attenMap, actMap\n","\n","'''\n","## Changeable Variables ##\n","load_dir_path='./dataset-males/XCAT_male_pt77'  # end_section = 16\n","#load_dir_path='./dataset-males/XCAT_male_pt80' # end_section=15\n","#load_dir_path='./dataset-males/XCAT_male_pt92' # end_section=15\n","start_section_num=0\n","end_section_num=16\n","overlap=12    # For our dataset, the overlap is 12\n","num_sino_channels=5\n","normalize_scale=-1\n","visualize=False\n","save_dir_path='./compiled'\n","save_prefix='male_pt77'\n","\n","lowCountSino, medCountSino, highCountSino, lowCountImage, medCountImage, highCountImage, obliqueImage, anniMap, attenMap, actMap = make_single_phantom_files(\n","                                load_dir_path, start_section_num, end_section_num, overlap, num_sino_channels,\n","                                normalize_scale=normalize_scale, visualize=visualize, save_dir_path=save_dir_path, save_prefix=save_prefix)\n","\n","print('low/med/high count sinograms')\n","print(lowCountSino.shape)\n","print(medCountSino.shape)\n","print(highCountSino.shape)\n","#show_single_commonmap_tensor(torch.from_numpy(highCountSino), nrow=47, figsize=(47,47), cmap='inferno')\n","#show_single_unmatched_tensor(torch.from_numpy(highCountSino[500:502]), fig_size=40)\n","\n","print('low/med/high count images')\n","print(lowCountImage.shape)\n","print(medCountImage.shape)\n","print(highCountImage.shape)\n","#show_single_commonmap_tensor(torch.from_numpy(highCountImage), nrow=47, figsize=(47,47), cmap='inferno')\n","#show_single_unmatched_tensor(torch.from_numpy(highCountImage[200:205]), fig_size=3)\n","\n","print('oblique image')\n","print(obliqueImage.shape)\n","#show_single_commonmap_tensor(torch.from_numpy(obliqueImage), nrow=47, figsize=(47,47), cmap='inferno')\n","#show_single_unmatched_tensor(torch.from_numpy(obliqueImage[200:205]), fig_size=3)\n","\n","print('anni/atten/act maps')\n","print(anniMap.shape)\n","print(attenMap.shape)\n","print(actMap.shape)\n","\n","#show_single_commonmap_tensor(torch.from_numpy(attenMap), nrow=40, figsize=(47,47), cmap='inferno')\n","#show_single_commonmap_tensor(torch.from_numpy(actMap), nrow=47, figsize=(35,47), cmap='inferno')\n","#show_single_unmatched_tensor(torch.from_numpy(actMap[200:205]), fig_size=3)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"Bx_hRaS1-LFx"},"source":["## Make Multiple Phantom Files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1759891265212,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"},"user_tz":300},"id":"xqT2DqXClktC","outputId":"cae96da8-85a4-470c-b35d-f57ebeeb40b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ntrain_codes=['male_pt96', 'male_pt108', 'male_pt118', 'male_pt139', 'male_pt141', 'male_pt144', 'male_pt145', 'male_pt155', 'male_pt157', 'male_pt163', 'male_pt164', 'male_pt167', 'male_pt169', 'male_pt171', 'male_pt178', 'male_pt180', 'male_pt184', 'male_pt196', 'male_pt200', 'male_pt168', 'female_pt71', 'female_pt76', 'female_pt86', 'female_pt89', 'female_pt98', 'female_pt99', 'female_pt117', 'female_pt152', 'female_pt153', 'female_pt166', 'female_pt175', 'female_pt176', 'female_pt182', 'female_pt170']\\nvalidation_codes=['male_pt77', 'male_pt80', 'male_pt92', 'male_pt150', 'male_pt154', 'male_pt173', 'female_pt140', 'female_pt142', 'female_pt143', 'female_pt151', 'female_pt162']\\ntest_codes=['male_pt93', 'male_pt128', 'male_pt159', 'male_pt201', 'female_pt106', 'female_pt147', 'female_pt149']\\n\\nphantoms_dir_path='./dataset-XCAT/'\\nphantom_dir_prefix='XCAT_'\\nsave_dir_path='./compiled-test'\\nnormalize_scale=1\\nnum_sino_channels=3\\nsave_types=[False,False,True,False,False,True,True,True,True,True]\\n\\n\\nphantom_list=test_codes\\nmake_multiple_phantom_files(phantoms_dir_path, phantom_list, phantom_dir_prefix=phantom_dir_prefix,\\n                                      start_section_num=0, overlap=12, num_sino_channels=num_sino_channels, normalize_scale=-1,\\n                                      save_types=save_types, save_dir_path=save_dir_path)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}],"source":["def make_multiple_phantom_files(phantoms_dir_path, phantom_list, phantom_dir_prefix='XCAT_',\n","                                          start_section_num=0, overlap=12, num_sino_channels=5, normalize_scale=-1,\n","                                          save_types=[False,False,True,False,False,True,True,True,True,True], save_dir_path='./'):\n","    '''\n","    Construct and save numpy files for multiple phantoms (training set, test set, etc.) Arguments are defined as before, with the following new ones:\n","\n","    phantoms_dir_path:       Path to the directory containing the phantoms directories. This is different than the load_dir_path, which would be a path to the files\n","                             within an individual phantom (in other words, the path to a single phantom directory).\n","    phantom_list:            List of the names of the phantom directories (without prefix) for which to construct the files.\n","    phantom_dir_prefix:      A prefix to place before entries in phantom_list which gives the full name of the phantom directories. This will usually be 'XCAT_'\n","    '''\n","\n","    for phantom_name in phantom_list:\n","\n","        phantom_dir_name=phantom_dir_prefix+phantom_name\n","        load_dir_path=phantoms_dir_path+'/'+phantom_dir_name\n","        start_section_num=start_section_num\n","\n","        ## Determine start_section_num and end_section_num for a single phantom\n","        end_section_num=max_trailing_integer(load_dir_path)\n","\n","        print('Phantom: ', phantom_dir_name)\n","        print('end_section_num: ', end_section_num)\n","\n","        # Return numpy data files. We don't save these here because they are saved in construct_single_phantom_files()\n","\n","        lowCountSino, medCountSino, highCountSino, lowCountImage, medCountImage, highCountImage, obliqueImage, anniMap, attenMap, actMap = make_single_phantom_files(\n","                                load_dir_path, start_section_num, end_section_num, overlap, num_sino_channels, normalize_scale=normalize_scale, visualize=False,\n","                                save_types=save_types, save_dir_path=save_dir_path, phantom_label=phantom_name)\n","\n","## Changeable Variables ##\n","'''\n","train_codes=['male_pt96', 'male_pt108', 'male_pt118', 'male_pt139', 'male_pt141', 'male_pt144', 'male_pt145', 'male_pt155', 'male_pt157', 'male_pt163', 'male_pt164', 'male_pt167', 'male_pt169', 'male_pt171', 'male_pt178', 'male_pt180', 'male_pt184', 'male_pt196', 'male_pt200', 'male_pt168', 'female_pt71', 'female_pt76', 'female_pt86', 'female_pt89', 'female_pt98', 'female_pt99', 'female_pt117', 'female_pt152', 'female_pt153', 'female_pt166', 'female_pt175', 'female_pt176', 'female_pt182', 'female_pt170']\n","validation_codes=['male_pt77', 'male_pt80', 'male_pt92', 'male_pt150', 'male_pt154', 'male_pt173', 'female_pt140', 'female_pt142', 'female_pt143', 'female_pt151', 'female_pt162']\n","test_codes=['male_pt93', 'male_pt128', 'male_pt159', 'male_pt201', 'female_pt106', 'female_pt147', 'female_pt149']\n","\n","phantoms_dir_path='./dataset-XCAT/'\n","phantom_dir_prefix='XCAT_'\n","save_dir_path='./compiled-test'\n","normalize_scale=1\n","num_sino_channels=3\n","save_types=[False,False,True,False,False,True,True,True,True,True]\n","\n","\n","phantom_list=test_codes\n","make_multiple_phantom_files(phantoms_dir_path, phantom_list, phantom_dir_prefix=phantom_dir_prefix,\n","                                      start_section_num=0, overlap=12, num_sino_channels=num_sino_channels, normalize_scale=-1,\n","                                      save_types=save_types, save_dir_path=save_dir_path)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"hEnN97KglktC"},"source":["## Make Datasets from Multiple Phantoms"]},{"cell_type":"code","source":["from pathlib import Path\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","from google.colab import drive\n","import time\n","\n","def compile_phantoms_chunked(save_dir_path, save_name, load_dir_path, phantom_list, chunk_size=None, use_colab=True):\n","    \"\"\"\n","    Stitch multiple phantom .npy files into single .npy files on Google Drive (chunked, low RAM).\n","    Does everything on google drive so may be a bit slower.\n","\n","    Args:\n","        save_dir_path: path to folder to save compiled arrays\n","        save_name: base name for output files\n","        load_dir_path: path to folder containing phantom .npy files\n","        phantom_list: list of phantom prefixes (without file type or .npy)\n","        chunk_size: optional number of examples to copy at a time (reduces RAM)\n","        use_colab: if True, mount Google Drive\n","    \"\"\"\n","    load_dir_path = Path(load_dir_path)\n","    save_dir_path = Path(save_dir_path)\n","    save_dir_path.mkdir(parents=True, exist_ok=True)\n","\n","    if use_colab:\n","        drive.mount('/content/drive', force_remount=True)\n","\n","    # Detect file types from the first phantom\n","    first_phantom = phantom_list[0]\n","    detected_types = []\n","    for fpath in load_dir_path.glob(f\"{first_phantom}-*.npy\"):\n","        fname = fpath.name\n","        type_str = fname[len(first_phantom)+1:-4]\n","        detected_types.append(type_str)\n","    file_types = sorted(detected_types)\n","    print(f\"Detected file types: {file_types}\")\n","\n","    for ftype in file_types:\n","        final_file_path = save_dir_path / f\"{save_name}-{ftype}.npy\"\n","\n","        if final_file_path.exists():\n","            print(f\"[SKIPPED] '{final_file_path}' already exists\")\n","            continue\n","\n","        # Step 1: compute final shape and dtype\n","        total_examples = 0\n","        base_shape = None\n","        dtype = None\n","        files = sorted(load_dir_path.glob(f\"*-{ftype}.npy\"))\n","        for f in files:\n","            arr = np.load(f, mmap_mode=\"r\")\n","            total_examples += arr.shape[0]\n","            if base_shape is None:\n","                base_shape = arr.shape[1:]\n","                dtype = arr.dtype\n","            else:\n","                if arr.shape[1:] != base_shape:\n","                    raise ValueError(f\"All arrays for '{ftype}' must match shape {base_shape} along axes 1+\")\n","                if arr.dtype != dtype:\n","                    raise ValueError(f\"All arrays for '{ftype}' must have same dtype {dtype}\")\n","            del arr\n","\n","        final_shape = (total_examples,) + base_shape\n","\n","        # Step 2: create memmap directly on Google Drive\n","        output = np.lib.format.open_memmap(str(final_file_path), mode='w+', dtype=dtype, shape=final_shape)\n","\n","        # Step 3: copy each file in chunks\n","        offset = 0\n","        for f in tqdm(files, desc=f\"Stitching {ftype}\"):\n","            arr = np.load(f, mmap_mode=\"r\")\n","            n_examples = arr.shape[0]\n","\n","            # Determine chunk size\n","            if chunk_size is None:\n","                # Copy the whole file at once\n","                output[offset:offset+n_examples] = arr\n","            else:\n","                for start in range(0, n_examples, chunk_size):\n","                    end = min(start + chunk_size, n_examples)\n","                    output[offset+start:offset+end] = arr[start:end]\n","\n","            offset += n_examples\n","            del arr\n","\n","        output.flush()\n","        del output\n","        print(f\"[DONE] '{final_file_path}' saved with shape {final_shape}\")\n","\n","\n","\n","def compile_phantoms_memmap(save_dir_path, save_name, load_dir_path, phantom_list, use_colab=True, max_retries=3):\n","    \"\"\"\n","    Stitch multiple phantom .npy files into memmaps by file type, memory-efficiently,\n","    skipping already existing compiled files in save_dir_path, with robust Google Drive copy,\n","    and consistent progress display for skipped and processed file types.\n","\n","    Uses the VM to speed things along. Will be limited by drive space in the VM.\n","    The next function does everything on Google Drive (may be slower)\n","    \"\"\"\n","    load_dir_path = Path(load_dir_path)\n","    save_dir_path = Path(save_dir_path)\n","    save_dir_path.mkdir(parents=True, exist_ok=True)\n","\n","    # Mount Google Drive if using Colab\n","    if use_colab:\n","        drive.mount('/content/drive', force_remount=True)\n","        work_output_dir = Path(\"/content/tmp_compiled\")\n","        work_output_dir.mkdir(parents=True, exist_ok=True)\n","    else:\n","        work_output_dir = save_dir_path\n","        work_output_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Detect file types from the first phantom\n","    first_phantom = phantom_list[0]\n","    detected_types = []\n","    for fpath in load_dir_path.glob(f\"{first_phantom}-*.npy\"):\n","        fname = fpath.name\n","        type_str = fname[len(first_phantom)+1:-4]  # strip \"phantom-\" prefix and \".npy\"\n","        detected_types.append(type_str)\n","    file_types = sorted(detected_types)\n","    print(f\"Detected file types: {file_types}\")\n","\n","    # Helper function to copy with retry\n","    def copy_with_retry(src_path, dst_path, retries=max_retries):\n","        for attempt in range(retries):\n","            try:\n","                buffer_size = 1024 * 1024\n","                total_size = os.path.getsize(src_path)\n","                with open(src_path, \"rb\") as src, open(dst_path, \"wb\") as dst:\n","                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"Copying {dst_path.name}\") as pbar:\n","                        while True:\n","                            buf = src.read(buffer_size)\n","                            if not buf:\n","                                break\n","                            dst.write(buf)\n","                            pbar.update(len(buf))\n","                return\n","            except OSError as e:\n","                print(f\"Copy attempt {attempt+1} failed: {e}\")\n","                if attempt < retries - 1:\n","                    print(\"Retrying in 5 seconds...\")\n","                    time.sleep(5)\n","                else:\n","                    raise e\n","\n","    # Process each file type\n","    for ftype in file_types:\n","        final_file_path = save_dir_path / f\"{save_name}-{ftype}.npy\"\n","\n","        if final_file_path.exists():\n","            with tqdm(total=1, desc=f\"Skipping {ftype}\", leave=False) as pbar:\n","                pbar.update(1)\n","            print(f\"[SKIPPED] File '{final_file_path}' already exists\")\n","            continue\n","\n","        # Gather all files of this type\n","        files = sorted(load_dir_path.glob(f\"*-{ftype}.npy\"))\n","\n","        # Step 1: determine total size and shape\n","        total_examples = 0\n","        base_shape = None\n","        dtype = None\n","        for f in files:\n","            arr = np.load(f, mmap_mode=\"r\")\n","            total_examples += arr.shape[0]\n","            if base_shape is None:\n","                base_shape = arr.shape[1:]\n","                dtype = arr.dtype\n","            else:\n","                if arr.shape[1:] != base_shape:\n","                    raise ValueError(f\"All arrays for type '{ftype}' must match shape {base_shape} along axes 1+\")\n","                if arr.dtype != dtype:\n","                    raise ValueError(f\"All arrays for type '{ftype}' must have the same dtype {dtype}\")\n","            del arr\n","\n","        # Step 2: create memmap in VM\n","        final_shape = (total_examples,) + base_shape\n","        memmap_path = work_output_dir / f\"{save_name}-{ftype}.npy\"\n","        output = np.lib.format.open_memmap(str(memmap_path), mode=\"w+\", dtype=dtype, shape=final_shape)\n","\n","        # Step 3: fill memmap one file at a time\n","        offset = 0\n","        for f in tqdm(files, desc=f\"Stitching {ftype}\"):\n","            arr = np.load(f, mmap_mode=\"r\")\n","            n = arr.shape[0]\n","            output[offset:offset+n] = arr\n","            offset += n\n","            del arr\n","\n","        # Step 4: flush and close memmap\n","        output.flush()\n","        del output\n","        print(f\"[DONE] Completed '{ftype}' with shape {final_shape}\")\n","\n","        # Step 5: copy memmap to Drive if in Colab\n","        if use_colab:\n","            copy_with_retry(memmap_path, final_file_path)\n","\n","            # Step 6: remove local VM file to free space\n","            os.remove(memmap_path)\n","            print(f\"[CLEANUP] Deleted local memmap '{memmap_path}' to free VM space\")\n","        else:\n","            # On local system, just rename/move memmap to final path\n","            memmap_path.rename(final_file_path)\n","\n","\n","\n","train_codes=['male_pt96', 'male_pt108', 'male_pt118', 'male_pt139', 'male_pt141', 'male_pt144', 'male_pt145', 'male_pt155', 'male_pt157', 'male_pt163', 'male_pt164', 'male_pt167', 'male_pt169', 'male_pt171', 'male_pt178', 'male_pt180', 'male_pt184', 'male_pt196', 'male_pt200', 'male_pt168', 'female_pt71', 'female_pt76', 'female_pt86', 'female_pt89', 'female_pt98', 'female_pt99', 'female_pt117', 'female_pt152', 'female_pt153', 'female_pt166', 'female_pt175', 'female_pt176', 'female_pt182', 'female_pt170']\n","test_codes=['male_pt77', 'male_pt80', 'male_pt92', 'male_pt150', 'male_pt154', 'male_pt173', 'female_pt140', 'female_pt142', 'female_pt143', 'female_pt151', 'female_pt162']\n","val_codes=['male_pt93', 'male_pt128', 'male_pt159', 'male_pt201', 'female_pt106', 'female_pt147', 'female_pt149']\n","short_codes =['male_pt96', 'male_pt108']\n","\n","save_dir_path = '/content/drive/MyDrive/Colab/Working/dataset/sets/'\n","\n","\n","load_dir_path = '/content/drive/MyDrive/Colab/Working/dataset/phantoms-test'\n","save_name = \"test\"\n","phantom_list = test_codes\n","\n","compile_phantoms_memmap(save_dir_path, save_name, load_dir_path, phantom_list, use_colab=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KXmsZEzWwE6n","outputId":"7db68fe7-22d9-432a-d6ca-b490689930ed","executionInfo":{"status":"ok","timestamp":1759891621170,"user_tz":300,"elapsed":355957,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Detected file types: ['actMap', 'anniMap', 'attenMap', 'highCountImage', 'highCountSino', 'obliqueImage']\n"]},{"output_type":"stream","name":"stderr","text":["Stitching actMap: 100%|| 11/11 [00:13<00:00,  1.26s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[DONE] Completed 'actMap' with shape (5729, 1, 180, 180)\n"]},{"output_type":"stream","name":"stderr","text":["Copying test-actMap.npy: 100%|| 742M/742M [00:01<00:00, 444MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[CLEANUP] Deleted local memmap '/content/tmp_compiled/test-actMap.npy' to free VM space\n"]},{"output_type":"stream","name":"stderr","text":["Stitching anniMap: 100%|| 11/11 [00:15<00:00,  1.44s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[DONE] Completed 'anniMap' with shape (5729, 1, 180, 180)\n"]},{"output_type":"stream","name":"stderr","text":["Copying test-anniMap.npy: 100%|| 742M/742M [00:01<00:00, 453MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[CLEANUP] Deleted local memmap '/content/tmp_compiled/test-anniMap.npy' to free VM space\n"]},{"output_type":"stream","name":"stderr","text":["Stitching attenMap: 100%|| 11/11 [00:17<00:00,  1.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[DONE] Completed 'attenMap' with shape (5729, 1, 180, 180)\n"]},{"output_type":"stream","name":"stderr","text":["Copying test-attenMap.npy: 100%|| 742M/742M [00:01<00:00, 472MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[CLEANUP] Deleted local memmap '/content/tmp_compiled/test-attenMap.npy' to free VM space\n"]},{"output_type":"stream","name":"stderr","text":["Stitching highCountImage: 100%|| 11/11 [00:19<00:00,  1.77s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[DONE] Completed 'highCountImage' with shape (5729, 1, 180, 180)\n"]},{"output_type":"stream","name":"stderr","text":["Copying test-highCountImage.npy: 100%|| 742M/742M [00:01<00:00, 422MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[CLEANUP] Deleted local memmap '/content/tmp_compiled/test-highCountImage.npy' to free VM space\n"]},{"output_type":"stream","name":"stderr","text":["Stitching highCountSino: 100%|| 11/11 [02:15<00:00, 12.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[DONE] Completed 'highCountSino' with shape (5729, 3, 382, 513)\n"]},{"output_type":"stream","name":"stderr","text":["Copying test-highCountSino.npy: 100%|| 13.5G/13.5G [01:11<00:00, 189MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[CLEANUP] Deleted local memmap '/content/tmp_compiled/test-highCountSino.npy' to free VM space\n"]},{"output_type":"stream","name":"stderr","text":["Stitching obliqueImage: 100%|| 11/11 [00:35<00:00,  3.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["[DONE] Completed 'obliqueImage' with shape (5729, 1, 180, 180)\n"]},{"output_type":"stream","name":"stderr","text":["Copying test-obliqueImage.npy: 100%|| 742M/742M [00:02<00:00, 365MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[CLEANUP] Deleted local memmap '/content/tmp_compiled/test-obliqueImage.npy' to free VM space\n"]}]},{"cell_type":"code","source":["f = Path(\"/content/drive/MyDrive/Colab/Working/dataset/sets/train-highCountSino.npy\")\n","print(f.exists(), f.stat().st_size / 1e9, \"GB\")"],"metadata":{"id":"yc5ouOBoJ2DF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759891621206,"user_tz":300,"elapsed":34,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"outputId":"9b99df01-6af1-4de6-e0a4-8600dd5aad02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True 41.928885488 GB\n"]}]},{"cell_type":"markdown","source":["# Sample, Trim, Resize Dataset"],"metadata":{"id":"3eVunZY89vmm"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torchvision.transforms as T\n","from tqdm import tqdm\n","import os\n","import json\n","import shutil\n","\n","import numpy as np\n","import torch\n","import torchvision.transforms as T\n","from tqdm import tqdm\n","import os\n","import json\n","import shutil\n","\n","def sample_trim_resize(\n","    input_dir,\n","    input_file,\n","    output_dir,\n","    output_file,\n","    sample_division=1,\n","    remove_n=0,\n","    new_height=None,\n","    new_width=None,\n","    seed=42,\n","    log_file='log-SampleTrimResize.txt',\n","    skip_if_exists_vm=False\n","):\n","    \"\"\"\n","    Sample, optionally remove slices, resize, and copy to Google Drive with monitoring.\n","    Automatically verifies that the copied file matches the VM file.\n","\n","    Args:\n","        input_dir: Path to input directory containing numpy file\n","        input_file: Name of input numpy file\n","        output_dir: Path to directory to save output (Google Drive)\n","        output_file: Name of output file\n","        sample_division: Keep every N-th slice\n","        remove_n: Number of slices to randomly remove before sampling\n","        new_height, new_width: Resize dimensions; if None, original used\n","        seed: Random seed for reproducibility\n","        log_file: Name of log file in current working directory\n","        skip_if_exists_vm: If True and the VM file exists, skip processing slices\n","    \"\"\"\n","\n","    # --- Clean filenames and paths ---\n","    input_file = input_file.lstrip('/')\n","    output_file = output_file.lstrip('/')\n","    input_path = os.path.join(input_dir, input_file)\n","    local_output_path = os.path.join(\"/content\", output_file)\n","    drive_output_path = os.path.join(output_dir, output_file)\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # --- Load input as memory-mapped array ---\n","    arr = np.load(input_path, mmap_mode='r')\n","    n_samples, channels, height, width = arr.shape\n","\n","    # --- RNG setup ---\n","    rng = np.random.default_rng(seed)\n","\n","    # --- Random slice removal ---\n","    all_indices = np.arange(n_samples)\n","    keep_mask = np.ones(n_samples, dtype=bool)\n","    if remove_n > 0:\n","        remove_idx = rng.choice(n_samples, size=remove_n, replace=False)\n","        keep_mask[remove_idx] = False\n","    remaining_idx = all_indices[keep_mask]\n","\n","    # --- Even sampling ---\n","    sampled_idx = remaining_idx[::sample_division]\n","    n_final = len(sampled_idx)\n","\n","    out_h = new_height if new_height is not None else height\n","    out_w = new_width if new_width is not None else width\n","\n","    # --- GPU device ---\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    # --- Prepare transform if resizing ---\n","    resize_transform = None\n","    if new_height is not None or new_width is not None:\n","        resize_transform = T.Resize((out_h, out_w), antialias=True)\n","\n","    # --- Process slices if VM file doesn't exist or skipping disabled ---\n","    if skip_if_exists_vm and os.path.exists(local_output_path):\n","        print(f\"VM file already exists: {local_output_path}. Skipping processing slices...\")\n","    else:\n","        result = np.lib.format.open_memmap(local_output_path, mode='w+', dtype=np.float32,\n","                                          shape=(n_final, channels, out_h, out_w))\n","        tqdm_bar = tqdm(sampled_idx, desc=\"Processing slices\")\n","        for i, idx in enumerate(tqdm_bar):\n","            slice_i = arr[idx]  # [C,H,W]\n","            if resize_transform:\n","                slice_tensor = torch.from_numpy(slice_i).to(device)\n","                slice_tensor = resize_transform(slice_tensor)\n","                slice_i = slice_tensor.cpu().numpy().astype(np.float32)\n","            result[i] = slice_i\n","\n","        # Flush memory-mapped array\n","        print(\"Flushing memory-mapped array to disk...\")\n","        result.flush()\n","\n","    # --- Copy to Drive if paths differ ---\n","    if os.path.abspath(local_output_path) != os.path.abspath(drive_output_path):\n","        print(f\"Copying output to Drive: {drive_output_path}\")\n","        shutil.copy(local_output_path, drive_output_path)\n","    else:\n","        print(\"Source and destination are the same  skipping copy.\")\n","\n","    # --- Verify copied file ---\n","    vm_size = os.path.getsize(local_output_path)\n","    drive_size = os.path.getsize(drive_output_path)\n","\n","    vm_arr = np.load(local_output_path, mmap_mode='r')\n","    drive_arr = np.load(drive_output_path, mmap_mode='r')\n","\n","    if (vm_size == drive_size and\n","        vm_arr.shape == drive_arr.shape and\n","        vm_arr.dtype == drive_arr.dtype):\n","        print(f\"\\n File verification successful! VM and Drive files match.\")\n","        print(f\"   Shape: {vm_arr.shape}, dtype: {vm_arr.dtype}, size: {vm_size/1e6:.2f} MB\")\n","    else:\n","        print(\"\\n File verification failed! VM and Drive files differ.\")\n","        print(f\"   VM -> shape: {vm_arr.shape}, dtype: {vm_arr.dtype}, size: {vm_size/1e6:.2f} MB\")\n","        print(f\"   Drive -> shape: {drive_arr.shape}, dtype: {drive_arr.dtype}, size: {drive_size/1e6:.2f} MB\")\n","\n","    # --- Log everything ---\n","    run_entry = {\n","        \"input_dir\": input_dir,\n","        \"input_file\": input_file,\n","        \"output_dir\": output_dir,\n","        \"output_file\": output_file,\n","        \"sample_division\": sample_division,\n","        \"remove_n\": remove_n,\n","        \"seed\": seed,\n","        \"new_height\": new_height,\n","        \"new_width\": new_width,\n","        \"output_shape\": vm_arr.shape\n","    }\n","    log_path = os.path.join(os.getcwd(), log_file)\n","    with open(log_path, 'a') as f:\n","        f.write(json.dumps(run_entry) + \"\\n\")\n","\n","    print(f\"   Log updated: {os.path.abspath(log_file)}\")\n","\n","\n","\n","sample_trim_resize(\n","    input_dir='/content/drive/MyDrive/Colab/Working/dataset/sets',\n","    input_file='train-highCountSino-382x513.npy',\n","    output_dir='/content/drive/MyDrive/Colab/Working/dataset/sets',\n","    output_file='train-highCountSino-180x180.npy',\n","    sample_division=1,\n","    remove_n=0,\n","    new_height=180,\n","    new_width=180,\n","    seed=0,                # default seed for reproducibility\n","    log_file=\"log-SampleTrimResize.txt\"\n","    skip_if_exists_vm=False\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxR3_7Mu9sfF","executionInfo":{"status":"ok","timestamp":1760313091894,"user_tz":300,"elapsed":337997,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"outputId":"8533ca09-a0cf-4dc1-cbbd-539a513399d4"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["Processing slices: 100%|| 17830/17830 [04:52<00:00, 61.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Flushing memory-mapped array to disk...\n","Copying output to Drive: /content/drive/MyDrive/Colab/Working/dataset/sets/train-highCountSino-180x180.npy\n","\n"," File verification successful! VM and Drive files match.\n","   Shape: (17830, 3, 180, 180), dtype: float32, size: 6932.30 MB\n","   Log updated: /content/log-SampleTrimResize.txt\n"]}]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"TH0xWXSKoKsg"}},{"cell_type":"markdown","metadata":{"id":"XiS3Zoc0lktE"},"source":["# Investigate"]},{"cell_type":"markdown","metadata":{"id":"6KwHlCP8lktE"},"source":["## Look at Dataset Shapes"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"yifJUUcMlktF","executionInfo":{"status":"ok","timestamp":1760284540315,"user_tz":300,"elapsed":719,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f1757db-9631-470f-9748-0f84159c5ea2"},"outputs":[{"output_type":"stream","name":"stdout","text":["(5729, 3, 382, 513)\n"]}],"source":["## SET VARIABLES ##\n","\n","config={'SI_normalize':False, 'SI_scale':1}\n","exclude_slices=(5,5)\n","fig_size=4\n","num_cols=3\n","crop_size=260\n","resize_size=180\n","normalize_scale=1\n","\n","\n","load_dir_path = '/content/drive/MyDrive/Colab/Working/dataset/sets/'\n","set_name = 'test-highCountSino.npy'\n","load_dir = f\"{load_dir_path}{set_name}\"\n","large_array=np.load(load_dir, mmap_mode='r')\n","\n","print(large_array.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"T7c-R4RGlktE"},"source":["## Look at ROIs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6l_UaPDUlktE","colab":{"base_uri":"https://localhost:8080/","height":157},"executionInfo":{"status":"ok","timestamp":1759891621267,"user_tz":300,"elapsed":60,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"outputId":"c2fd9a49-1e12-4b88-cd24-a2e40d2ebadd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nheader_name_act2='PhantomAct2_act.h33'\\nheader_name_act4='PhantomAct4_act.h33'\\nheader_name_mask_background='PhantomMaskBackground17_act.h33'\\nheader_name_mask_hot='PhantomMaskHot17_act.h33'\\n\\nexclude_slices=(0,0)\\nmask_dir_path='../STIR-GATE-Connection-simplified/QAPhantoms/Masks/'\\nload_dir_path='../STIR-GATE-Connection-simplified/QAPhantoms/FinishedPhantoms/'\\n\\n\\nheader_act2, tensor_act2                      = read_interfile_data(load_dir_path, header_name_act2,            exclude_slices=exclude_slices)\\nheader_act4, tensor_act4                      = read_interfile_data(load_dir_path, header_name_act4,            exclude_slices=exclude_slices)\\nheader_mask_background, tensor_mask_background= read_interfile_data(mask_dir_path, header_name_mask_background, exclude_slices=exclude_slices)\\nheader_mask_hot, tensor_mask_hot              = read_interfile_data(mask_dir_path, header_name_mask_hot,        exclude_slices=exclude_slices)\\n\\nx=ROI_NEMA_hot(tensor_act2, tensor_act4, tensor_mask_background, tensor_mask_hot)\\n\\nprint(x)\\n\\n#show_single_unmatched_tensor(tensor_act2)\\n#show_single_commonmap_tensor(tensor_act2)\\n#show_single_unmatched_tensor(tensor_act4)\\n#show_single_commonmap_tensor(tensor_act4)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["'''\n","header_name_act2='PhantomAct2_act.h33'\n","header_name_act4='PhantomAct4_act.h33'\n","header_name_mask_background='PhantomMaskBackground17_act.h33'\n","header_name_mask_hot='PhantomMaskHot17_act.h33'\n","\n","exclude_slices=(0,0)\n","mask_dir_path='../STIR-GATE-Connection-simplified/QAPhantoms/Masks/'\n","load_dir_path='../STIR-GATE-Connection-simplified/QAPhantoms/FinishedPhantoms/'\n","\n","\n","header_act2, tensor_act2                      = read_interfile_data(load_dir_path, header_name_act2,            exclude_slices=exclude_slices)\n","header_act4, tensor_act4                      = read_interfile_data(load_dir_path, header_name_act4,            exclude_slices=exclude_slices)\n","header_mask_background, tensor_mask_background= read_interfile_data(mask_dir_path, header_name_mask_background, exclude_slices=exclude_slices)\n","header_mask_hot, tensor_mask_hot              = read_interfile_data(mask_dir_path, header_name_mask_hot,        exclude_slices=exclude_slices)\n","\n","x=ROI_NEMA_hot(tensor_act2, tensor_act4, tensor_mask_background, tensor_mask_hot)\n","\n","print(x)\n","\n","#show_single_unmatched_tensor(tensor_act2)\n","#show_single_commonmap_tensor(tensor_act2)\n","#show_single_unmatched_tensor(tensor_act4)\n","#show_single_commonmap_tensor(tensor_act4)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"t1G2PZiFlktD"},"source":["## Inspect Alignment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZV8c9EkplktD","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"error","timestamp":1759891621289,"user_tz":300,"elapsed":19,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"outputId":"5c8894f8-b79e-45a1-acd3-5d6ae6b8d9be"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"incomplete input (ipython-input-4250258164.py, line 52)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4250258164.py\"\u001b[0;36m, line \u001b[0;32m52\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"]}],"source":["## CHECK ALIGNMENT ##\n","'''\n","fig_size=3\n","start_index=400\n","end_index=403\n","mult_factor=100000\n","\n","anniMap=torch.from_numpy(np.load('./compiled/male_pt96-anniMap.npy', mmap_mode='r'))\n","actMap=torch.from_numpy(np.load('./compiled/male_pt96-actMap.npy', mmap_mode='r'))\n","attenMap=torch.from_numpy(np.load('./compiled/male_pt96-attenMap.npy', mmap_mode='r'))\n","\n","obliqueImage=torch.from_numpy(np.load('./compiled/male_pt96-obliqueImage.npy', mmap_mode='r'))\n","highCountSino=torch.from_numpy(np.load('./compiled/male_pt96-highCountSino.npy', mmap_mode='r'))\n","highCountImage=torch.from_numpy(np.load('./compiled/male_pt96-highCountImage.npy', mmap_mode='r'))\n","\n","print('####highCountSino')\n","print(highCountSino.size())\n","print(highCountSino.sum())\n","print('####highCountImage')\n","print(highCountImage.size())\n","print(highCountImage.sum()/180**2)\n","print('####obliqueImage')\n","print(obliqueImage.size())\n","print(obliqueImage.sum()/180**2)\n","print('####actMap')\n","print(actMap.size())\n","print(actMap.sum()/180**2)\n","print('####anniMap')\n","print(anniMap.size())\n","print(anniMap.sum()/180**2)\n","print('####attenMap')\n","print(attenMap.size())\n","print(attenMap.sum()/180**2)\n","\n","\n","\n","show_single_unmatched_tensor(highCountSino[1:4], fig_size=12)\n","\n","tensor_sum0=torch.add(actMap[start_index:end_index], mult_factor*highCountImage[start_index:end_index])\n","tensor_sum1=torch.add(actMap[start_index:end_index], mult_factor*obliqueImage[start_index:end_index])\n","tensor_sum2=torch.add(actMap[start_index:end_index], mult_factor*attenMap[start_index:end_index])\n","tensor_sum3=torch.add(actMap[start_index:end_index], mult_factor*anniMap[start_index:end_index])\n","tensor_sum4=torch.add(actMap[start_index:end_index], mult_factor*actMap[start_index:end_index])\n","\n","print('Lining Images Up on Top of Each Other')\n","print('=====================================')\n","show_single_unmatched_tensor(tensor_sum0, fig_size=fig_size)\n","show_single_unmatched_tensor(tensor_sum1, fig_size=fig_size)\n","show_single_unmatched_tensor(tensor_sum2, fig_size=fig_size)\n","show_single_unmatched_tensor(tensor_sum3, fig_size=fig_size)\n","show_single_unmatched_tensor(tensor_sum4, fig_size=fig_size)\n","'''"]},{"cell_type":"code","source":["## CHECK ALIGNMENT ##\n","fig_size=3\n","start_index=400\n","end_index=403\n","mult_factor=500\n","\n","actMapMM =np.load('/content/drive/MyDrive/Colab/Working/dataset/sets/train-actMap.npy', mmap_mode='r')\n","attenMapMM=np.load('/content/drive/MyDrive/Colab/Working/dataset/sets/train-attenMap.npy', mmap_mode='r')\n","\n","actMap=torch.from_numpy(actMapMM[start_index:end_index,:])\n","attenMap=torch.from_numpy(attenMapMM[start_index:end_index,:])\n","\n","print('####actMap')\n","print(actMap.size())\n","print(actMap.sum()/180**2)\n","print('####attenMap')\n","print(attenMap.size())\n","print(attenMap.sum()/180**2)\n","\n","tensor_sum0=torch.add(actMap, mult_factor*attenMap)\n","\n","print('Lining Images Up on Top of Each Other')\n","print('=====================================')\n","show_single_unmatched_tensor(tensor_sum0, fig_size=fig_size)"],"metadata":{"id":"okuIrdZ6NpkG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","import numpy as np\n","\n","# Path to the folder containing your compiled files\n","compiled_dir = Path(\"/content/drive/MyDrive/Colab/Working/dataset/sets\")\n","\n","# List all .npy compiled files\n","compiled_files = sorted(compiled_dir.glob(\"*.npy\"))\n","\n","# Open each file as a memmap and print its shape\n","for fpath in compiled_files:\n","    print(f\"File: {fpath.name}\")\n","    # open as memmap (read-only, minimal RAM usage)\n","    mm = np.load(fpath, mmap_mode='r')\n","    print(f\"  Shape: {mm.shape}, dtype: {mm.dtype}\")\n","\n","    show_single_unmatched_tensor(torch.from_numpy(mm[300:305]), fig_size=5)\n","    del mm  # release memmap"],"metadata":{"id":"50SJChk-MZIm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2w1mf8iLlktG"},"source":["## Visualize Interfiles"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wW6ND1UqlktH"},"outputs":[],"source":["## NOTES ##\n","# reconstruction shapes = 337x337\n","# sinogram shapes = 382x513\n","\n","## SET VARIABLES ##\n","\n","'''\n","config={'SI_normalize':False, 'SI_scale':1}\n","exclude_slices=5\n","fig_size=3\n","num_cols=3\n","normalize_scale=1\n","\n","dataset_path='./dataset-males/'\n","phantom_dir='XCAT_male_pt77' # 71 76 86 89 98 99 106 117 140 142 143 147\n","section_num='0'\n","\n","\n","## LOAD INTERFILES\n","phantom_path=os.path.join(dataset_path, phantom_dir)\n","\n","# The ground truth activity is input to GATE.\n","header_info, activity = read_interfile_data(phantom_path, 'act_map_toGATE_'+section_num+'.h33', exclude_slices=exclude_slices,\n","                                            shift_x=-4, shift_y=5, crop_size=-1, resize_size=-1, normalize_scale=normalize_scale)\n","# Attenuation is exported by GATE and input to STIR. I take this as the ground truth geometry.\n","header_info, atten =   read_interfile_data(phantom_path, 'atten_map_fromGATE_'+section_num+'.h33', exclude_slices=exclude_slices,\n","                                           shift_x=0, shift_y=0, crop_size=-1, resize_size=-1, normalize_scale=normalize_scale)\n","header_info, anni =   read_interfile_data(phantom_path, 'anni_map_fromGATE_'+section_num+'.h33', exclude_slices=exclude_slices,\n","                                           shift_x=0, shift_y=0, crop_size=-1, resize_size=-1, normalize_scale=normalize_scale)\n","\n","# Reconstructions are both cropped and resized but not shifted (since it is exported by STIR, it lines up with the attenuation map)\n","header_info, highCountImage = read_interfile_data(phantom_path, 'rebin_highCountImage_'+section_num+'.hv', exclude_slices=exclude_slices,\n","                                                  shift_x=0, shift_y=0, crop_size=251, resize_size=180, normalize_scale=normalize_scale)\n","header_info, oblique = read_interfile_data(phantom_path, 'oblique_image_'+section_num+'.hv', exclude_slices=exclude_slices,\n","                                           shift_x=0, shift_y=0, crop_size=251, resize_size=180, normalize_scale=normalize_scale)\n","\n","header_info, sino = read_interfile_data(phantom_path, 'rebin_highCountSino_'+section_num+'.hv', exclude_slices=exclude_slices,\n","                                        shift_x=0, shift_y=0, crop_size=251, resize_size=180, normalize_scale=-1)\n","\n","#reconstructed = reconstruct(sino[0:num_cols], config, image_size=resize_size, recon_type='MLEM', circle=False)\n","#reconstructed = reconstruct(sino[0:num_cols], config, image_size=resize_size, recon_type='FBP', circle=False)\n","\n","## CALCULATE METRICS ##\n","\n","MSE_highCountImage=calculate_metric(activity, highCountImage, MSE, return_dataframe=False, label='default', crop_factor=1)\n","MSE_oblique=calculate_metric(activity, oblique, MSE, return_dataframe=False, label='default', crop_factor=1)\n","\n","SSIM_rebin=calculate_metric(activity, highCountImage, SSIM, return_dataframe=False, label='default', crop_factor=1)\n","SSIM_oblique=calculate_metric(activity, oblique, SSIM, return_dataframe=False, label='default', crop_factor=1)\n","\n","print('MSE (rebin): ', MSE_highCountImage)\n","print('MSE (oblique): ', MSE_oblique)\n","print('SSIM (rebin): ', SSIM_rebin)\n","print('SSIM (oblique): ', SSIM_oblique)\n","\n","print('ATTENUATION')\n","show_single_unmatched_tensor(atten[0:num_cols], fig_size=fig_size)\n","print('GROUND TRUTH')\n","show_single_unmatched_tensor(activity[0:num_cols], fig_size=fig_size)\n","print('FORE RECON')\n","show_single_unmatched_tensor(highCountImage[0:num_cols], fig_size=fig_size)\n","print('OBLIQUE RECON')\n","show_single_unmatched_tensor(oblique[0:num_cols], fig_size=fig_size)\n","'''\n","'''\n","## CHECK ALIGNMENT ##\n","tensor_sum0=torch.add(anni[0:num_cols], atten[0:num_cols])\n","tensor_sum1=torch.add(activity[0:num_cols],atten[0:num_cols])\n","tensor_sum2=torch.add(activity[0:num_cols],2*highCountImage[0:num_cols])\n","tensor_sum3=torch.add(activity[0:num_cols],2*oblique[0:num_cols])\n","tensor_sum4=torch.add(oblique[0:num_cols],atten[0:num_cols])\n","#tensor_sum3=torch.add(activity[0:num_cols],reconstructed[0:num_cols])\n","\n","print('Lining Images Up on Top of Each Other')\n","print('=====================================')\n","show_single_unmatched_tensor(tensor_sum0, fig_size=fig_size)\n","show_single_unmatched_tensor(tensor_sum1, fig_size=fig_size)\n","show_single_unmatched_tensor(tensor_sum2, fig_size=fig_size)\n","show_single_unmatched_tensor(tensor_sum3, fig_size=fig_size)\n","show_single_unmatched_tensor(tensor_sum4, fig_size=fig_size)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"wqRdJRNYmpk6"},"source":["# Classes"]},{"cell_type":"markdown","metadata":{"id":"SR6WK0lIERty"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEAomee7EWjg"},"outputs":[],"source":["def NpArrayDataLoader(image_array, sino_array, config, image_size = 90, sino_size=90, image_channels=1, sino_channels=1, augment=False, index=0):\n","    '''\n","    Function to load an image and a sinogram. Returns 4 pytorch tensors: the original dataset sinogram and image,\n","    and scaled and normalized sinograms and images.\n","\n","    image_array:    image numpy array\n","    sino_array:     sinogram numpy array\n","    config:         configuration dictionary with hyperparameters\n","    image_size:     shape to resize image to (for output)\n","    image_channels: number of channels for output images\n","    sino_size:      shape to resize sinograms to (for output)\n","    sino_channels:  number of channels in output sinograms (for photopeak sinograms, this is 1)\n","    augment:        perform data augmentation?\n","    index:          index of the image/sinogram to grab\n","    '''\n","    ## Set Normalization Variables ##\n","    if (train_type=='GAN') or (train_type=='SUP'):\n","        if train_SI==True:\n","            SI_normalize=config['SI_normalize']\n","            SI_scale=config['SI_scale']\n","            IS_normalize=False     # If the Sinogram-->Image network (SI) is being trained, don't waste time normalizing sinograms\n","            IS_scale=1             # If the Sinogram-->Image network (SI) is being trained, don't waste time scaling sinograms\n","        else:\n","            IS_normalize=config['IS_normalize']\n","            IS_scale=config['IS_scale']\n","            SI_normalize=False\n","            SI_scale=1\n","    else:\n","        IS_normalize=config['IS_normalize']\n","        SI_normalize=config['SI_normalize']\n","        IS_scale=config['IS_scale']\n","        SI_scale=config['SI_scale']\n","\n","    ## Data Augmentation Functions ##\n","    def RandRotate(image_2D, sinogram_3D):\n","\n","        def IntersectCircularBorder(image):\n","            '''\n","            Function for determining whether an image itersects a circular boundary inscribed within the square FOV.\n","            This function is not currently used.\n","            '''\n","            y_max = image.shape[1]\n","            x_max = image.shape[2]\n","\n","            r_max = y_max/2.0\n","            x_center = (x_max-1)/2.0 # the -1 comes from the fact that the coordinates of a pixel start at 0, not 1\n","            y_center = (y_max-1)/2.0\n","\n","            margin_sum = 0\n","            for y in range(0, y_max):\n","                for x in range(0, x_max):\n","                    if r_max < ((x-x_center)**2 + (y-y_center)**2)**0.5 :\n","                        margin_sum += torch.sum(image[:,y,x]).item()\n","\n","            return_value = True if margin_sum == 0 else False\n","            return return_value\n","\n","        def IntersectSquareBorder(image):\n","            '''\n","            Function for determining whether the image intersects the edge of the square FOV. If it does not, then the image\n","            is fully specified by the sinogram, and data augmentation can be performed. If the image does\n","            intersect the edge of the image, then some of it may be cropped outside the FOV. In this case,\n","            augmentation via rotation should not be performed.\n","            '''\n","            max_idx = image.shape[1]-1\n","            margin_sum = torch.sum(image[:,0,:]).item() + torch.sum(image[:,max_idx,:]).item() \\\n","                        +torch.sum(image[:,:,0]).item() + torch.sum(image[:,:,max_idx]).item()\n","            return_value = False if margin_sum == 0 else True\n","            return return_value\n","\n","        if IntersectSquareBorder(image_2D) == False:\n","            bins = sinogram_3D.shape[1]                 # Sinogram is square, so sinogram_3D.shape[1] = sinogram_3D.shape[2]\n","            bins_shifted = np.random.randint(0, bins)\n","            angle = int(bins_shifted * 180/bins)\n","\n","            image_2D = transforms.functional.rotate(image_2D, angle, fill=0) # Rotate image. Fill in unspecified pixels with zeros.\n","            sinogram_3D = torch.roll(sinogram_3D, bins_shifted, dims=(2,)) # Cycle (or 'Roll') sinogram by that angle\n","            sinogram_3D[:,:, 0:bins_shifted] = torch.flip(sinogram_3D[:,:,0:bins_shifted], dims=(1,)) # flip the cycled portion of the sinogram vertically\n","\n","        return image_2D, sinogram_3D\n","\n","    def VerticalFlip(image_2D, sinogram_3D):\n","        image_2D = torch.flip(image_2D,dims=(1,)) # Flip image vertically\n","        sinogram_3D = torch.flip(sinogram_3D,dims=(1,2)) # Flip sinogram horizontally and vertically\n","        return image_2D, sinogram_3D\n","\n","    def HorizontalFlip(image_2D, sinogram_3D):\n","        image_2D = torch.flip(image_2D, dims=(2,)) # Flip image horizontally\n","        sinogram_3D = torch.flip(sinogram_3D, dims=(2,)) # Flip sinogram horizontally\n","        return image_2D, sinogram_3D\n","\n","    ## Select Data ##\n","    image_2D = torch.from_numpy(image_array[index,:]) # image_2D.shape = (1, 71, 71)\n","    sinogram_3D = torch.from_numpy(sino_array[index,:]) # sinogram_3D.shape = (3,101,180,)\n","\n","    ## Run Data Augmentation on Selected Data ##\n","    if augment==True:\n","        image_2D, sinogram_3D = RandRotate(image_2D, sinogram_3D)           # Always rotates image by a random angle\n","        if np.random.randn(1)[0]>0: # Half of the time, flips the image vertically\n","            image_2D, sinogram_3D = VerticalFlip(image_2D, sinogram_3D)\n","        if np.random.randn(1)[0]>0: # Half of the time, flips the image horizontally\n","            image_2D, sinogram_3D = HorizontalFlip(image_2D, sinogram_3D)\n","\n","    ## Create A Set of Resized Outputs ##\n","    sinogram_3D_resize = transforms.Resize(size = (sino_size, sino_size), antialias=True)(sinogram_3D)\n","    image_2D_resize    = transforms.Resize(size = (image_size, image_size), antialias=True)(image_2D)\n","\n","    ## Normalize Resized Outputs (optional) ##\n","    if SI_normalize:\n","        a = torch.reshape(image_2D_resize, (1,-1))\n","        a = nn.functional.normalize(a, p=1, dim = 1)\n","        image_2D_resize = torch.reshape(a, (1, image_size, image_size))\n","    if IS_normalize:\n","        a = torch.reshape(sinogram_3D_resize, (3,-1))                     # Flattens each sinogram.\n","        a = nn.functional.normalize(a, p=1, dim = 1)                      # Normalizes along dimension 1 (pixel values for each of the 3 channels)\n","        sinogram_3D_resize = torch.reshape(a, (3, sino_size, sino_size))  # Reshapes images back into square matrices.\n","\n","    ## Adjust Output Channels of Resized Outputs ##\n","    if image_channels==1:\n","        image_out = image_2D_resize                 # For image_channels = 1, the image is just left alone\n","    else:\n","        image_out = image_2D_resize.repeat(3,1,1)   # For image_channels = 3, the image is repeated 3 times. This chould be altered to account for RGB images, etc.\n","\n","    if sino_channels==1:\n","        sino_out = sinogram_3D_resize[0:1,:]        # Selects photopeak sinogram only\n","    else:\n","        sino_out = sinogram_3D_resize               # Keeps full sinogram with all channels\n","\n","    # Returns both original and altered sinograms and images, assigned to CPU or GPU\n","    return sinogram_3D.to(device), IS_scale*sino_out.to(device), image_2D.to(device), SI_scale*image_out.to(device)\n","\n","class NpArrayDataSet(Dataset):\n","    '''\n","    Class for loading data from .np files, given file directory strings and set of optional transformations.\n","    In the dataset used in our first two conference papers, the data repeat every 17500 steps but with different augmentations.\n","    For the dataset with FORE rebinning, the dataset contains no augmented examples. Augmentation is performed on the fly.\n","    '''\n","    def __init__(self, image_path, sino_path, config, image_size = 90, sino_size=90, image_channels=1, sino_channels=1,\n","                 augment=False, offset=0, num_examples=-1, sample_division=1):\n","        '''\n","        image_path:         path to images in data set\n","        sino_path:          path to sinograms in data set\n","        config:             configuration dictionary with hyperparameters\n","        image_size:         shape to resize image to (for output)\n","        image_channels:     number of channels in images\n","        sino_size:          shape to resize sinograms to (for output)\n","        sino_channels:      number of channels in sinograms (for photopeak sinograms, this is 1)\n","        augment:            Set True to perform on-the-fly augmentation of data set. Set False to not perform augmentation.\n","        offset:             To begin dataset at beginning of the datafile, set offset=0. To begin on the second image, offset = 1, etc.\n","        num_examples:       Max number of examples to load into dataset. Set to -1 to load the maximum number from the numpy array.\n","        sample_division:    set to 1 to use every example, 2 to use every other example, etc. (If sample_division=2, the dataset will be half the size.)\n","        '''\n","        ## Load Data to Arrays ##\n","        image_array = np.load(image_path, mmap_mode='r')       # self.image_tensor.shape=(#examples x1x71x71)\n","        sino_array = np.load(sino_path, mmap_mode='r')         # self.sinogram_tensor.shape=(#examples x3x101x180)\n","\n","        ## Set Instance Variables ##\n","        if num_examples==-1:\n","            self.image_array = image_array[offset:,:]\n","            self.sino_array = sino_array[offset:,:]\n","        else:\n","            self.image_array = image_array[offset : offset + num_examples, :]\n","            self.sino_array = sino_array[offset : offset + num_examples, :]\n","\n","        self.config = config\n","        self.image_size = image_size\n","        self.sino_size = sino_size\n","        self.image_channels = image_channels\n","        self.sino_channels = sino_channels\n","        self.augment = augment\n","        self.sample_division = sample_division\n","\n","    def __len__(self):\n","        length = int(len(self.image_array)/sample_division)\n","        return length\n","\n","    def __getitem__(self, idx):\n","\n","        idx = idx*self.sample_division\n","\n","        sino_ground, sino_ground_scaled, image_ground, image_ground_scaled = NpArrayDataLoader(self.image_array, self.sino_array, self.config, self.image_size,\n","                                                                                self.sino_size, self.image_channels, self.sino_channels,\n","                                                                                augment=self.augment, index=idx)\n","\n","        return sino_ground, sino_ground_scaled, image_ground, image_ground_scaled\n","        # Returns both original, as well as altered, sinograms and images\n"]},{"cell_type":"markdown","metadata":{"id":"tQy_8kavlktI"},"source":["# To Do"]},{"cell_type":"code","source":["!ls -lh /content/drive/MyDrive/Colab/Working/dataset/sets/"],"metadata":{"id":"Hmm60oZUre_X","executionInfo":{"status":"ok","timestamp":1760313143751,"user_tz":300,"elapsed":100,"user":{"displayName":"Peter Lindstrom","userId":"12573886945126491618"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"67837353-0cb6-484f-d6ad-8a5d88cb7436"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["total 83G\n","-rw------- 1 root root 709M Oct  8 02:41 test-actMap.npy\n","-rw------- 1 root root 709M Oct  8 02:41 test-anniMap.npy\n","-rw------- 1 root root 709M Oct  8 02:42 test-attenMap.npy\n","-rw------- 1 root root 709M Oct  8 02:42 test-highCountImage.npy\n","-rw------- 1 root root  13G Oct 12 22:15 test-highCountSino-382x513.npy\n","-rw------- 1 root root 709M Oct  8 02:46 test-obliqueImage.npy\n","-rw------- 1 root root 2.2G Oct  7 23:09 train-actMap.npy\n","-rw------- 1 root root 2.2G Oct  7 23:12 train-anniMap.npy\n","-rw------- 1 root root 2.2G Oct  7 23:13 train-attenMap.npy\n","-rw------- 1 root root 2.2G Oct  7 23:15 train-highCountImage.npy\n","-rw------- 1 root root 6.5G Oct 12 23:51 train-highCountSino-180x180.npy\n","-rw------- 1 root root  40G Oct 12 22:16 train-highCountSino-382x513.npy\n","-rw------- 1 root root 2.2G Oct  7 23:41 train-obliqueImage.npy\n","-rw------- 1 root root 452M Oct  8 01:46 val-actMap.npy\n","-rw------- 1 root root 452M Oct  8 01:46 val-anniMap.npy\n","-rw------- 1 root root 452M Oct  8 01:46 val-attenMap.npy\n","-rw------- 1 root root 452M Oct  8 01:46 val-highCountImage.npy\n","-rw------- 1 root root 8.1G Oct 12 22:15 val-highCountSino-382x513.npy\n","-rw------- 1 root root 452M Oct  8 01:46 val-obliqueImage.npy\n"]}]},{"cell_type":"markdown","metadata":{"id":"zejDAeSYlktR"},"source":["Use os.path.join for paths instead of adding strings"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1Urzm0xHEIIQnPkLTDcedozai56Vx-vCG","timestamp":1759890501903},{"file_id":"1IKZAHG3MkebzxT6rbCXh6UyK1A_-pgjW","timestamp":1662946787019},{"file_id":"1rhTeO9VU1bsXALEA91XBWt5RmysWxoO_","timestamp":1657063751190}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}