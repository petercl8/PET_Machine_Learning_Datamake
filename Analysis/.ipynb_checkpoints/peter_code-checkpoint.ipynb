{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoNd1Rb3zmng"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "odmouzHe6txY",
    "outputId": "63efaeca-ab47-4dc3-f257-1a54c889cc8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/peter/miniconda3/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/peter/miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/peter/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/peter/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: torchvision in /home/peter/miniconda3/lib/python3.12/site-packages (0.18.1)\n",
      "Requirement already satisfied: numpy in /home/peter/miniconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.3.1 in /home/peter/miniconda3/lib/python3.12/site-packages (from torchvision) (2.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/peter/miniconda3/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/peter/miniconda3/lib/python3.12/site-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/peter/miniconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1->torchvision) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/peter/miniconda3/lib/python3.12/site-packages (from jinja2->torch==2.3.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/peter/miniconda3/lib/python3.12/site-packages (from sympy->torch==2.3.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: pandas in /home/peter/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/peter/miniconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/peter/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/peter/miniconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/peter/miniconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/peter/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-image in /home/peter/miniconda3/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/peter/miniconda3/lib/python3.12/site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/peter/miniconda3/lib/python3.12/site-packages (from scikit-image) (1.13.0)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/peter/miniconda3/lib/python3.12/site-packages (from scikit-image) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in /home/peter/miniconda3/lib/python3.12/site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /home/peter/miniconda3/lib/python3.12/site-packages (from scikit-image) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/peter/miniconda3/lib/python3.12/site-packages (from scikit-image) (2024.6.18)\n",
      "Requirement already satisfied: packaging>=21 in /home/peter/miniconda3/lib/python3.12/site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/peter/miniconda3/lib/python3.12/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: numpy in /home/peter/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "## Pytorch ##\n",
    "!pip install torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "torch.manual_seed(0)  # For testing purposes\n",
    "\n",
    "## Torchvision ##\n",
    "!pip install torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "\n",
    "## Numpy/MatPlotLib ##\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.pyplot import savefig\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "## Pandas ##\n",
    "!pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "## SciKit #\n",
    "!pip install scikit-image\n",
    "from skimage import metrics\n",
    "from skimage.metrics import structural_similarity\n",
    "from skimage.transform import radon, iradon\n",
    "from skimage.transform import iradon\n",
    "from skimage import morphology\n",
    "from skimage.morphology import opening, erosion\n",
    "#from skimage.restoration import denoise_bilateral, denoise_tv_chambolle, denoise_wavelet\n",
    "\n",
    "## SciPy ##\n",
    "#from scipy.stats import moment as compute_moment\n",
    "\n",
    "## Python ##\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "#from IPython.display import display, clear_output\n",
    "\n",
    "!pip install numpy\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "## Determine what Hardware we have ##\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT CODE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_interfile_header(header_file):\n",
    "    \"\"\"Read and parse the Interfile header.\"\"\"\n",
    "    header_info = {}\n",
    "    with open(header_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('!'):\n",
    "                continue\n",
    "            key_value = re.split(r':=', line, maxsplit=1)\n",
    "            if len(key_value) == 2:\n",
    "                key = key_value[0].strip()\n",
    "                value = key_value[1].strip()\n",
    "                header_info[key] = value\n",
    "    return header_info\n",
    "\n",
    "def read_interfile_data(header_info, data_file):\n",
    "    \"\"\"Read the binary data from the Interfile data file.\"\"\"\n",
    "    matrix_size = [int(size) for size in header_info['matrix size'].split(',')]\n",
    "    data_type = header_info['number format']\n",
    "    if data_type == 'float':\n",
    "        dtype = np.float32\n",
    "    elif data_type == 'short':\n",
    "        dtype = np.int16\n",
    "    elif data_type == 'byte':\n",
    "        dtype = np.uint8\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data type: {data_type}\")\n",
    "\n",
    "    # Calculate the total number of elements in the 3D stack\n",
    "    total_elements = np.prod(matrix_size)\n",
    "\n",
    "    # Read the binary data\n",
    "    data = np.fromfile(data_file, dtype=dtype, count=total_elements)\n",
    "\n",
    "    # Reshape to the 3D stack\n",
    "    data = data.reshape(matrix_size)\n",
    "    return data\n",
    "print(image_data.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN OUTPUT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the header and data files\n",
    "header_file = 'path/to/image.hdr'\n",
    "data_file = 'path/to/image.img'\n",
    "\n",
    "# Read header and data\n",
    "header_info = read_interfile_header(header_file)\n",
    "image_data = read_interfile_data(header_info, data_file)\n",
    "\n",
    "print(image_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSWJFsOU6zsL"
   },
   "source": [
    "# User Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7uyTV83_hQEI"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "### General Setup ###\n",
    "#####################\n",
    "\n",
    "# Basic Options #\n",
    "run_mode='visualize'  # Options: 'tune' / 'train' / 'test' / 'visualize'\n",
    "sino_size=90          # Resize input sinograms to this size (Options: 90, 180)\n",
    "sino_channels=1       # Number of channels (sinograms). Options: 1, 3. Unless using scattered coincidences, set to 1.\n",
    "image_size=90         # Image size (Options: 90, 180)\n",
    "image_channels=1      # Number of channels (images)\n",
    "train_type='SUP'      # 'SUP' / 'GAN' / 'CYCLESUP' / 'CYCLEGAN' (Supervisory only/GAN/Cycle consistency+supervisory/CycleGAN)\n",
    "train_SI=True         # If training GAN or SUP, set True to train Gen_SI (Sinogram-->Image), or False to train Gen_IS (Image-->Sinogram)\n",
    "\n",
    "# Global Directories #\n",
    "local_dir=     '/content/drive/MyDrive/Colab/Working/'                              # Directories not explicitly assigned below are created in this directory\n",
    "plot_dir=     '/content/drive/MyDrive/Colab/Working/Plots/'                          # Directory to save plots to\n",
    "checkpoint_dir='/content/drive/MyDrive/Colab/Working/Checkpoints-TrainOnQuartile'   # If not using Ray Tune (not tuning), PyTorch saves and loads checkpoint file from here\n",
    "                                                                                    # All checkpoint files (for training, testing, visualizing) save the states for a particular network.\n",
    "                                                                                    # Therefore, the hyperparameters for the loaded CNN must match the data in the checkpoint file.\n",
    "                                                                                    # The configuration dictionary, which contains these hyperparameter values, is set in the 'Supervisory\" cell, below.\n",
    "############\n",
    "## Tuning ##\n",
    "############\n",
    "# Note: When tuning, ALWAYS select \"restart and run all\" from Runtime menu in Google Colab, or there may be bugs.\n",
    "tune_scheduler = 'ASHA'     # Use FIFO for simple first in/first out, or ASHA for utilizing early stopping\n",
    "tune_dataframe_dir= '/content/drive/MyDrive/Colab/Working/Dataframes-TuneOnQuartile'\n",
    "tune_csv_file='frame-tunedOnLowSSIM-tunedSSIM-ASHA' # .csv file to save tuning dataframe to\n",
    "tune_exp_name='search-Quartile-lowSSIM-tunedSSIM-D'\n",
    "tune_dataframe_fraction=0.33# At what fraction of the tuning process do you save values to the tuning dataframe.\n",
    "#tune_exp_name='search-90x1-Full-tunedMSE-evenReport'    # Experiment file: Ray tune (and Tensorboard) write to this file relative to the local dir.\n",
    "tune_restore=False          # Restore a run (from file tune_exp_name in local_dir). Use this if a tuning run that terminated early.\n",
    "tune_max_t = 25             # Maximum number of reports per network. For even training examplereporting, 20 is a good number for ASHA. For FIFO, use 10.\n",
    "                            # For constant batch size reporting (tune_even_reporting=False), 35 works well.\n",
    "tune_minutes = 180           # How long to run RayTune. 180 minutes is good for 90x90 input. 210 minutes for 180x180.\n",
    "tune_for = 'SSIM'            # Tune for which optimization metric?: 'MSE', 'SSIM', or 'CUSTOM' (user defined, defined later in code).\n",
    "tune_iter_per_report=20     # Default value = 10. This is the number of training iterations per Raytune report for batch size = 512.\n",
    "                            # For a batch size of 256, the iterations/report would be twice this number. For batch size = 128, it would be four times, etc.\n",
    "                            # If tune_even_reporting = False, you can use 30.\n",
    "tune_even_reporting=False   # Set to True to ensure we report to Raytune at an even number of training examples, regardless of batch size\n",
    "tune_augment=True           # Augment data (on the fly) for tuning?\n",
    "\n",
    "##############\n",
    "## Training ##\n",
    "##############\n",
    "train_load_state=False      # Set to True to load pretrained weights. Use if training terminated early.\n",
    "train_save_state=False      # Save network weights to train_checkpoint_file file as it trains\n",
    "train_checkpoint_file='checkpoint-tunedLowSSIM-trainedHighSSIM-100epochs' # Checkpoint file to load/save\n",
    "#train_checkpoint_file='checkpoint-90x1-tunedLDM_w10s8-b5c-6epochs'\n",
    "#train_checkpoint_file='checkpoint-90x1-tunedLDM_w5s2-6epochs'\n",
    "training_epochs = 100         # Number of training epochs\n",
    "train_augment=True         # Augment data (on the fly) for training?\n",
    "train_display_step=10        # For supervised learning or GAN, set to: 20, For cycle-consistent network, set to 10\n",
    "train_sample_division=1     # To evenly sample the training set by a given factor, set this to an integer greater than 1 (ex: to sample every other example, set to 2)\n",
    "train_show_times=False       # Show calculation times during training?\n",
    "\n",
    "###########\n",
    "# Testing #\n",
    "###########\n",
    "#test_dataframe_dir= '/content/drive/MyDrive/Colab/Working/Dataframes-Test-Quartile'   # Directory for metric dataframes\n",
    "test_dataframe_dir= '/content/drive/MyDrive/Colab/Working/Dataframes-TestOnFull'\n",
    "test_csv_file = 'combined-tunedLowSSIM-trainedLowSSIM-onTestSet-wMLEM' # csv dataframe file to save testing results to\n",
    "test_checkpoint_file='checkpoint-tunedLowSSIM-trainedLowSSIM-100epochs'\n",
    "test_display_step=15        # Make this a larger number to save bit of time (displays images/metrics less often)\n",
    "test_batch_size=25          # This doesn't affect the final metrics, just the displayed metrics as testing procedes\n",
    "chunk_size=875              # How many examples do you want to test at once? NOTE: This should be a multiple of test_batch_size AND also go into the test set size evenly.\n",
    "testset_size=35000          # Size of the set to test. This must be <= the number of examples in your test set file.\n",
    "begin_at=0                  # Begin testing at this example number.\n",
    "compute_MLEM=True           # Compute MLEM for dataframes (takes a lot longer. Otherwise, only FBP is calculated)\n",
    "test_set_type='test'            # Set to 'test' to test on the test set. Set to 'train' to test on the training set.\n",
    "# Defaults\n",
    "merge_dataframes=True       # Merge the smaller/chunked dataframes at the end of the test run into one large dataframe?\n",
    "test_show_times=False       # Show calculation times?\n",
    "test_shuffle=False          # Shuffle test set when testing?\n",
    "test_sample_division=1      # To evenly sample the test set by a given factor, set this to an integer greater than 1.\n",
    "\n",
    "\n",
    "###############\n",
    "## Visualize ##\n",
    "###############\n",
    "#visualize_checkpoint_file='checkpoint-90x1-tunedMSE-fc6-6epochs' # Checkpoint file to load/save\n",
    "visualize_checkpoint_file='checkpoint-tunedHigh-trainedHigh-100epochs'\n",
    "visualize_batch_size = 10 # Set value to exactly 120 to see a large grid of images OR =<10 for reconstructions and ground truth with matched color scales\n",
    "visualize_offset=0        # Image to begin at. Set to 0 to start at beginning. 1200 is a good value to see images that are cut off by the edge of the FOV.\n",
    "visualize_type='train'    # Set to 'test' or 'train' to visualize the test set or training set, respectively\n",
    "visualize_shuffle=True    # Shuffle data set when visualizing\n",
    "\n",
    "####################################\n",
    "### Data Set Directory and Files ###\n",
    "####################################\n",
    "\n",
    "data_dir=      '/content/drive/MyDrive/Repository/PET_Data/'                       # Where training data is located\n",
    "#data_dir = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/'           # Where training data is located\n",
    "\n",
    "#tune_sino_file=  'tune_sino-10k.npy'\n",
    "#tune_image_file= 'tune_image-10k.npy'\n",
    "#tune_sino_file= 'train_sino-highMSE-17500.npy'\n",
    "#tune_image_file='train_image-highMSE-17500.npy'\n",
    "#tune_sino_file= 'train_sino-lowMSE-17500.npy'\n",
    "#tune_image_file='train_image-lowMSE-17500.npy'\n",
    "tune_sino_file= 'train_sino-lowSSIM-17500.npy'\n",
    "tune_image_file='train_image-lowSSIM-17500.npy'\n",
    "\n",
    "#train_sino_file= 'train_sino-70k.npy'\n",
    "#train_image_file='train_image-70k.npy'\n",
    "#train_sino_file= 'train_sino-highMSE-17500.npy'\n",
    "#train_image_file='train_image-highMSE-17500.npy'\n",
    "#train_sino_file= 'train_sino-lowMSE-17500.npy'\n",
    "#train_image_file='train_image-lowMSE-17500.npy'\n",
    "train_sino_file= 'train_sino-lowSSIM-17500.npy'\n",
    "train_image_file= 'train_image-lowSSIM-17500.npy'\n",
    "\n",
    "test_sino_file=  'test_sino-35k.npy'\n",
    "test_image_file= 'test_image-35k.npy'\n",
    "#test_sino_file= 'test_sino-highMSE-8750.npy'\n",
    "#test_image_file= 'test_image-highMSE-8750.npy'\n",
    "#test_sino_file= 'test_sino-lowMSE-8750.npy'\n",
    "#test_image_file= 'test_image-lowMSE-8750.npy'\n",
    "\n",
    "####################################################################\n",
    "## Assign Values for Various Scenarios: visualize/tune/train/test ##\n",
    "####################################################################\n",
    "\n",
    "tune_sino_path=os.path.join(data_dir, tune_sino_file)\n",
    "tune_image_path=os.path.join(data_dir, tune_image_file)\n",
    "train_sino_path=os.path.join(data_dir, train_sino_file)\n",
    "train_image_path=os.path.join(data_dir, train_image_file)\n",
    "test_sino_path=os.path.join(data_dir, test_sino_file)\n",
    "test_image_path=os.path.join(data_dir, test_image_file)\n",
    "\n",
    "## Run-Type Specific Assignments ##\n",
    "if run_mode=='tune':\n",
    "    sino_path=tune_sino_path\n",
    "    image_path=tune_image_path\n",
    "    augment=tune_augment\n",
    "    shuffle = True\n",
    "    num_epochs=1000         # Tuning is stopped when the iteration = tune_max_t (defined later). We set num_epochs to a large number so tuning doesn't terminate early.\n",
    "    load_state=False        # Set to True to load pretrained weights\n",
    "    save_state=False        # Save network weights to checkpoint file as it trains\n",
    "    checkpoint_file = ''    # Leave this empty ''. The checkpoint path is constructed regardless, so this ensures that no error occurs.\n",
    "    global_display_step=tune_iter_per_report\n",
    "    offset=0\n",
    "    show_times=False\n",
    "    sample_division=1\n",
    "    tune_dataframe_path = os.path.join(tune_dataframe_dir, tune_csv_file+'.csv')\n",
    "    if tune_restore==False:\n",
    "        tune_dataframe = pd.DataFrame({'SI_dropout': [], 'SI_exp_kernel': [], 'SI_gen_fill': [], 'SI_gen_hidden_dim': [], 'SI_gen_neck': [], 'SI_layer_norm': [],\n",
    "                                   'SI_normalize': [],'SI_pad_mode': [], 'batch_size': [], 'gen_lr': [], 'num_params': [], 'mean_CNN_MSE': [], 'mean_CNN_SSIM': [],\n",
    "                                   'mean_CNN_CUSTOM': []})\n",
    "        tune_dataframe.to_csv(tune_dataframe_path, index=False)\n",
    "\n",
    "if run_mode=='train':\n",
    "    sino_path=train_sino_path\n",
    "    image_path=train_image_path\n",
    "    augment=train_augment\n",
    "    shuffle=True\n",
    "    num_epochs=training_epochs\n",
    "    load_state=train_load_state\n",
    "    save_state=train_save_state\n",
    "    checkpoint_file = train_checkpoint_file\n",
    "    global_display_step=train_display_step\n",
    "    offset=0\n",
    "    show_times=train_show_times\n",
    "    sample_division=train_sample_division\n",
    "\n",
    "if run_mode=='test':\n",
    "    if test_set_type=='test': # Test on test set\n",
    "        sino_path=test_sino_path\n",
    "        image_path=test_image_path\n",
    "    else: # Test on training set\n",
    "        sino_path=train_sino_path\n",
    "        image_path=train_image_path\n",
    "    augment = False\n",
    "    shuffle = test_shuffle\n",
    "    num_epochs=1\n",
    "    load_state=True # Set to True to load pretrained weights.\n",
    "    save_state=False # Do not save network weights to checkpoint file as we are only testing.\n",
    "    checkpoint_file = test_checkpoint_file\n",
    "    global_display_step=test_display_step\n",
    "    offset=0\n",
    "    show_times=test_show_times\n",
    "    sample_division=test_sample_division\n",
    "\n",
    "if run_mode=='visualize':\n",
    "    if visualize_type=='test':\n",
    "        sino_path=test_sino_path  # test_sino_path\n",
    "        image_path=test_image_path # test_image_path\n",
    "    else:\n",
    "        sino_path=train_sino_path  # test_sino_path\n",
    "        image_path=train_image_path # test_image_path\n",
    "    augment = False\n",
    "    shuffle = visualize_shuffle\n",
    "    num_epochs=1\n",
    "    load_state=True\n",
    "    save_state=False\n",
    "    checkpoint_file = visualize_checkpoint_file\n",
    "    global_display_step=1\n",
    "    show_times=False\n",
    "    offset=visualize_offset\n",
    "    sample_division=1\n",
    "\n",
    "\n",
    "## Assign dataframe and checkpoing paths ##\n",
    "# This requires the assignment of checkpoint files, so is done after the run_type specific assignments ##\n",
    "test_dataframe_path = os.path.join(test_dataframe_dir, test_csv_file+'.csv')\n",
    "checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8x2aams6ekN"
   },
   "source": [
    "# Configuration Dicts\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXA-yS5rMYIZ"
   },
   "source": [
    "## Supervisory\n",
    "\n",
    "In this cell, set the correct hyperparameter dictionary to config_SUP_SI. This is the dictionary of hyperparameters that determine the form of the network that will be trained, tested, or visualized. You will usually find these hyperparameters by performing tuning and examining the best performing networks in tensorboard.\n",
    "\n",
    "If traning supervisory loss networks only, you don't need to worry about the other dictionaries in this section (GANs, Cycle-Consistent). You also don't need to worry about \"Search Spaces\", as this is simply a dictionary of the search space that Ray Tune uses when tuning. Feel free to look at it though, to see whow I set up the search space. The last section (Set Correct Config) is where the configuration dictionary gets assigned. The dictionary is either a searchable space, if tuning, or a set of fixed hyperparameters, if training, testing, or visualizing the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pf1yniv1MNFf"
   },
   "outputs": [],
   "source": [
    "### Below networks were tuned on whole dataset ###\n",
    "# 1x90x90, Tuned for MSE - fc6 #\n",
    "'''\n",
    "config_SUP_SI={\n",
    "  \"SI_dropout\": False,\n",
    "  \"SI_exp_kernel\": 4,\n",
    "  \"SI_gen_fill\": 0,\n",
    "  \"SI_gen_final_activ\": None,\n",
    "  \"SI_gen_hidden_dim\": 14,\n",
    "  \"SI_gen_mult\": 2.3737518721494038,\n",
    "  \"SI_gen_neck\": 5,\n",
    "  \"SI_gen_z_dim\": 300,\n",
    "  \"SI_layer_norm\": \"instance\",\n",
    "  \"SI_normalize\": True,\n",
    "  \"SI_pad_mode\": \"zeros\",\n",
    "  \"SI_scale\": 8100,\n",
    "  \"batch_size\": 266,\n",
    "  \"gen_b1\": 0.5194977285709309,\n",
    "  \"gen_b2\": 0.4955647195661826,\n",
    "  \"gen_lr\": 0.0006569034263698925,\n",
    "  \"sup_criterion\": nn.MSELoss()\n",
    "}\n",
    "'''\n",
    "'''\n",
    "# 1x90x90, Tuned for MAE (mean absolute error) - b08 #\n",
    "config_SUP_SI={\n",
    "  \"SI_dropout\": True,\n",
    "  \"SI_exp_kernel\": 3,\n",
    "  \"SI_gen_fill\": 0,\n",
    "  \"SI_gen_final_activ\": nn.Tanh(),\n",
    "  \"SI_gen_hidden_dim\": 29,\n",
    "  \"SI_gen_mult\": 3.4493572412953926,\n",
    "  \"SI_gen_neck\": 5,\n",
    "  \"SI_gen_z_dim\": 92,\n",
    "  \"SI_layer_norm\": \"instance\",\n",
    "  \"SI_normalize\": True,\n",
    "  \"SI_pad_mode\": \"zeros\",\n",
    "  \"SI_scale\": 8100,\n",
    "  \"batch_size\": 184,\n",
    "  \"gen_b1\": 0.41793988944151467,\n",
    "  \"gen_b2\": 0.15133808988276928,\n",
    "  \"gen_lr\": 0.0012653525173041019,\n",
    "  \"sup_criterion\": nn.L1Loss()\n",
    "}\n",
    "'''\n",
    "'''\n",
    "# 1x90x90, Tuned for SSIM - 14d #\n",
    "config_SUP_SI = {\n",
    "  \"SI_dropout\": False,\n",
    "  \"SI_exp_kernel\": 4,\n",
    "  \"SI_gen_fill\": 0,\n",
    "  \"SI_gen_final_activ\": nn.Tanh(),\n",
    "  \"SI_gen_hidden_dim\": 23,\n",
    "  \"SI_gen_mult\": 1.6605902406330195,\n",
    "  \"SI_gen_neck\": 5,\n",
    "  \"SI_gen_z_dim\": 789,\n",
    "  \"SI_layer_norm\": \"instance\",\n",
    "  \"SI_normalize\": True,\n",
    "  \"SI_pad_mode\": \"zeros\",\n",
    "  \"SI_scale\": 8100,\n",
    "  \"batch_size\": 71,\n",
    "  \"gen_b1\": 0.2082092731474774,\n",
    "  \"gen_b2\": 0.27147903136187507,\n",
    "  \"gen_lr\": 0.0005481469822215635,\n",
    "  \"sup_criterion\": nn.MSELoss()\n",
    "}\n",
    "'''\n",
    "'''\n",
    "# 1x90x90, Tuned for Local Distributions Metric, 5x5 window, stride 2\n",
    "config_SUP_SI={\n",
    "  \"SI_dropout\": True,\n",
    "  \"SI_exp_kernel\": 3,\n",
    "  \"SI_gen_fill\": 2,\n",
    "  \"SI_gen_final_activ\": nn.Sigmoid(),\n",
    "  \"SI_gen_hidden_dim\": 18,\n",
    "  \"SI_gen_mult\": 2.4691388140182475,\n",
    "  \"SI_gen_neck\": 11,\n",
    "  \"SI_gen_z_dim\": 444,\n",
    "  \"SI_layer_norm\": \"instance\",\n",
    "  \"SI_normalize\": True,\n",
    "  \"SI_pad_mode\": \"zeros\",\n",
    "  \"SI_scale\": 8100,\n",
    "  \"batch_size\": 33,\n",
    "  \"gen_b1\": 0.8199882799898334,\n",
    "  \"gen_b2\": 0.1207854128656507,\n",
    "  \"gen_lr\": 0.0001095057659925285,\n",
    "  \"sup_criterion\": nn.BCEWithLogitsLoss()\n",
    "}\n",
    "'''\n",
    "'''\n",
    "# 1x90x90, Tuned for Local Distributions Metric, 10x10 window, stride 8 (b5c)\n",
    "config_SUP_SI={\n",
    "  \"SI_dropout\": False,\n",
    "  \"SI_exp_kernel\": 4,\n",
    "  \"SI_gen_fill\": 0,\n",
    "  \"SI_gen_final_activ\": None,\n",
    "  \"SI_gen_hidden_dim\": 9,\n",
    "  \"SI_gen_mult\": 2.1547197646081444,\n",
    "  \"SI_gen_neck\": 5,\n",
    "  \"SI_gen_z_dim\": 344,\n",
    "  \"SI_layer_norm\": \"batch\",\n",
    "  \"SI_normalize\": False,\n",
    "  \"SI_pad_mode\": \"zeros\",\n",
    "  \"SI_scale\": 8100,\n",
    "  \"batch_size\": 47,\n",
    "  \"gen_b1\": 0.31108788447029295,\n",
    "  \"gen_b2\": 0.3445239707919786,\n",
    "  \"gen_lr\": 0.0007561178182660596,\n",
    "  \"sup_criterion\": nn.L1Loss()\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "### Below networks were tuned on 1/4 of dataset (high MSE or low MSE) ####\n",
    "\n",
    "# 1x90x90, Tuned for SSIM, highSSIM quartile, - c867539\n",
    "config_SUP_SI = {\n",
    "  \"SI_dropout\": False,\n",
    "  \"SI_exp_kernel\": 3,\n",
    "  \"SI_gen_fill\": 0,\n",
    "  \"SI_gen_final_activ\": nn.Tanh(),\n",
    "  \"SI_gen_hidden_dim\": 14,\n",
    "  \"SI_gen_mult\": 3.1366081867376066,\n",
    "  \"SI_gen_neck\": 5,\n",
    "  \"SI_gen_z_dim\": 1235,\n",
    "  \"SI_layer_norm\": \"instance\",\n",
    "  \"SI_normalize\": True,\n",
    "  \"SI_pad_mode\": \"reflect\",\n",
    "  \"SI_scale\": 8100,\n",
    "  \"batch_size\": 512,\n",
    "  \"gen_b1\": 0.36092827701745117,\n",
    "  \"gen_b2\": 0.2959809747063715,\n",
    "  \"gen_lr\": 0.0003914885622973457,\n",
    "  \"sup_criterion\": nn.MSELoss()\n",
    "}\n",
    "\n",
    "'''\n",
    "# 1x90x90, Tuned for MSE, lowMSE quartile - d3c\n",
    "config_SUP_SI = {\n",
    "  \"SI_dropout\": False,\n",
    "  \"SI_exp_kernel\": 3,\n",
    "  \"SI_gen_fill\": 0,\n",
    "  \"SI_gen_final_activ\": nn.Tanh(),\n",
    "  \"SI_gen_hidden_dim\": 10,\n",
    "  \"SI_gen_mult\": 3.5952046080348117,\n",
    "  \"SI_gen_neck\": 5,\n",
    "  \"SI_gen_z_dim\": 1144,\n",
    "  \"SI_layer_norm\": \"batch\",\n",
    "  \"SI_normalize\": True,\n",
    "  \"SI_pad_mode\": \"zeros\",\n",
    "  \"SI_scale\": 8100,\n",
    "  \"batch_size\": 338,\n",
    "  \"gen_b1\": 0.21119520045946658,\n",
    "  \"gen_b2\": 0.3219437242478679,\n",
    "  \"gen_lr\": 0.0012228287967471555,\n",
    "  \"sup_criterion\": nn.L1Loss()\n",
    "}\n",
    "'''\n",
    "'''\n",
    "# 1x90x90, Tuned for MSE, highMSE quartile - 66e\n",
    "config_SUP_SI = {\n",
    "  \"SI_dropout\": False,\n",
    "  \"SI_exp_kernel\": 4,\n",
    "  \"SI_gen_fill\": 0,\n",
    "  \"SI_gen_final_activ\": nn.Tanh(),\n",
    "  \"SI_gen_hidden_dim\": 13,\n",
    "  \"SI_gen_mult\": 2.427097790975542,\n",
    "  \"SI_gen_neck\": 1,\n",
    "  \"SI_gen_z_dim\": 1943,\n",
    "  \"SI_layer_norm\": \"instance\",\n",
    "  \"SI_normalize\": True,\n",
    "  \"SI_pad_mode\": \"zeros\",\n",
    "  \"SI_scale\": 8100,\n",
    "  \"batch_size\": 399,\n",
    "  \"gen_b1\": 0.5173104983713961,\n",
    "  \"gen_b2\": 0.5269533977675209,\n",
    "  \"gen_lr\": 0.00042406256400739315,\n",
    "  \"sup_criterion\": nn.MSELoss()\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpBoGKmTL-eR"
   },
   "source": [
    "## GANs + Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "p4jrm83ALpTH"
   },
   "outputs": [],
   "source": [
    "## Best Configs for GANs ##\n",
    "\n",
    "config_GAN_SI = { # Older, this still outperforms the more recent tuning\n",
    "    'SI_disc_adv_criterion': nn.MSELoss(),\n",
    "    'SI_normalize': True, # True\n",
    "    'SI_scale': 1400, # 1      # Added later\n",
    "    'SI_gen_neck': 1, # 1\n",
    "    'SI_layer_norm': 'batch',\n",
    "    'SI_pad_mode': 'reflect',\n",
    "    'SI_dropout': False,\n",
    "    'SI_exp_kernel': 3,\n",
    "    'SI_gen_fill': 0,\n",
    "    'SI_gen_mult': 1.41,\n",
    "    'SI_gen_z_dim': 115,\n",
    "    'SI_gen_final_activ': nn.Sigmoid(),\n",
    "    'SI_disc_patchGAN': True,\n",
    "    'SI_gen_hidden_dim': 46,\n",
    "    'SI_disc_hidden_dim': 25,\n",
    "    'SI_disc_b1': 0.102081,\n",
    "    'SI_disc_b2': 0.999,\n",
    "    # Gets Overwritten Below\n",
    "    'SI_disc_lr': 0.000167384,\n",
    "    'batch_size': 78,\n",
    "    'gen_adv_criterion': nn.MSELoss(),\n",
    "    'gen_lr': 0.000167384,\n",
    "    'gen_b1': 0.102081,\n",
    "    'gen_b2': 0.999,\n",
    "    }\n",
    "\n",
    "config_GAN_IS = { # new, looks good by step 400, somewhat blocky. May be outperforming config_GAN_SI\n",
    "  \"IS_disc_adv_criterion\": nn.BCEWithLogitsLoss(),\n",
    "  \"IS_disc_b1\": 0.3335905891003811,\n",
    "  \"IS_disc_b2\": 0.999,\n",
    "  \"IS_disc_hidden_dim\": 11,\n",
    "  \"IS_disc_patchGAN\": True,\n",
    "  \"IS_dropout\": False,\n",
    "  \"IS_exp_kernel\": 3,\n",
    "  \"IS_gen_fill\": 0,\n",
    "  \"IS_gen_final_activ\": None,\n",
    "  \"IS_gen_hidden_dim\": 15,\n",
    "  \"IS_gen_mult\": 3,\n",
    "  \"IS_gen_neck\": 11,\n",
    "  \"IS_gen_z_dim\": 5,\n",
    "  \"IS_layer_norm\": \"instance\",\n",
    "  \"IS_normalize\": False,\n",
    "  \"IS_pad_mode\": \"reflect\",\n",
    "  \"IS_scale\": 1,\n",
    "  # Gets Overwritten Below\n",
    "  \"IS_disc_lr\": 0.00021705437338035208,\n",
    "  \"batch_size\": 88,\n",
    "  \"gen_adv_criterion\": nn.MSELoss(),\n",
    "  \"gen_b1\": 0.46293297275979556,\n",
    "  \"gen_b2\": 0.999,\n",
    "  \"gen_lr\": 0.00042810775483742824\n",
    "}\n",
    "\n",
    "'''\n",
    "# this config looks decent at step 1100, worse at 1440, better at 1900, etc. (variable). It isn't blocky.\n",
    "config_GAN_IS_old = {\n",
    "    \"batch_size\": 82,\n",
    "    \"gen_adv_criterion\": nn.BCEWithLogitsLoss(),\n",
    "    \"gen_lr\": 3.365297856241193e-05,\n",
    "    \"gen_b1\": 0.11790916451301556,\n",
    "    \"gen_b2\": 0.999,\n",
    "\n",
    "    \"IS_disc_adv_criterion\": nn.BCEWithLogitsLoss(),\n",
    "    \"IS_normalize\": False, # FALSE\n",
    "    'IS_scale': 1, # 1\n",
    "    'IS_gen_mult': 3,\n",
    "    'IS_gen_fill': 0,\n",
    "    'IS_gen_neck': 5, # Wide neck\n",
    "    \"IS_gen_z_dim\": 115,\n",
    "    'IS_layer_norm': 'instance',\n",
    "    'IS_pad_mode': 'reflect',\n",
    "    'IS_dropout': False,\n",
    "    'IS_exp_kernel': 4,\n",
    "    \"IS_gen_final_activ\": nn.Tanh(), # nn.Tanh()\n",
    "    \"IS_disc_patchGAN\": True,\n",
    "    \"IS_gen_hidden_dim\": 16,\n",
    "    \"IS_disc_hidden_dim\": 19,\n",
    "    \"IS_disc_lr\": 0.00020392229473545828,\n",
    "    \"IS_disc_b1\": 0.35984156365558084,\n",
    "    \"IS_disc_b2\": 0.999,\n",
    "    }\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kldB7WhyQCDF"
   },
   "outputs": [],
   "source": [
    "## I've looked at configurations in 'search-CycleGAN' through (and including) '4a92'\n",
    "\n",
    "config_CYCLEGAN={ # Works, yeah! (\"4a92\")\n",
    "    \"IS_disc_adv_criterion\": nn.BCEWithLogitsLoss(),\n",
    "    \"IS_disc_b1\": 0.3335905891003811,\n",
    "    \"IS_disc_b2\": 0.999,\n",
    "    \"IS_disc_hidden_dim\": 11,\n",
    "    \"IS_disc_lr\": 0.0006554051278271163,\n",
    "    \"IS_disc_patchGAN\": True,\n",
    "    \"IS_dropout\": False,\n",
    "    \"IS_exp_kernel\": 3,\n",
    "    \"IS_gen_fill\": 0,\n",
    "    \"IS_gen_final_activ\": None,\n",
    "    \"IS_gen_hidden_dim\": 15,\n",
    "    \"IS_gen_mult\": 3,\n",
    "    \"IS_gen_neck\": 11,\n",
    "    \"IS_gen_z_dim\": 5,\n",
    "    \"IS_layer_norm\": \"instance\",\n",
    "    \"IS_normalize\": False,\n",
    "    \"IS_pad_mode\": \"reflect\",\n",
    "    \"IS_scale\": 1,\n",
    "    \"SI_disc_adv_criterion\": nn.MSELoss(),\n",
    "    \"SI_disc_b1\": 0.102081,\n",
    "    \"SI_disc_b2\": 0.999,\n",
    "    \"SI_disc_hidden_dim\": 25,\n",
    "    \"SI_disc_lr\": 0.0005793968896471209,\n",
    "    \"SI_disc_patchGAN\": True,\n",
    "    \"SI_dropout\": False,\n",
    "    \"SI_exp_kernel\": 3,\n",
    "    \"SI_gen_fill\": 0,\n",
    "    \"SI_gen_final_activ\": nn.Sigmoid(),\n",
    "    \"SI_gen_hidden_dim\": 46,\n",
    "    \"SI_gen_mult\": 1.41,\n",
    "    \"SI_gen_neck\": 1,\n",
    "    \"SI_gen_z_dim\": 115,\n",
    "    \"SI_layer_norm\": \"batch\",\n",
    "    \"SI_normalize\": True,\n",
    "    \"SI_pad_mode\": \"reflect\",\n",
    "    \"SI_scale\": 1400,\n",
    "    \"batch_size\": 91,\n",
    "    \"cycle_criterion\": nn.MSELoss(),\n",
    "    \"gen_adv_criterion\": nn.MSELoss(),\n",
    "    \"gen_b1\": 0.1610671788990834,\n",
    "    \"gen_b2\": 0.999,\n",
    "    \"gen_lr\": 0.0023450700434171526,\n",
    "    \"lambda_adv\": 1,\n",
    "    \"lambda_cycle\": 1, #1\n",
    "    \"lambda_sup\": 0, # 0\n",
    "    \"sup_criterion\": nn.L1Loss()\n",
    "    }\n",
    "\n",
    "'''\n",
    "## Was best for training the CycleGAN all at once ##\n",
    "# Below config is \"SM_1662\", the lowest optim_metric in 9h run, 90x90 symmetrical (not symmetrized parameters) networks.\n",
    "# It was trained on IO_channels==3 but seems to work fine for IO_channels==1. Also, both discriminators use the same architecture,\n",
    "# which is really better suited for the sinogram (Disc_S_90).\n",
    "#\n",
    "# Lessons Learned:\n",
    "# 1) Utilized: different size necks, final activations, channels, patchGAN\n",
    "# 2) NOT Uilized: fill Conv2d layers, different adv_criterion (for disc loss), different normalizations\n",
    "\n",
    "config={ # Symmetrize == FALSE (final activations don't match). This was the best over full tune train time (9 hours). Use two Sinogram discriminators.\n",
    "'batch_size': 107,\n",
    "'gen_b1': 0.339,\n",
    "'gen_b2': 0.999,\n",
    "'gen_lr': 0.000103,\n",
    "\"cycle_criterion\": nn.L1Loss(),\n",
    "\"sup_criterion\": nn.L1Loss(),\n",
    "\"gen_adv_criterion\": nn.KLDivLoss(),\n",
    "\"lambda_adv\": 1,\n",
    "\"lambda_cycle\": 2,\n",
    "\"lambda_sup\": 0,\n",
    "\n",
    "\"IS_disc_adv_criterion\": nn.MSELoss(),\n",
    "\"IS_disc_b1\": 0.19520417398460468,\n",
    "\"IS_disc_b2\": 0.999,\n",
    "\"IS_disc_hidden_dim\": 23,\n",
    "\"IS_disc_lr\": 0.0022230036964765274,  # disc_lr is 10X faster than SI\n",
    "\"IS_disc_patchGAN\": False,            # true for SI (make sense, images can be more true/false in patches)\n",
    "\"IS_gen_fill\": 0,                     # fill=0 for both IS and SI\n",
    "\"IS_gen_final_activ\": nn.Sigmoid(),   # tuned final activations opposite than for GANs\n",
    "\"IS_gen_hidden_dim\": 8,               # IS much less complex than SI (8 vs 16 hidden_dim)\n",
    "\"IS_gen_mult\": 3,                     # mult=3 for both IS and SI\n",
    "\"IS_gen_z_dim\": 5,\n",
    "\"IS_normalize\": True,                 # both are normalized here\n",
    "\"IS_scale\": 1,                        # OMG, this is weird. We are normalizing both, but the SI image scale is 1400x the IS. Could this by why final activation is now Sigmoid?\n",
    "\n",
    "'IS_layer_norm': 'batch', # Batch\n",
    "'IS_pad_mode': 'reflect',\n",
    "'IS_dropout': False,\n",
    "\"IS_gen_neck\": 5,            # 2\n",
    "'IS_exp_kernel': 4,          # 4\n",
    "\n",
    "\"SI_disc_adv_criterion\": nn.MSELoss(),\n",
    "\"SI_disc_b1\": 0.30423542819878224,\n",
    "\"SI_disc_b2\": 0.999,\n",
    "\"SI_disc_hidden_dim\": 23,\n",
    "\"SI_disc_lr\": 0.00020737432489437965,\n",
    "\"SI_disc_patchGAN\": True,\n",
    "\"SI_gen_fill\": 0,\n",
    "\"SI_gen_final_activ\": nn.Tanh(),\n",
    "\"SI_gen_hidden_dim\": 22,\n",
    "\"SI_gen_mult\": 3,\n",
    "\"SI_gen_z_dim\": 1195,                 # Represents an 8x drop in information into narrowest part of neck\n",
    "\"SI_normalize\": True, # True\n",
    "\"SI_scale\": 1400,\n",
    "\n",
    "'SI_layer_norm': 'batch',\n",
    "'SI_pad_mode': 'reflect',\n",
    "'SI_dropout': False,\n",
    "\"SI_gen_neck\": 1,            # 1\n",
    "'SI_exp_kernel': 4,          # 4\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt-bnmQuLr12"
   },
   "source": [
    "## Search Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vTSHCV3G6mSz"
   },
   "outputs": [],
   "source": [
    "#################################################################################################################################################################\n",
    "## (config_RAY_SI OR config_RAY_IS) gets combined with (config_RAY_SUP or config_RAY_GAN) to form a single hyperparameter space for searching a single network ##\n",
    "#################################################################################################################################################################\n",
    "\n",
    "## Note: For the Coursera CycleGAN:\n",
    "# gen_adv_criterion = disc_adv_criterion = nn.MSELoss()\n",
    "# cycle_criterion = ident_criterion = nn.L1Loss()\n",
    "# for notes on momentum, see: https://distill.pub/2017/momentum/\n",
    "\n",
    "config_RAY_SI = { # Dictionary for Generator: Sinogram-->Image\n",
    "    # Data Loading\n",
    "    'SI_normalize': tune.choice([True, False]),                 # Normalize dataloader outputs and outputs of generator? If so, the pixel values in the image all add up to 1.\n",
    "    'SI_scale': 90*90,                                          # If normalizing the pixel images, multiply images by this value. The pixel values will then add up to this number.\n",
    "    # Generator Network\n",
    "    'SI_gen_mult': tune.uniform(1.1, 4),                        # Factor by which to multiply channels/block as one moves twowards the center of the network\n",
    "    'SI_gen_fill': tune.choice([0,1,2]),                        # Number of constant-sized Conv2d layers/block\n",
    "    'SI_gen_neck': tune.choice([1,5,11]),                       # Size of network neck: 1 = smallest, 11 = largest\n",
    "    'SI_gen_z_dim': tune.lograndint(64, 4000),                  # If network utilized smallest neck size (1x1 = a dense layer), this is the number of channels in the neck\n",
    "    'SI_layer_norm': tune.choice(['batch', 'instance','none']), # Layer normalization type ('none' seems to be, in practice, never chosen by tuning)\n",
    "    'SI_pad_mode': tune.choice(['zeros', 'reflect']),           # Padding type\n",
    "    'SI_dropout': tune.choice([True,False]),                    # Implement dropout in network? (without cross-validation, this is never chosen)\n",
    "    'SI_exp_kernel': tune.choice([3,4]),                        # Expanding kernel size: 3x3 or 4x4\n",
    "    'SI_gen_final_activ':  tune.choice([nn.Tanh(), nn.Sigmoid(), None]), # Options: tune.choice([nn.Tanh(), nn.Sigmoid(), None]), # nn.ReLU6(), nn.Hardsigmoid(), nn.ReLU(), nn.PReLU(), None\n",
    "    'SI_gen_hidden_dim': tune.lograndint(5, 30),                # Generator channel scaling factor. Larger numbers give more total channels.\n",
    "    # Discriminator Network\n",
    "    'SI_disc_hidden_dim': tune.lograndint(10, 30),              # Discriminator channel scaling factor\n",
    "    'SI_disc_patchGAN': tune.choice([True, False]),             # Use PatchGAN or not\n",
    "    # Discriminator Optimizer\n",
    "    'SI_disc_lr': tune.loguniform(1e-4,1e-2),\n",
    "    'SI_disc_b1': tune.loguniform(0.1, 0.999),\n",
    "    'SI_disc_b2': tune.loguniform(0.1, 0.999),\n",
    "    'SI_disc_adv_criterion': tune.choice([nn.MSELoss(), nn.BCEWithLogitsLoss()]), # Possible options: tune.choice([nn.MSELoss(), nn.KLDivLoss(), nn.BCEWithLogitsLoss()]),\n",
    "    }\n",
    "\n",
    "config_RAY_IS = { # Dictionary for Generator: Image-->Sinogram\n",
    "    # Data Loading\n",
    "    'IS_normalize': False, # tune.choice([True, False]), # Normalize outputs or not\n",
    "    'IS_scale': 90*90,\n",
    "    # Generator Network\n",
    "    'IS_gen_mult': tune.uniform(1.1, 4),\n",
    "    'IS_gen_fill': tune.choice([0,1,2]),\n",
    "    'IS_gen_neck': tune.choice([1,5,11]),\n",
    "    'IS_gen_z_dim': tune.lograndint(64, 4000),\n",
    "    'IS_layer_norm': tune.choice(['batch', 'instance','none']),\n",
    "    'IS_pad_mode': tune.choice(['zeros', 'reflect']),\n",
    "    'IS_dropout': tune.choice([True,False]),\n",
    "    'IS_exp_kernel': tune.choice([3,4]),\n",
    "    'IS_gen_final_activ': tune.choice([nn.Tanh(), nn.Sigmoid(), None]), # nn.ReLU6(), nn.Hardsigmoid(), nn.ReLU(), nn.PReLU(), None\n",
    "    'IS_gen_hidden_dim': tune.lograndint(5, 30),\n",
    "    # Discriminator Network\n",
    "    'IS_disc_hidden_dim': tune.lograndint(10, 30),\n",
    "    'IS_disc_patchGAN': tune.choice([True, False]),\n",
    "    # Discriminator Optimizer\n",
    "    'IS_disc_lr': tune.loguniform(1e-4,1e-2),\n",
    "    'IS_disc_b1': tune.loguniform(0.1, 0.999),\n",
    "    'IS_disc_b2': tune.loguniform(0.1, 0.999),\n",
    "    'IS_disc_adv_criterion': tune.choice([nn.MSELoss(), nn.BCEWithLogitsLoss()]),\n",
    "    }\n",
    "\n",
    "config_RAY_SUP = { # This dictionary is merged with either config_RAY_IS or config_RAY_SI to form a single dictionary for supervisory learning\n",
    "    # New: New parameters added to config_RAY_SI (related to generator optimizer)\n",
    "    'batch_size': tune.choice([32, 64, 128, 256, 512]), # tune.lograndint(30, 400),\n",
    "    'gen_lr': tune.loguniform(1e-4,1e-2),\n",
    "    'gen_b1': tune.loguniform(0.1, 0.999),\n",
    "    'gen_b2': tune.loguniform(0.1, 0.999),\n",
    "    'sup_criterion': tune.choice([nn.MSELoss(), nn.BCEWithLogitsLoss(), nn.L1Loss(), nn.KLDivLoss(reduction='batchmean')]), # Not SI or IS because used for both\n",
    "    # Overwrites: overwrites values from config_RAY_SI. This is done so time isn't wasted looking for unused hyperparameters.\n",
    "    'SI_disc_hidden_dim': 1,\n",
    "    'SI_disc_patchGAN': 1,\n",
    "    'SI_disc_lr': 1,\n",
    "    'SI_disc_b1': 1,\n",
    "    'SI_disc_b2': 1,\n",
    "    'SI_disc_adv_criterion': 1,\n",
    "    'IS_disc_hidden_dim': 1,\n",
    "    'IS_disc_patchGAN': 1,\n",
    "    'IS_disc_lr': 1,\n",
    "    'IS_disc_b1': 1,\n",
    "    'IS_disc_b2': 1,\n",
    "    'IS_disc_adv_criterion': 1,\n",
    "    }\n",
    "\n",
    "config_RAY_GAN = { # This is MERGED with either config_RAY_IS or config_RAY_SI to form a single dictionary for a generative adversarial network.\n",
    "    # New Parameters\n",
    "    'batch_size': tune.choice([32, 64, 128, 256, 512]),  #tune.lograndint(30, 400),\n",
    "    'gen_lr': tune.loguniform(1e-4,1e-2),\n",
    "    'gen_b1': tune.loguniform(0.1, 0.999),\n",
    "    'gen_b2': 0.999, #tune.loguniform(0.1, 0.999),\n",
    "    'gen_adv_criterion': tune.choice([nn.MSELoss(), nn.BCEWithLogitsLoss()]),\n",
    "    }\n",
    "\n",
    "config_GAN_RAY_cycle = { # Mixed New/Overwrites (when combined with config_SI/config_IS) to form a single dictionary for a cycle-consistent generative adversarial network.\n",
    "    # Overwrites\n",
    "    'gen_adv_criterion': nn.MSELoss(), #tune.choice([nn.MSELoss(), nn.KLDivLoss(), nn.BCEWithLogitsLoss()]),\n",
    "    'IS_disc_lr': tune.loguniform(1e-4,1e-2),\n",
    "    'SI_disc_lr': tune.loguniform(1e-4,1e-2),\n",
    "    'batch_size': tune.choice([32, 64, 128, 256, 512]),\n",
    "    'gen_lr': tune.loguniform(0.5e-4,1e-2),\n",
    "    'gen_b1': tune.loguniform(0.1, 0.999),\n",
    "    'gen_b2': 0.999, #tune.loguniform(0.1, 0.999),\n",
    "    # New\n",
    "    'cycle_criterion': tune.choice([nn.MSELoss(), nn.L1Loss()]),\n",
    "    'sup_criterion': tune.choice([nn.MSELoss(), nn.KLDivLoss(reduction='batchmean'), nn.L1Loss(), nn.BCEWithLogitsLoss()]),\n",
    "    'lambda_adv': 1,\n",
    "    'lambda_sup': 0,\n",
    "    'lambda_cycle': 1,\n",
    "    }\n",
    "\n",
    "config_SUP_RAY_cycle = { # Mixed New/Overwrites (when combined with config_SI/config_IS) to form a single dictionary for a cycle-consistent, partially supervised network.\n",
    "    # Not Used\n",
    "    'gen_adv_criterion': nn.MSELoss(), #tune.choice([nn.MSELoss(), nn.KLDivLoss(), nn.BCEWithLogitsLoss()]),\n",
    "    'IS_disc_lr': 1e-4, #tune.loguniform(1e-4,1e-2),\n",
    "    'SI_disc_lr': 1e-4, #tune.loguniform(1e-4,1e-2),\n",
    "    # Overwrites\n",
    "    'batch_size': tune.choice([32, 64, 128, 256, 512]),\n",
    "    'gen_lr': tune.loguniform(0.5e-4,1e-2),\n",
    "    'gen_b1': tune.loguniform(0.1, 0.999), # DCGan uses 0.5, https://distill.pub/2017/momentum/\n",
    "    'gen_b2': tune.loguniform(0.1, 0.999),\n",
    "    'sup_criterion': tune.choice([nn.MSELoss(), nn.KLDivLoss(), nn.L1Loss(), nn.BCEWithLogitsLoss()]),\n",
    "    # New\n",
    "    'cycle_criterion': tune.choice([nn.MSELoss(), nn.L1Loss()]),\n",
    "    'lambda_adv': 0,\n",
    "    'lambda_sup': 1,\n",
    "    'lambda_cycle':  tune.uniform(0, 10),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ_qmj4fJgK7"
   },
   "source": [
    "## Set Correct Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qUppJO1XJdof"
   },
   "outputs": [],
   "source": [
    "## Combine Dictionaries ##\n",
    "if run_mode=='train' or 'test' or 'visualize':\n",
    "    if train_type=='SUP':\n",
    "        if train_SI==True:\n",
    "            config = config_SUP_SI\n",
    "        if train_SI==False:\n",
    "            config = config_SUP_IS\n",
    "    if train_type=='GAN':\n",
    "        if train_SI==True:\n",
    "            config = config_GAN_SI\n",
    "        if train_SI==False:\n",
    "            config = config_GAN_IS\n",
    "    if train_type=='CYCLEGAN':\n",
    "        config = config_CYCLEGAN\n",
    "    if train_type=='CYCLESUP':\n",
    "        config = config_CYCLESUP\n",
    "\n",
    "if run_mode=='tune':\n",
    "    if train_type=='SUP':\n",
    "        if train_SI==True:\n",
    "            config = {**config_RAY_SI, **config_RAY_SUP}\n",
    "        if train_SI==False:\n",
    "            config = {**config_RAY_IS, **config_RAY_SUP}\n",
    "    if train_type=='GAN':\n",
    "        if train_SI==True:\n",
    "            config = {**config_RAY_SI, **config_RAY_GAN}\n",
    "        if train_SI==False:\n",
    "            config = {**config_RAY_IS, **config_RAY_GAN}\n",
    "    if train_type=='CYCLESUP':\n",
    "        config = {**config_SUP_SI, **config_SUP_IS, **config_SUP_RAY_cycle}\n",
    "    if train_type=='CYCLEGAN':\n",
    "        config = {**config_GAN_SI, **config_GAN_IS, **config_GAN_RAY_cycle}\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqRdJRNYmpk6"
   },
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR6WK0lIERty"
   },
   "source": [
    "## Dataset - POSSIBLY USEFUL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EEAomee7EWjg"
   },
   "outputs": [],
   "source": [
    "## Read Pytorch documentation on data loading, etc.\n",
    "\n",
    "def NpArrayDataLoader(image_array, sino_array, config, image_size = 90, sino_size=90, image_channels=1, sino_channels=1, augment=False, index=0):\n",
    "    '''\n",
    "    Function to load images and sinograms. Returns 4 pytorch tensors: the original dataset sinogram and image,\n",
    "    and scaled and normalized sinograms and images.\n",
    "\n",
    "    image_array:    image numpy array\n",
    "    sino_array:     sino numpy array\n",
    "    config:         configuration dictionary with hyperparameters\n",
    "    image_size:     shape to resize image to (for output)\n",
    "    image_channels: number of channels for output images\n",
    "    sino_size:      shape to resize sinograms to (for output)\n",
    "    sino_channels:  number of channels in output sinograms (for photopeak sinograms, this is 1)\n",
    "    augment:        perform data augmentation?\n",
    "    index:          to begin at beginning of data set, index=0. To begin on the second image, index = 1, etc.\n",
    "    '''\n",
    "    ## Set Normalization Variables ##\n",
    "    if (train_type=='GAN') or (train_type=='SUP'):\n",
    "        if train_SI==True:\n",
    "            SI_normalize=config['SI_normalize']\n",
    "            SI_scale=config['SI_scale']\n",
    "            IS_normalize=False     # If the Sinogram-->Image network (SI) is being trained, don't waste time normalizing sinograms\n",
    "            IS_scale=1             # If the Sinogram-->Image network (SI) is being trained, don't waste time scaling sinograms\n",
    "        else:\n",
    "            IS_normalize=config['IS_normalize']\n",
    "            IS_scale=config['IS_scale']\n",
    "            SI_normalize=False\n",
    "            SI_scale=1\n",
    "    else:\n",
    "        IS_normalize=config['IS_normalize']\n",
    "        SI_normalize=config['SI_normalize']\n",
    "        IS_scale=config['IS_scale']\n",
    "        SI_scale=config['SI_scale']\n",
    "\n",
    "    ## Data Augmentation Functions ##\n",
    "    def RandRotate(image_2D, sinogram_3D):\n",
    "\n",
    "        def IntersectCircularBorder(image):\n",
    "            '''\n",
    "            Function for determining whether an image itersects a circular boundary inscribed within the square FOV.\n",
    "            This function is not currently used.\n",
    "            '''\n",
    "            y_max = image.shape[1]\n",
    "            x_max = image.shape[2]\n",
    "\n",
    "            r_max = y_max/2.0\n",
    "            x_center = (x_max-1)/2.0 # the -1 comes from the fact that the coordinates of a pixel start at 0, not 1\n",
    "            y_center = (y_max-1)/2.0\n",
    "\n",
    "            margin_sum = 0\n",
    "            for y in range(0, y_max):\n",
    "                for x in range(0, x_max):\n",
    "                    if r_max < ((x-x_center)**2 + (y-y_center)**2)**0.5 :\n",
    "                        margin_sum += torch.sum(image[:,y,x]).item()\n",
    "\n",
    "            return_value = True if margin_sum == 0 else False\n",
    "            return return_value\n",
    "\n",
    "        def IntersectSquareBorder(image):\n",
    "            '''\n",
    "            Function for determining whether the image intersects the edge. If it does not, then the image\n",
    "            is fully specified by the sinogram, and data augmentation can be performed. If the image does\n",
    "            intersect the edge of the image, then some of it may be cropped outside the FOV. In this case,\n",
    "            augmentation via rotation should not be performed.\n",
    "            '''\n",
    "            max_idx = image.shape[1]-1\n",
    "            margin_sum = torch.sum(image[:,0,:]).item() + torch.sum(image[:,max_idx,:]).item() \\\n",
    "                        +torch.sum(image[:,:,0]).item() + torch.sum(image[:,:,max_idx]).item()\n",
    "            return_value = False if margin_sum == 0 else True\n",
    "            return return_value\n",
    "\n",
    "        if IntersectSquareBorder(image_2D) == False:\n",
    "            bins = sinogram_3D.shape[1]\n",
    "            bins_shifted = np.random.randint(0, bins)\n",
    "            angle = int(bins_shifted * 180/bins)\n",
    "\n",
    "            image_2D = transforms.functional.rotate(image_2D, angle, fill=0) # Rotate image\n",
    "            sinogram_3D = torch.roll(sinogram_3D, bins_shifted, dims=(2,)) # Cycle (or 'Roll') sinogram by that angle\n",
    "            sinogram_3D[:,:, 0:bins_shifted] = torch.flip(sinogram_3D[:,:,0:bins_shifted], dims=(1,)) # flip the cycled portion of the sinogram\n",
    "\n",
    "        return image_2D, sinogram_3D\n",
    "\n",
    "    def VerticalFlip(image_2D, sinogram_3D):\n",
    "        image_2D = torch.flip(image_2D,dims=(1,)) # Flip image vertically\n",
    "        sinogram_3D = torch.flip(sinogram_3D,dims=(1,2)) # Flip sinogram horizontally and vertically\n",
    "        return image_2D, sinogram_3D\n",
    "\n",
    "    def HorizontalFlip(image_2D, sinogram_3D):\n",
    "        image_2D = torch.flip(image_2D, dims=(2,)) # Flip image horizontally\n",
    "        sinogram_3D = torch.flip(sinogram_3D, dims=(2,)) # Flip sinogram horizontally\n",
    "        return image_2D, sinogram_3D\n",
    "\n",
    "    ## Select Data ##\n",
    "    image_2D = torch.from_numpy(image_array[index,:]) # image_2D.shape = (1, 71, 71)\n",
    "    sinogram_3D = torch.from_numpy(sino_array[index,:]) # sinogram_3D.shape = (3,101,180,)\n",
    "\n",
    "    ## Run Data Augmentation on Selected Data ##\n",
    "    if augment==True:\n",
    "        image_2D, sinogram_3D = RandRotate(image_2D, sinogram_3D)           # Always rotates image by a random angle\n",
    "        if np.random.randn(1)[0]>0: # Half of the time, flips the image vertically\n",
    "            image_2D, sinogram_3D = VerticalFlip(image_2D, sinogram_3D)\n",
    "        if np.random.randn(1)[0]>0: # Half of the time, flips the image horizontally\n",
    "            image_2D, sinogram_3D = HorizontalFlip(image_2D, sinogram_3D)\n",
    "\n",
    "    ## Create A Set of Resized Outputs ##\n",
    "    sinogram_3D_resize = transforms.Resize(size = (sino_size, sino_size), antialias=True)(sinogram_3D)\n",
    "    image_2D_resize    = transforms.Resize(size = (image_size, image_size), antialias=True)(image_2D)\n",
    "\n",
    "    ## Normalize Resized Outputs (optional) ##\n",
    "    if SI_normalize:\n",
    "        a = torch.reshape(image_2D_resize, (1,-1))\n",
    "        a = nn.functional.normalize(a, p=1, dim = 1)\n",
    "        image_2D_resize = torch.reshape(a, (1, image_size, image_size))\n",
    "    if IS_normalize:\n",
    "        a = torch.reshape(sinogram_3D_resize, (3,-1))                     # Flattens each sinogram.\n",
    "        a = nn.functional.normalize(a, p=1, dim = 1)                      # Normalizes along dimension 1 (pixel values for each of the 3 channels)\n",
    "        sinogram_3D_resize = torch.reshape(a, (3, sino_size, sino_size))  # Reshapes images back into square matrices.\n",
    "\n",
    "    ## Adjust Output Channels of Resized Outputs ##\n",
    "    if image_channels==1:\n",
    "        image_out = image_2D_resize                 # For image_channels = 1, the image is just left alone\n",
    "    else:\n",
    "        image_out = image_2D_resize.repeat(3,1,1)   # For image_channels = 3, the image is repeated 3 times. This chould be altered to account for RGB images, etc.\n",
    "\n",
    "    if sino_channels==1:\n",
    "        sino_out = sinogram_3D_resize[0:1,:]        # Selects photopeak sinogram only\n",
    "    else:\n",
    "        sino_out = sinogram_3D_resize               # Keeps full sinogram with all channels\n",
    "\n",
    "    # Returns both original and altered sinograms and images\n",
    "    return sinogram_3D.to(device), IS_scale*sino_out.to(device), image_2D.to(device), SI_scale*image_out.to(device)\n",
    "\n",
    "class NpArrayDataSet(Dataset):\n",
    "    '''\n",
    "    Class for loading data from .np files, given file directory strings and set of optional transformations.\n",
    "    In the dataset used in the paper, the data repeat every 17500 steps but with different augmentations\n",
    "    '''\n",
    "    def __init__(self, image_path, sino_path, config, image_size = 90, sino_size=90, image_channels=1, sino_channels=1,\n",
    "                 augment=False, offset=0, num_examples=-1, sample_division=1):\n",
    "        '''\n",
    "        image_path:         path to images in data set\n",
    "        sino_path:          path to sinograms in data set\n",
    "        config:             configuration dictionary with hyperparameters\n",
    "        image_size:         shape to resize image to (for output)\n",
    "        image_channels:     number of channels in images\n",
    "        sino_size:          shape to resize sinograms to (for output)\n",
    "        sino_channels:      number of channels in sinograms (for photopeak sinograms, this is 1)\n",
    "        augment:            Set True to perform on-the-fly augmentation of data set. Set False to not perform augmentation.\n",
    "        offset:             To begin dataset at beginning of the datafile, set offset=0. To begin on the second image, offset = 1, etc.\n",
    "        num_examples:       Max number of training examples to load into dataset. Set to -1 to load the maximum number from the numpy array.\n",
    "                            The total number loaded = max_examples - offset\n",
    "        sample_division:    set to 1 to use every example, 2 to use every other example, etc. (If sample_division=2, the dataset will be half the size.)\n",
    "        '''\n",
    "        ## Load Data to Arrays ##\n",
    "        image_array = np.load(image_path, mmap_mode='r')       # self.image_tensor.shape=(#examples x1x71x71)\n",
    "        sino_array = np.load(sino_path, mmap_mode='r')         # self.sinogram_tensor.shape=(#examples x3x101x180)\n",
    "\n",
    "        ## Set Instance Variables ##\n",
    "        if num_examples==-1:\n",
    "            self.image_array = image_array[offset:,:]\n",
    "            self.sino_array = sino_array[offset:,:]\n",
    "        else:\n",
    "            self.image_array = image_array[offset : offset + num_examples, :]\n",
    "            self.sino_array = sino_array[offset : offset + num_examples, :]\n",
    "\n",
    "        self.config = config\n",
    "        self.image_size = image_size\n",
    "        self.sino_size = sino_size\n",
    "        self.image_channels = image_channels\n",
    "        self.sino_channels = sino_channels\n",
    "        self.augment = augment\n",
    "        self.sample_division = sample_division\n",
    "\n",
    "    def __len__(self):\n",
    "        length = int(len(self.image_array)/sample_division)\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        idx = idx*self.sample_division\n",
    "\n",
    "        sino_ground, sino_scaled, image_ground, image_scaled = NpArrayDataLoader(self.image_array, self.sino_array, self.config, self.image_size,\n",
    "                                                                                self.sino_size, self.image_channels, self.sino_channels,\n",
    "                                                                                augment=self.augment, index=idx)\n",
    "\n",
    "        return sino_ground, sino_scaled, image_ground, image_scaled\n",
    "        # Returns both original, as well as altered, sinograms and images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzKaOERZgrrH"
   },
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QzHWhXMIgoTV"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "##### Block Generating Functions #####\n",
    "######################################\n",
    "\n",
    "def contract_block(in_channels, out_channels, kernel_size, stride, padding=0, padding_mode='reflect', fill=0, norm='batch', drop=False):\n",
    "    '''\n",
    "    Function to construct a single \"contracting block.\" Each contracting block consists of one 2D convolutional layer, which decreases\n",
    "    the size (height and width) of the data. There are then up to three 2D convolution layers which do not change the height or width\n",
    "    (e.g. \"constant size layers\").\n",
    "\n",
    "    in_channels:    number of channels at the input of contracting block\n",
    "    out_channels:   number of channels at the output of contracting block\n",
    "    kernel_size:    size of the kernel for the 1st 2D convolutional layer in the contracting block\n",
    "    stride:         stride of the convolution for the 1st 2D convolutional layer in the contracting block\n",
    "    padding:        amount of padding for the the 1st 2D convolutional layer in the contracting block\n",
    "    padding_mode:   padding mode (options: \"zeros\", \"reflect\")\n",
    "    fill:           number of \"constant size\" 2D convolutional layers\n",
    "    norm:           type of layer normalization (\"batch\", \"instance\", or \"none\")\n",
    "    dropout:        include dropout layers in the contracting block? (True or False)\n",
    "    '''\n",
    "\n",
    "    if norm=='batch':    norm = nn.BatchNorm2d(out_channels)\n",
    "    if norm=='instance': norm = nn.InstanceNorm2d(out_channels)\n",
    "    if norm=='none':     norm = nn.Sequential()\n",
    "    dropout = nn.Dropout() if drop==True else nn.Sequential()\n",
    "\n",
    "    block1 =  nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode=padding_mode), norm, dropout, nn.ReLU())\n",
    "    if fill==0:\n",
    "        block2 = nn.Sequential() # If fill=0, there are no \"constant size\" convolutional layers, and so block2 is empty.\n",
    "    if fill==1:\n",
    "        block2 = nn.Sequential(nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU())\n",
    "    elif fill==2:\n",
    "        block2 = nn.Sequential(nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU(),\n",
    "                                nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU())\n",
    "    elif fill==3:\n",
    "        block2 = nn.Sequential(nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU(),\n",
    "                                nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU(),\n",
    "                                nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode), norm, dropout, nn.ReLU())\n",
    "    return nn.Sequential(block1, block2)\n",
    "\n",
    "def expand_block(in_channels, out_channels, kernel_size=3, stride=2, padding=0, output_padding=0, padding_mode='zeros', fill=0, norm='batch', drop=False, final_layer=False):\n",
    "    '''\n",
    "    Function to construct a single \"expanding block.\" Each expanding block consists of one 2D transposed convolution layer which increases\n",
    "    the size of the incoming data (height and width). There are then up to three 2D convolution layers which do not change the height or\n",
    "    width (e.g. \"constant size layers\").\n",
    "\n",
    "    in_channels:    number of channels at the input of the expanding block\n",
    "    out_channels:   number of channels at the output of the expanding block\n",
    "    kernel_size:    size of the kernel for the 1st 2D transposed convolutional layer in the expanding block\n",
    "    stride:         stride of the convolution for the 1st 2D transposed convolutional layer in the expanding block\n",
    "    padding:        amount of padding for the the 1st 2D transposed convolutional layer in the expanding block\n",
    "    padding_mode:   padding mode (ex: \"zeros\", \"reflect\")\n",
    "    fill:           number of \"constant size\" 2D convolutional layers\n",
    "    norm:           type of layer normalization (\"batch\", \"instance\", or \"none\")\n",
    "    dropout:        include dropout in the expanding block (True or False)\n",
    "    final_layer:    Is this the final layer in the expanding block? (True or False)\n",
    "    '''\n",
    "\n",
    "    if norm=='batch':       norm = nn.BatchNorm2d(out_channels)\n",
    "    if norm=='instance':    norm = nn.InstanceNorm2d(out_channels)\n",
    "    if norm=='none':        norm = nn.Sequential()\n",
    "    dropout = nn.Dropout() if drop==True else nn.Sequential()\n",
    "\n",
    "    block1 = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, padding_mode=padding_mode)\n",
    "    if fill==0:\n",
    "        block2 = nn.Sequential()\n",
    "    if fill==1:\n",
    "        block2 = nn.Sequential(norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode))\n",
    "    elif fill==2:\n",
    "        block2 = nn.Sequential(norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode),\n",
    "                                norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode))\n",
    "    elif fill==3:\n",
    "        block2 = nn.Sequential(norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode),\n",
    "                                norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode),\n",
    "                                norm, dropout, nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, 1, 1, padding_mode=padding_mode))\n",
    "\n",
    "    if final_layer==False: # If not the final layer, I add normalization, dropout and activation.\n",
    "        block3 = nn.Sequential(norm, dropout, nn.ReLU())\n",
    "    else:                  # Otherwise, I leave off the normalization, dropout, and activation. This allows me to do it explicitly\n",
    "                           # at the end of the network using tuned parameters.\n",
    "        block3 = nn.Sequential()\n",
    "    return nn.Sequential(block1, block2, block3)\n",
    "\n",
    "#################################\n",
    "##### 90x90 Generator Class #####\n",
    "#################################\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, config, gen_SI=True, input_size=90, input_channels=3, output_channels=3):\n",
    "        '''\n",
    "        A class to generate a 90x90-->90x90 or 180x180-->90x90 encoder-decoder network. The role of each item in the \"config\" dictionary is commented below.\n",
    "        In addition to, the class constructor takes the following as inputs:\n",
    "\n",
    "        gen_SI:             Equals True if the generator generates images from sinograms. Equals false if the generator generates sinograms from images.\n",
    "                            In a cycle-consistent network, this class generates two networks from the same config dictionary. Hence, the need\n",
    "                            for this parameter.\n",
    "        input_size:         size of the input image (90 or 180).\n",
    "        input_channels:     number of generator input channels\n",
    "        output_channels:    number of generator output channels\n",
    "\n",
    "        '''\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        ## Set Instance Variables ##\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        ## If gen_SI == True, we use the \"SI..\" keys from the config dictionary to construct the generator network. ##\n",
    "        if gen_SI:\n",
    "            # The following instance variables are defined since these will be used in the forward() method below. #\n",
    "\n",
    "            self.final_activation = config['SI_gen_final_activ']    # {'instance', 'batch', 'none'} : Type of activation function employed at the very end of network\n",
    "            self.normalize=config['SI_normalize']                   # {True, False} : Normalization\n",
    "            self.scale=config['SI_scale']                           # Any real number: Scale factor by which the outoput is multiplied, if the output is normalized\n",
    "\n",
    "            # The following variables are used in the constructor, and not the forward() method) so there is no need for instance variables. #\n",
    "\n",
    "            neck=config['SI_gen_neck'] #            {1,5,11} :                      Width of narrowest part (neck) of the network. The smaller the number, the narrower the neck.\n",
    "            exp_kernel=config['SI_exp_kernel'] #    {3,4} :                         Square kernel width (or height) for the expanding part of the network.\n",
    "            z_dim=config['SI_gen_z_dim'] #          (Any real number) :             Number of channels in the network neck, if neck=1. If neck=5 or 11, this parameter isn't used.\n",
    "            hidden_dim=config['SI_gen_hidden_dim']# (Any real number) :             scales all channels in network by the same linear factor. Larger hidden_dim -->more complex network\n",
    "            fill=config['SI_gen_fill'] #            {0,1,2,3} :                     Number of \"constant size\" 2D convolutions in each block\n",
    "            mult=config['SI_gen_mult'] #            (Any real number) :             Multiplicative factor by which network channels increase as the layers decrease in height & width\n",
    "            norm=config['SI_layer_norm'] #          {'instance', 'batch', 'none'} : Type of layer normalization\n",
    "            pad=config['SI_pad_mode'] #             {'zeros', 'reflect'} :          Type of padding in each layer/block\n",
    "            drop=config['SI_dropout'] #             {'True', 'False'} :             Whether dropout is used in the network\n",
    "\n",
    "        #If gen_SI == False, we use the \"IS..\" keys from the config dictionary to construct the generator network. ##\n",
    "        else:\n",
    "            self.final_activation = config['IS_gen_final_activ']\n",
    "            self.normalize=config['IS_normalize']\n",
    "            self.scale=config['IS_scale']\n",
    "\n",
    "            neck=config['IS_gen_neck']\n",
    "            exp_kernel=config['IS_exp_kernel']\n",
    "            z_dim=config['IS_gen_z_dim']\n",
    "            hidden_dim=config['IS_gen_hidden_dim']\n",
    "            fill=config['IS_gen_fill']\n",
    "            mult=config['IS_gen_mult']\n",
    "            norm=config['IS_layer_norm']\n",
    "            pad=config['IS_pad_mode']\n",
    "            drop=config['IS_dropout']\n",
    "\n",
    "        ## Abbreviations used for Block Definitions -- used to make code less awkward #\n",
    "        in_chan = input_channels\n",
    "        out_chan = output_channels\n",
    "\n",
    "        dim_0 = int(hidden_dim*mult**0) # Number of output channels of 1st block/input channels of 2nd block\n",
    "        dim_1 = int(hidden_dim*mult**1) # Number of output channels of 2nd block/input channels of 3rd block\n",
    "        dim_2 = int(hidden_dim*mult**2) # Follows pattern above\n",
    "        dim_3 = int(hidden_dim*mult**3)\n",
    "        dim_4 = int(hidden_dim*mult**4)\n",
    "        dim_5 = int(hidden_dim*mult**5)\n",
    "\n",
    "        ### Block Definitions ###\n",
    "\n",
    "        ## Build the Contracting Path ##\n",
    "        # The formula for the output size of a transposed convolution (nn.Conv2d) in Pytorch is as follows:\n",
    "        # Hf = [Hi+2*padding-dilation(kernel-1)-1]/stride + 1 = [Hi+2*padding-kernel]/stride + 1 (for dialation=1)\n",
    "\n",
    "        if input_size==180:\n",
    "            self.contract = nn.Sequential(\n",
    "                # nn.Conv2d: Hf = [Hi+2*padding-dilation(kernel-1)-1]/stride + 1 = [Hi+2*padding-kernel]/stride + 1 (for dialation=1)\n",
    "                # Sinogram Shape: (3,90,90)\n",
    "                contract_block(in_chan,   dim_0,     3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [180+2-3]/2 + 1 = 90\n",
    "                contract_block(dim_0,     dim_1,     3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [90+2-3]/2 + 1 = 45.5\n",
    "                contract_block(dim_1,     dim_2, 3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [45+2-3]/2 + 1 = 23\n",
    "                contract_block(dim_2,     dim_2,     4, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [23+2-4]/2 + 1 = 11.5\n",
    "            )\n",
    "        elif input_size==90:\n",
    "            self.contract = nn.Sequential(\n",
    "                contract_block(in_chan, dim_0, 3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [90+2-3]/2 + 1 = 45.5  : a 90x90 input gives a 45x45 output\n",
    "                contract_block(dim_0,   dim_1, 3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [45+2-3]/2 + 1 = 23    : a 45x45 input gives a 23x23 output\n",
    "                contract_block(dim_1,   dim_2, 4, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [23+2-4]/2 + 1 = 11.5  : a 23x23 input gives a 11x11 output\n",
    "            )\n",
    "\n",
    "        ## Build the Neck. There are 3 options ##\n",
    "        # neck=1 gives the narrowest (1x1) neck #\n",
    "        if neck==1:\n",
    "            self.neck = nn.Sequential(\n",
    "                contract_block(dim_2, dim_3, 4, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [11+2-4]/2 + 1 = 5.5\n",
    "                contract_block(dim_3, dim_4, 3, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm           ), # H = [5+2*1-3]/2 + 1 = 3\n",
    "                contract_block(dim_4, z_dim, 3, stride=1, padding=0,                   fill=0,    norm='batch'        ), # H = 1   ||norm is set to 'batch' because 'instance' won't work on 1x1 layer\n",
    "                expand_block(  z_dim, dim_4, 3, stride=2, padding=0,                   fill=fill, norm=norm           ), # H = [1-1]*2+5 = 3\n",
    "                expand_block(  dim_4, dim_3, 4, stride=2, padding=2, output_padding=1, fill=fill, norm=norm           ), # H = [3-1]*2+4-2*2+1 = 5\n",
    "            )\n",
    "\n",
    "        # neck=5 gives the middle width (5x5) neck #\n",
    "        if neck==5:\n",
    "            self.neck = nn.Sequential(\n",
    "                contract_block(dim_2, dim_3, 4, stride=2, padding=1, padding_mode=pad, fill=fill, norm=norm, drop=drop), # H = [11+2-4]/2 + 1 = 5.5\n",
    "                contract_block(dim_3, dim_3, 5, stride=1, padding=2, padding_mode=pad,            norm=norm           ), # H = [5+2*2-5]/1 + 1 = 5 (Constant Block)\n",
    "                contract_block(dim_3, dim_3, 5, stride=1, padding=2, padding_mode=pad,            norm=norm           ), # H = [5+2*2-5]/1 + 1 = 5 (Constant Block)\n",
    "                contract_block(dim_3, dim_3, 5, stride=1, padding=2, padding_mode=pad,            norm=norm           ), # H = [5+2*2-5]/1 + 1 = 5 (Constant Block)\n",
    "                #contract_block(dim_3, dim_3, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [5+2*2-5]/1 + 1 = 5 (Constant Block) # Add this next tuning!\n",
    "            )\n",
    "\n",
    "        # neck=11 gives the thickest (11x11) neck #\n",
    "        if neck==11:\n",
    "            self.neck = nn.Sequential(\n",
    "                contract_block(dim_2, dim_2, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [11+2*2-5]/1 + 1 = 11 (Constant Block)\n",
    "                contract_block(dim_2, dim_2, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [11+2*2-5]/1 + 1 = 11 (Constant Block)\n",
    "                contract_block(dim_2, dim_2, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [11+2*2-5]/1 + 1 = 11 (Constant Block)\n",
    "                contract_block(dim_2, dim_2, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [11+2*2-5]/1 + 1 = 11 (Constant Block)\n",
    "                contract_block(dim_2, dim_2, kernel_size=5, stride=1, padding=2, padding_mode=pad, norm=norm), # H = [11+2*2-5]/1 + 1 = 11 (Constant Block)\n",
    "            )\n",
    "\n",
    "        ## Build the Expanding Blocks ##\n",
    "        # The formula for the output size of a transposed convolution (nn.ConvTranspose2d:) in Pytorch is as follows:\n",
    "        # Hf = (Hi-1)*stride -2*padding +dilation*(kernel-1) +output_padding+1\n",
    "        #    = (Hi-1)*stride +kernel -2*padding +output_padding (for dialation=1)\n",
    "\n",
    "        # For neck=1 or 5, the output from previous layers is 5x5. Therefore, these can use the same expanding blocks #\n",
    "        if (neck==1 or neck==5):\n",
    "            if exp_kernel==3:\n",
    "            # Expanding block for neck=1 or 5, expanding kernel size = 3)\n",
    "                self.expand = nn.Sequential(\n",
    "                    expand_block(dim_3, dim_2,                      kernel_size=3, stride=2, padding=0, output_padding=0, fill=fill, norm=norm), # H = (5-1)*2  +3         = 11\n",
    "                    expand_block(dim_2, dim_1,                      kernel_size=3, stride=2, padding=1, output_padding=1, fill=fill, norm=norm), # H = (11-1)*2 +3 -2*1 +1 = 22\n",
    "                    expand_block(dim_1, dim_0,                      kernel_size=3, stride=2, padding=0, output_padding=0, fill=fill, norm=norm), # H = (22-1)*2 +3         = 45\n",
    "                    expand_block(dim_0, out_chan, final_layer=True, kernel_size=3, stride=2, padding=1, output_padding=1, fill=fill, norm=norm), # H = (45-1)*2 +3 -2*1 +1 = 90\n",
    "                )\n",
    "\n",
    "            elif exp_kernel==4:\n",
    "            # Expanding block for neck=1 or 5, expanding kernel size = 4\n",
    "                self.expand = nn.Sequential(\n",
    "                    expand_block(dim_3, dim_2,                      kernel_size=4, stride=2, padding=1, output_padding=1, fill=fill, norm=norm),  # H = (5-1)*2  +4 -2*1 +1 = 11\n",
    "                    expand_block(dim_2, dim_1,                      kernel_size=4, stride=2, padding=1, output_padding=0, fill=fill, norm=norm),  # H = (11-1)*2 +4 -2*1    = 22\n",
    "                    expand_block(dim_1, dim_0,                      kernel_size=4, stride=2, padding=1, output_padding=1, fill=fill, norm=norm),  # H = (21-1)*2 +4 -2*1 +1 = 45\n",
    "                    expand_block(dim_0, out_chan, final_layer=True, kernel_size=4, stride=2, padding=1, output_padding=0, fill=fill, norm=norm),  # H = (45-1)*2 +4 -2*1    = 90\n",
    "                )\n",
    "\n",
    "        # For neck=11, the output is 11x11. This neck requires its own expanding blocks #\n",
    "        if neck==11:\n",
    "            if exp_kernel==3:\n",
    "            # Expanding block for neck=11, expanding kernel size = 3\n",
    "                self.expand = nn.Sequential(\n",
    "                    expand_block(dim_2, dim_1,                      kernel_size=3, stride=2, padding=1, output_padding=1, fill=fill, norm=norm),  # H = (11-1)*2 +3 -2*1 +1 = 22\n",
    "                    expand_block(dim_1, dim_0,                      kernel_size=3, stride=2, padding=0, output_padding=0, fill=fill, norm=norm),  # H = (22-1)*2 +3         = 45\n",
    "                    expand_block(dim_0, out_chan, final_layer=True, kernel_size=3, stride=2, padding=1, output_padding=1, fill=fill, norm=norm),  # H = (45-1)*2 +3 -2*1 +1 = 90\n",
    "                )\n",
    "\n",
    "            if exp_kernel==4:\n",
    "            # Expanding block for neck=11, expanding kernel size = 4\n",
    "                self.expand = nn.Sequential(\n",
    "                    expand_block(dim_2, dim_1,                      kernel_size=4, stride=2, padding=1, output_padding=0, fill=fill, norm=norm),  # H = (11-1)*2 +4 -2*1    = 22\n",
    "                    expand_block(dim_1, dim_0,                      kernel_size=4, stride=2, padding=1, output_padding=1, fill=fill, norm=norm),  # H = (21-1)*2 +4 -2*1 +1 = 45\n",
    "                    expand_block(dim_0, out_chan, final_layer=True, kernel_size=4, stride=2, padding=1, output_padding=0, fill=fill, norm=norm),  # H = (45-1)*2 +4 -2*1    = 90\n",
    "                )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # This method gets run when the network is called to produce an output from an input #\n",
    "\n",
    "        batch_size = len(input)  # Get batch size\n",
    "\n",
    "        a = self.contract(input) # Run input through contracting blocks\n",
    "        a = self.neck(a)         # Run output from contracting blocks through the neck\n",
    "        a = self.expand(a)       # Run outoput from the neck through the expanding blocks\n",
    "\n",
    "        if self.final_activation:   # Optional final activations\n",
    "            a = self.final_activation(a)\n",
    "        if self.normalize:          # Optionally normalize\n",
    "            a = torch.reshape(a,(batch_size, self.output_channels, 90**2)) # Flattens each image\n",
    "            a = nn.functional.normalize(a, p=1, dim = 2)\n",
    "            a = torch.reshape(a,(batch_size, self.output_channels , 90, 90)) # Reshapes images back into square matrices\n",
    "            a = self.scale*a        # If normalizing, multiply the outputs by a scale factor\n",
    "\n",
    "        return a                    # Return the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcAixWoDBypD"
   },
   "source": [
    "## Discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yCx0M-G0B04j"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "#### SINOGRAMS DISCRIMINATOR ####\n",
    "#################################\n",
    "\n",
    "class Disc_S_90(nn.Module):\n",
    "    '''\n",
    "    Through experimentation it has been found that sinogram discriminators work best with a fat network neck.\n",
    "    This class takes as input a 90x90.\n",
    "    '''\n",
    "    def __init__(self, config, disc_I=True, input_channels=3):\n",
    "        super(Disc_S_90, self).__init__()\n",
    "\n",
    "        hidden_dim=config['IS_disc_hidden_dim']\n",
    "        patchGAN=config['IS_disc_patchGAN']\n",
    "\n",
    "        ## Sequence 1 ##\n",
    "        self.seq1 = nn.Sequential(\n",
    "            # Sinogram Shape: (in_channels,90,90)\n",
    "            # nn.Conv2d: Hf = [Hi+2*padding-dilation(kernel-1)-1]/stride + 1\n",
    "            #               = [Hi+2*padding-kernel]/stride + 1 (for dialation=1)\n",
    "\n",
    "            # Feature Map Block\n",
    "            nn.Conv2d(in_channels=sino_channels, out_channels=hidden_dim, kernel_size=7, padding=3, padding_mode='reflect'),\n",
    "\n",
    "            # Contracting Block without normalization:\n",
    "            # H1 = (90-4)/2+1 = 44\n",
    "            nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim*2, kernel_size=4, stride=2, padding=0, padding_mode='reflect'),\n",
    "                nn.LeakyReLU(negative_slope=0.2),\n",
    "\n",
    "            # Contracting Blocks:\n",
    "            # H1 = (44-4)/2+1 = 21\n",
    "            nn.Conv2d(in_channels=hidden_dim*2, out_channels=hidden_dim*3, kernel_size=4, stride=2, padding=0, padding_mode='reflect'),\n",
    "                nn.InstanceNorm2d(hidden_dim*3), nn.LeakyReLU(negative_slope=0.2),\n",
    "            # H1 = (21-4)/2+1 = 9.5 = 9\n",
    "            nn.Conv2d(in_channels=hidden_dim*3, out_channels=hidden_dim*4, kernel_size=4, stride=2, padding=0, padding_mode='reflect'),\n",
    "                nn.InstanceNorm2d(hidden_dim*4), nn.LeakyReLU(negative_slope=0.2),\n",
    "            # H1 = (9-4)/2+1 = 3.5 = 3\n",
    "            nn.Conv2d(in_channels=hidden_dim*4, out_channels=hidden_dim*5, kernel_size=4, stride=2, padding=0, padding_mode='reflect'),\n",
    "                nn.InstanceNorm2d(hidden_dim*5), nn.LeakyReLU(negative_slope=0.2),\n",
    "        )\n",
    "\n",
    "        ## PatchGAN ##\n",
    "        if patchGAN==True:\n",
    "            # H = [3+2*1-3]/1+1 = 3 (3x3x3 matrix)\n",
    "            self.seq2 = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dim*5, hidden_dim*5, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                    nn.BatchNorm2d(hidden_dim*5), nn.LeakyReLU(negative_slope=0.2),\n",
    "            )\n",
    "        else:\n",
    "            self.seq2 = nn.Sequential(\n",
    "                # H0 = (3-3)/1+1 = 1 (1x1x3 matrix)\n",
    "                nn.Conv2d(in_channels=hidden_dim*5, out_channels=hidden_dim*5, kernel_size=3),\n",
    "                    nn.BatchNorm2d(hidden_dim*5), nn.LeakyReLU(negative_slope=0.2),\n",
    "            )\n",
    "        ## 1x1 Convolution ##\n",
    "        self.seq3 = nn.Conv2d(hidden_dim * 5, sino_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        a = self.seq1(image)\n",
    "        b = self.seq2(a) # a tensor\n",
    "        c = self.seq3(b)\n",
    "        #return disc_pred.view(len(disc_pred), -1) # returns a flattened tensor\n",
    "        return c.squeeze()\n",
    "\n",
    "##############################\n",
    "#### IMAGES DISCRIMINATOR ####\n",
    "##############################\n",
    "\n",
    "class Disc_I_90(nn.Module):\n",
    "    def __init__(self, config, disc_I=True, input_channels=3):\n",
    "        super(Disc_I_90, self).__init__()\n",
    "\n",
    "        hidden_dim=config['SI_disc_hidden_dim']\n",
    "        patchGAN=config['SI_disc_patchGAN']\n",
    "\n",
    "        ## Sequence 1 ##\n",
    "        self.seq1 = nn.Sequential(\n",
    "            # Image Shape: (1,90,90)\n",
    "            # nn.Conv2d: Hf = [Hi+2*padding-dilation(kernel-1)-1]/stride + 1\n",
    "            #               = [Hi+2*padding-kernel]/stride + 1 (for dialation=1)\n",
    "\n",
    "            # H = [90-4]/2+1 = 44\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=hidden_dim, kernel_size=4, stride=2),\n",
    "                nn.BatchNorm2d(hidden_dim), nn.LeakyReLU(negative_slope=0.2),\n",
    "            # H = [44-4]/2+1 = 21\n",
    "            nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim*2, kernel_size=4, stride=2),\n",
    "                nn.BatchNorm2d(hidden_dim*2), nn.LeakyReLU(negative_slope=0.2),\n",
    "            # H = [21-4]/2+1 = 9.5 = 9\n",
    "            nn.Conv2d(in_channels=hidden_dim*2, out_channels=hidden_dim*4, kernel_size=4, stride=2),\n",
    "                nn.BatchNorm2d(hidden_dim*4), nn.LeakyReLU(negative_slope=0.2),\n",
    "            # H = [9+2-4]/2+1 = 4.5 = 4\n",
    "            nn.Conv2d(in_channels=hidden_dim*4, out_channels=hidden_dim*4, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(hidden_dim*4), nn.LeakyReLU(negative_slope=0.2),\n",
    "        )\n",
    "\n",
    "        ## Sequence 2 ##\n",
    "        if patchGAN==True:\n",
    "            # H = [4+2-3]/1+1 = 4\n",
    "            self.seq2=nn.Conv2d(hidden_dim*4, 1, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "        else:\n",
    "            # H = [4-4]/2+1 = 1\n",
    "            self.seq2=nn.Conv2d(hidden_dim*4, 1, kernel_size=4, stride=2)\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        a = self.seq1(image)\n",
    "        disc_pred = self.seq2(a) # a tensor\n",
    "        #return disc_pred.view(len(disc_pred), -1) # returns a flattened tensor\n",
    "        return disc_pred.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDdfXS_b90J7"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc9w1e_ACjmb"
   },
   "source": [
    "## Cropping & Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-IbC9Sgg9RcT"
   },
   "outputs": [],
   "source": [
    "def crop_single_image_by_size(image, crop_size=-1):\n",
    "    '''\n",
    "    Function to crop a single image to a square shape, with even margins around the edges.\n",
    "\n",
    "    image:       Input image tensor of shape [height, width]\n",
    "    crop_size:   Edge size of (square) image to keep. The edges are discarded.\n",
    "    '''\n",
    "    x_size = image.shape[1]\n",
    "\n",
    "    margin_low = int((x_size-crop_size)/2.0)  # (90-71)/2 = 19/2 = 9.5 -->9\n",
    "    margin_high = x_size-crop_size-margin_low # 90-71-9 = 10\n",
    "\n",
    "    pix_min = 0 + margin_low\n",
    "    pix_max = x_size - margin_high\n",
    "\n",
    "    image = image[pix_min : pix_max , pix_min : pix_max]\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def crop_image_tensor_with_corner(batch, crop_size, corner=(0,0)):\n",
    "    '''\n",
    "    Function which returns a smaller, cropped version of a tensor (multiple images)\n",
    "\n",
    "    batch       a batch of images with dimensions: (num_images, channel, y_dimension, x_dimension)\n",
    "    corner      upper-left corner of window\n",
    "    crop_size   size of cropping window\n",
    "    '''\n",
    "\n",
    "    y_min = corner[0]\n",
    "    y_max = corner[0]+crop_size\n",
    "    x_min = corner[1]\n",
    "    x_max = corner[1]+crop_size\n",
    "\n",
    "    return batch[:, :, y_min:y_max , x_min:x_max ]\n",
    "\n",
    "\n",
    "## Crop Image by Factor ##\n",
    "def crop_image_tensor_by_factor(image_tensor, crop_factor=1):\n",
    "    '''\n",
    "    Function to crop an image tensor.\n",
    "\n",
    "    image_tensor:   Input image tensor of shape [image number, channel, height, width]\n",
    "    crop_factor:    Fraction of image to keep. The images are trimmed so the edges are discarded.\n",
    "    '''\n",
    "    x_size = image_tensor.shape[3]\n",
    "    y_size = image_tensor.shape[2]\n",
    "\n",
    "    margin = int(x_size*(1-crop_factor)/2)\n",
    "\n",
    "    x_min = 0 + margin\n",
    "    x_max = x_size - margin\n",
    "    y_min = 0 + margin\n",
    "    y_max = y_size - margin\n",
    "\n",
    "    return image_tensor[:,:, y_min:y_max , x_min:x_max ]\n",
    "\n",
    "def weights_init(m): # 'm' represents layers in the generator or discriminator.\n",
    "\n",
    "    #Function for initializing network weights to normal distribution, with mean 0 and s.d. 0.02\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjQTwQeHF6BO"
   },
   "source": [
    "## Reconstructions & Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "0WD7RY-uF13-"
   },
   "outputs": [],
   "source": [
    "def iradon_MLEM(sino_ground, azi_angles=None, max_iter=15, circle=True, crop_size=64):\n",
    "    '''\n",
    "    Function to reconstruct a single PET image from a single sinogram using ML-EM.\n",
    "\n",
    "    sino_ground:    sinogram (photopeak). This is a numpy array with minimum values of 0.\n",
    "    azi_angles:     list of azimuthal angles for sinogram. If set to None, angles are assumed to span [0,180)\n",
    "    max_iter:       Maximum number of iterations for ML-EM algorithm.\n",
    "    circle:         circle=True: The projection data spans the width (or height) of the activity distribution, and the reconstructed image is circular.\n",
    "                    circle=False: The projection data (sinograms) spans the corner-to-corner line of the activity distribution, and the reconstructed image is square.\n",
    "    crop_size:      Size to crop the image to after performing ML-EM. For ML-EM performed on a 90x90 sinogram, the\n",
    "                    output image will be 90x90. However, it is necessary to crop this to 64x64 to get the same FOV\n",
    "                    as the dataset.\n",
    "    '''\n",
    "    if azi_angles==None:\n",
    "        num_angles = sino_ground.shape[1] # Width\n",
    "        azi_angles=np.linspace(0, 180, num_angles, endpoint=False)\n",
    "\n",
    "    ## Create Sensitivity Image ##\n",
    "    sino_ones = np.ones(sino_ground.shape)\n",
    "    sens_image = iradon(sino_ones, azi_angles, circle=circle, filter_name=None)\n",
    "\n",
    "    if circle==False:\n",
    "        def modify_sens(image, const_factor=0.9, slope=0.03):\n",
    "            '''\n",
    "            Modifies an image so that the area in the central FOV remains constant, but values at edges are attenuated.\n",
    "            image               image to modify\n",
    "            constant_factor     fraction of the image to leave alone\n",
    "            slope               increase this to attenuate images at the edges more\n",
    "            '''\n",
    "            def shape_piecewise(r, const_value, slope):\n",
    "                if r <= const_value:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 1+slope*(r-const_value)\n",
    "\n",
    "            y_max = image.shape[0]\n",
    "            x_max = image.shape[1]\n",
    "\n",
    "            const_dist = const_factor*x_max/2 # radius over which image remains constant\n",
    "\n",
    "            x_center = (x_max-1)/2.0 # the -1 comes from the fact that the coordinates of a pixel start at 0, not 1\n",
    "            y_center = (y_max-1)/2.0\n",
    "\n",
    "            for y in range(0, y_max):\n",
    "                for x in range(0, x_max):\n",
    "                    r = ((x-x_center)**2 + (y-y_center)**2)**0.5\n",
    "\n",
    "                    total_factor = shape_piecewise(r, const_dist, slope) # creates a circular shaped piece-wise\n",
    "                    #total_factor = shape_piecewise(abs(x-x_center), const_dist, slope) * shape_piecewise(abs(y-y_center), const_dist, slope) # square-shaped piecewise\n",
    "                    #total_factor = shape_piecewise(abs(y-y_center), const_dist, slope) # vertical only\n",
    "\n",
    "                    image[y,x] = image[y,x]*total_factor\n",
    "\n",
    "            return image\n",
    "        sens_image = modify_sens(sens_image)\n",
    "\n",
    "    ## Create blank reconstruction ##\n",
    "    image_recon  = np.ones(sens_image.shape)\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        if circle==True:\n",
    "            sens_image = sens_image + 0.001 # Guarantees the denominator is >0\n",
    "\n",
    "        sino_recon = radon(image_recon, azi_angles, circle=circle) #\n",
    "        sino_recon[sino_recon==0]=1000 # Set a limit on the denominator (next line)\n",
    "        sino_ratio = sino_ground / (sino_recon) #\n",
    "        image_ratio = iradon(sino_ratio, azi_angles, circle=circle, filter_name=None) / sens_image\n",
    "        image_ratio[image_ratio>1.5]=1.5 # Sets limit on backprojected ratio, on how fast image can grow. Threshold and set value should equal each other (good value=1.5)\n",
    "        image_recon = image_recon * image_ratio\n",
    "        image_recon[image_recon<0]=0 # Sets floor on image pixels. No need to adjust.\n",
    "\n",
    "        #footprint = morphology.disk(1)\n",
    "        #image_recon = opening(image_recon, footprint)\n",
    "\n",
    "    image_cropped = crop_single_image_by_size(image_recon, crop_size=crop_size)\n",
    "\n",
    "    return image_cropped\n",
    "\n",
    "def reconstruct(sinogram_tensor, config, image_size=90, recon_type='FBP', circle=True):\n",
    "    '''\n",
    "    Function for calculating a reconstructed PET image tensor, given a sinogram_tensor. One image is reconstructed for\n",
    "    each sinogram in the sinogram_tensor.\n",
    "\n",
    "    sinogram_tensor:    Tensor of sinograms of size (number of images)x(channels)x(height)x(width).\n",
    "                        Only the first channel (photopeak) is used for recontruction here.\n",
    "    config:             configuration dictionary\n",
    "    image_size:         size of output (images are resized to this shape)\n",
    "    recon_type:         Can be set to 'MLEM' for maximum-likelihood expectation maximization, or 'FBP' for\n",
    "                        filtered back-projection.\n",
    "    circle              circle=True: The projection data spans the width (or height) of the activity distribution, and the reconstructed image is circular.\n",
    "                        circle=False: The projection data (sinograms) spans the corner-to-corner line of the activity distribution, and the reconstructed image is square.\n",
    "\n",
    "    Function returns a tensor of reconstructed images. Returned images are resized, and normalized and scaled (according to the keys in the configuration dictionary.)\n",
    "    '''\n",
    "    normalize = config[\"SI_normalize\"]\n",
    "    scale = config['SI_scale']\n",
    "\n",
    "    photopeak_array = torch.clamp(sinogram_tensor[:,0,:,:], min=0).detach().cpu().numpy()\n",
    "    # Note: there really is no need to clamp the sinogram as it contains no negative values.\n",
    "\n",
    "    ## Reconstruct Individual Sinograms ##\n",
    "    first=True\n",
    "    for sino in photopeak_array[0:,]:\n",
    "        if recon_type == 'FBP':\n",
    "            image = iradon(sino.squeeze(),\n",
    "                        circle=False, # For an unknown reason, circle=False gives better reconstructions here. Maybe due to errors introduced in interpolation.\n",
    "                        preserve_range=True,\n",
    "                        filter_name='cosine' # Options: 'ramp', 'shepp-logan', 'cosine', 'hamming', 'hann'\n",
    "                        )\n",
    "        else:\n",
    "            image = iradon_MLEM(sino, circle=circle)\n",
    "\n",
    "        ## Morphologic Opening - removes outlier pixels than can cause problems with image normalization\n",
    "        #footprint = morphology.disk(1)\n",
    "        #image = opening(image, footprint)\n",
    "\n",
    "        ## Concatenate Images ##\n",
    "        image = np.expand_dims(image, axis=0) # Add a dimension to the beginning of the reconstructed image\n",
    "        if first==True:\n",
    "            image_array = image\n",
    "            first=False\n",
    "        else:\n",
    "            image_array = np.append(image_array, image, axis=0)\n",
    "\n",
    "    ## For All Images: create resized/dimensioned Torch tensor ##\n",
    "    image_array = np.expand_dims(image_array, axis=1)        # Creates channels dimension\n",
    "    a = torch.from_numpy(image_array)                        # Converts to Torch tensor\n",
    "    a = torch.clamp(a, min=0)                                # You HAVE to clamp before normalizing or the negative values throw it off.\n",
    "    a = transforms.Resize(size = (image_size, image_size), antialias=True)(a) # Resize tensor\n",
    "\n",
    "    ## Normalize Entire Tensor ##\n",
    "    if normalize:\n",
    "        batch_size = len(a)\n",
    "        a = torch.reshape(a,(batch_size, 1, image_size**2)) # Flattens each image\n",
    "        a = nn.functional.normalize(a, p=1, dim = 2)\n",
    "        a = torch.reshape(a,(batch_size, 1 , image_size, image_size)) # Reshapes images back into square matrices\n",
    "        a = scale*a\n",
    "\n",
    "    return a.to(device)\n",
    "\n",
    "def project(image_tensor, circle=False, theta=-1):\n",
    "    '''\n",
    "    Perform the forward radon transform to calculate projections from images. Returns an array of sinograms.\n",
    "\n",
    "    image_tensor:   tensor of PET images\n",
    "    theta:          numpy array of projection angles. Default is [0,180)\n",
    "    '''\n",
    "    image_collapsed = torch.clamp(image_tensor[:,0,:,:], min=0).detach().squeeze().cpu().numpy()\n",
    "\n",
    "    if theta==-1:\n",
    "        theta = np.arange(0,180)\n",
    "\n",
    "    first=True\n",
    "    for image in image_collapsed[0:,]:\n",
    "        sino = radon(image,\n",
    "                    circle=circle,\n",
    "                    preserve_range=True,\n",
    "                    theta=theta,\n",
    "                    )\n",
    "        sino = np.moveaxis(np.atleast_3d(sino), 2, 0) # Adds a blank axis and moves it to the beginning\n",
    "        if first==True:\n",
    "            sino_array=sino\n",
    "            first=False\n",
    "        else:\n",
    "            sino_array = np.append(sino_array, sino, axis=0)\n",
    "\n",
    "    return torch.from_numpy(sino_array)\n",
    "\n",
    "'''\n",
    "### Functions that are no longer used ###\n",
    "\n",
    "def FBP2(sinogram_tensor, config, image_size = 90, circle=circle):\n",
    "    #This is an alternative filtered back-projection implementation. Not currently used.\n",
    "\n",
    "    normalize = config[\"SI_normalize\"]\n",
    "    scale = config['SI_scale']\n",
    "\n",
    "    photopeak_array = sinogram_tensor[:,0,:,:].detach().squeeze().cpu().numpy()\n",
    "    # Note: there's no need to clamp the sinogram as it contains no negative values.\n",
    "\n",
    "    first=True\n",
    "    for sino in photopeak_array[0:,]:\n",
    "        image = iradon(sino,\n",
    "                    circle=circle,\n",
    "                    preserve_range=True,\n",
    "                    filter_name='cosine' # Options: 'ramp', 'shepp-logan', 'cosine', 'hamming', 'hann'\n",
    "                    )\n",
    "\n",
    "        ## For Individual Images: create resized/dimensioned Torch tensors ##\n",
    "        image = np.expand_dims(image, axis = 0) # Creates an extra dimension at beginning for images in the batch.\n",
    "        image = torch.from_numpy(image)             # I convert the array to a tensor so I can perform the the resizing and clamping below\n",
    "        image = torch.clamp(image, min=0)           # Clamping\n",
    "        image = (transforms.Resize(size = (image_size, image_size))(image)).numpy() # I convert back to Numpy so I can use the append function later.\n",
    "\n",
    "        ## Normalize each individual image ##\n",
    "        if normalize==True:\n",
    "            image = image/np.sum(image)\n",
    "\n",
    "        if first==True:\n",
    "            image_array = image\n",
    "            first=False\n",
    "        else:\n",
    "            image_array = np.append(image_array, image, axis=0)\n",
    "\n",
    "    image_array = np.expand_dims(image_array, axis=1) # Creates channels dimension\n",
    "\n",
    "    return scale*torch.from_numpy(image_array).to(device)\n",
    "\n",
    "\n",
    "def shape_smooth(r,R=10000): # NOTE: this function is currently not used\n",
    "\n",
    "    #Returns a smoothly varying function. At r=0, returns 1. As r increases, returned value decreases.\n",
    "    #The function if a portion of a circle.\n",
    "\n",
    "    return (R**2-r**2)**0.5+1-R\n",
    "\n",
    "\n",
    "def weights_init(m): # 'm' represents layers in the generator or discriminator.\n",
    "\n",
    "    #Function for initializing network weights to normal distribution, with mean 0 and s.d. 0.02\n",
    "\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.uniform_(m.weight, 0, 0.0001)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.uniform_(m.weight, 0, 0.0001)\n",
    "        torch.nn.init.constant_(m.bias, 0.02)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Doag6oulGKVo"
   },
   "source": [
    "## Display Images - POSSIBLY USEFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bV8lRESBGS52"
   },
   "outputs": [],
   "source": [
    "def show_single_unmatched_tensor(image_tensor, grid=False, cmap='jet', fig_size=1):\n",
    "    '''\n",
    "    Function for visualizing images. The images are displayed, each with their own colormap scaling, so quantitative comparisons are not possible.\n",
    "    You may send any size image tensor to this function. Only a certain number will be plotted. Works with 1-channel or 3-channel images.\n",
    "    If using the single-channel grid option, it plots 120 images in a 15x8 grid.\n",
    "\n",
    "    image_tensor:   image tensor of shape [num, chan, height, width]\n",
    "    grid:           If True, displays images in a 15x8 grid (120 images in total). If false, images are displayed in a horizontal line.\n",
    "    cmap:           Matplotlib color map\n",
    "    fig_size:       figure size\n",
    "    '''\n",
    "    print(f'Shape: {image_tensor.shape} // Min: {torch.min(image_tensor)} // Max: {torch.max(image_tensor)} \\\n",
    "    //Mean Sum (per image): {torch.sum(image_tensor).item()/(image_tensor.shape[0]*image_tensor.shape[1])} // Sum (a single image): {torch.sum(image_tensor[0,0,:])}')\n",
    "\n",
    "    image_tensor=image_tensor.detach().squeeze().cpu()\n",
    "    image_tensor = torch.clamp(image_tensor, min=0)\n",
    "    #image_np = image_grid.mean(dim=0).squeeze().numpy() # This also works!\n",
    "\n",
    "    ## Plot 3 Channel Images ##\n",
    "    if image_tensor.size(dim=1)==3:\n",
    "        print(f'Mean (Ch 0): {torch.mean(image_tensor[:,0,:,:])} // Mean (Ch 1): {torch.mean(image_tensor[:,1,:,:])} // Mean (Ch 2): {torch.mean(image_tensor[:,2,:,:])}')\n",
    "\n",
    "        # Plot Grid #\n",
    "        if grid:\n",
    "            num, chan = 3, 3\n",
    "            fig_size=fig_size\n",
    "\n",
    "            fig, ax = plt.subplots(num, chan, figsize=(fig_size*num, fig_size*chan), constrained_layout=True)\n",
    "            for N in range(0, num): # Iterate over image number\n",
    "                for C in range(0, chan): # Iterate over channels\n",
    "                    img = image_tensor[N,C,:,:]\n",
    "                    ax[N,C].axis('off')\n",
    "                    ax[N,C].imshow(img.squeeze(), cmap=cmap)\n",
    "\n",
    "        # Plot in-Line #\n",
    "        else:\n",
    "            num, chan = 3, 3\n",
    "            fig_size=fig_size\n",
    "\n",
    "            fig, ax = plt.subplots(1, num*(chan+1), figsize=(fig_size, fig_size*num*(chan+1)), constrained_layout=True)\n",
    "            i=0\n",
    "            for N in range(0, num): # Iterate over image number\n",
    "                for C in range(0, chan): # Iterate over channels\n",
    "                    img = image_tensor[N,C,:,:]\n",
    "                    ax[i].axis('off')\n",
    "                    ax[i].imshow(img.squeeze(), cmap=cmap)\n",
    "                    i+=1\n",
    "                blank = torch.ones_like(img)\n",
    "                ax[i].axis('off')\n",
    "                ax[i].imshow(blank.squeeze())\n",
    "                i+=1\n",
    "\n",
    "    ## Plot 1 Channel Images ##\n",
    "    else:\n",
    "        # Plot Grid #\n",
    "        # Note: This plots 120 images at a time!\n",
    "        if grid:\n",
    "            fig_size=fig_size\n",
    "            cols, rows = 15, 8\n",
    "            figure=plt.figure(figsize=(cols*fig_size,rows*fig_size))\n",
    "        # Plot in-Line #\n",
    "        else:\n",
    "            fig_size=fig_size\n",
    "            cols, rows = 9 , 1\n",
    "            figure=plt.figure(figsize=(cols*fig_size,rows*fig_size))\n",
    "\n",
    "        for i in range(0, cols*rows):\n",
    "            img = image_tensor[i]\n",
    "            figure.add_subplot(rows,cols,i+1) # MatplotLib indeces start at 1\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(img.squeeze(), cmap=cmap)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def show_multiple_matched_tensors(*image_tensors, cmap='jet', fig_size=0.8):\n",
    "    '''\n",
    "    Function for visualizing images from multiple tensors. Each image is \"matched\" with images from the other tensors,\n",
    "    and each matched set of images (one from each tensor) is plotted with the same colormap.\n",
    "    Send only the images you want plotted to this function. Works with both single channel and multi-channel images.\n",
    "\n",
    "    image_tensors:  list of tensors, each of which may contain multiple images.\n",
    "    '''\n",
    "    for tensor in image_tensors:\n",
    "        # Begin by printing statistics for each tensor\n",
    "        print(f'Shape: {tensor.shape} // Min: {torch.min(tensor)} // Max: {torch.max(tensor)} \\\n",
    "        // Mean: {torch.mean(tensor)} // Mean Sum (per image): {torch.sum(tensor).item()/(tensor.shape[0]*tensor.shape[1])} // Sum (a single image): {torch.sum(tensor[0,0,:])}')\n",
    "\n",
    "    combined_tensor = torch.cat(image_tensors, dim=0).detach().cpu()\n",
    "    combined_tensor = torch.clamp(combined_tensor, min=0)\n",
    "\n",
    "    num_rows = len(image_tensors)           # The number of rows equals the number of tensors (images to match)\n",
    "    num_cols = len(image_tensors[0])        # Each set of matched images is displayed in a single column.\n",
    "    num_chan = image_tensors[0].size(dim=1)\n",
    "\n",
    "    ## Plot 1 Channel Images ##\n",
    "    if num_chan==1:\n",
    "        fig_size=fig_size # Figure size for individual image\n",
    "        fig, ax = plt.subplots(num_rows, num_cols, squeeze=False, figsize=(fig_size*num_cols, fig_size*num_rows), constrained_layout=True)\n",
    "        #fig, ax = plt.subplots(num_rows, num_cols, constrained_layout=True)\n",
    "\n",
    "        i=0 # i = column number\n",
    "        for col in range(0, num_cols): # Iterate over column number\n",
    "            img_list=[]\n",
    "            min_list=[]\n",
    "            max_list=[]\n",
    "\n",
    "            # Construct image list and normalization object for matched images in a column (iterating over rows) #\n",
    "            for row in range(0,num_rows):                               # We iterate over rows in orcer\n",
    "                img = combined_tensor[row*num_cols+col, 0 ,:,:]         # Grab the right image\n",
    "                img_list.append(img)                                    # We construct a new image list for each row\n",
    "                min_list.append(torch.min(img).item())                  # Create list of image minimums\n",
    "                max_list.append(torch.max(img).item())                  # Create list of image maximums\n",
    "            norm = Normalize(vmin=min(min_list), vmax=max(max_list))    # We construct a normalization object with min/max = min/max pixel value for all images in list\n",
    "\n",
    "            # Plot normalized images in a single column (iterating over rows) #\n",
    "            for row in range(0,num_rows):\n",
    "                ax[row, i].axis('off')\n",
    "                ax[row, i].imshow(img_list[row].squeeze(), cmap=cmap, norm=norm) # Squeeze gets rid of extra channel dimension\n",
    "            i+=1\n",
    "\n",
    "    ## Plot Multi-Channel Images ##\n",
    "    else:\n",
    "        print(f'Mean (Ch 0): {torch.mean(combined_tensor[:,0,:,:])} // Mean (Ch 1): {torch.mean(combined_tensor[:,1,:,:])} // Mean (Ch 2): {torch.mean(combined_tensor[:,2,:,:])}')\n",
    "\n",
    "        if num_cols>3:  # Restricts to 3-channels. You could get rid of this without an issue.\n",
    "            num_cols=3\n",
    "\n",
    "        fig_size=fig_size\n",
    "        # Construct figure and axes. Note: 'num_chan+1' arises from the divider btw. each multi-channel image\n",
    "        fig, ax = plt.subplots(num_rows, num_cols*(num_chan+1), squeeze=False, figsize=(fig_size*num_cols*(num_chan+1), fig_size*num_rows), constrained_layout=True)\n",
    "\n",
    "        i=0\n",
    "        for col in range(0, num_cols):      # Iterate over column number\n",
    "            for chan in range(0, num_chan): # Iterate over channels\n",
    "                img_list=[]\n",
    "                min_list=[]\n",
    "                max_list=[]\n",
    "\n",
    "                # Iterates over rows to construct an image list and normalization object a single column. All matched images have the same channel. #\n",
    "                for row in range(0,num_rows):\n",
    "                    img = combined_tensor[row*num_cols+col, chan ,:,:] # Constructs an image list where each row has the same channel #\n",
    "                    img_list.append(img)\n",
    "                    min_list.append(torch.min(img).item())\n",
    "                    max_list.append(torch.max(img).item())\n",
    "                norm = Normalize(vmin=min(min_list), vmax=max(max_list))\n",
    "\n",
    "                # Iterates over rows to plot matched images in a single column. These share the same channel. #\n",
    "                for row in range(0,num_rows):\n",
    "                    ax[row, i].axis('off')\n",
    "                    ax[row, i].imshow(img_list[row].squeeze(), cmap=cmap, norm=norm) # Squeeze gets rid of extra channel dimension\n",
    "                i+=1\n",
    "\n",
    "            # After all channels have been iterated, the complete multi-channel image has been plotted. Now we plot a divider before the next image #\n",
    "            for row in range(0,num_rows):\n",
    "                blank = torch.ones_like(img)\n",
    "                ax[row, i].axis('off')\n",
    "                ax[row, i].imshow(blank.squeeze())\n",
    "            i+=1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def show_single_commonmap_tensor(image_tensor, cmap='jet'):\n",
    "    '''\n",
    "    Function for visualizing images from one tensor, all of which will be plotted with the same scaled colormap. Only works with single-channel image tensors.\n",
    "\n",
    "    *image_tensor:  tensor of exactly 120 images (will plot to a grid of 15x8)\n",
    "    '''\n",
    "    tensor = torch.clamp(image_tensor, min=0).detach().cpu()\n",
    "    image_grid = make_grid(tensor, nrow=15)\n",
    "\n",
    "    #print(f'Shape: {tensor.shape} // Min: {torch.min(tensor)} // Max: {torch.max(tensor)} \\\n",
    "    #// Mean: {torch.mean(tensor)} // Mean Sum (per image): {torch.sum(tensor).item()/(tensor.shape[0]*tensor.shape[1])} // Sum (a single image): {torch.sum(tensor[0,0,:])}')\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=(27, 18))\n",
    "    ax.axis('off')\n",
    "\n",
    "    image_grid = image_grid[0,:].squeeze()\n",
    "    #plt.imshow(image_grid, cmap=cmap)\n",
    "    im = ax.imshow(image_grid, cmap=cmap)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "def show_multiple_commonmap_tensors(*image_tensors, cmap='jet'):\n",
    "    '''\n",
    "    Function for visualizing images from multiple tensors, all of which will be plotted with the same scaled colormap. Only works with single-channel image tensors.\n",
    "\n",
    "    *image_tensors: list of image tensors, all of which should contain the same number of images. Only send the number of images you want to plot to this function.\n",
    "    '''\n",
    "    # Print tensor statistics #\n",
    "    for tensor in image_tensors:\n",
    "        print(f'Shape: {tensor.shape} // Min: {torch.min(tensor)} // Max: {torch.max(tensor)} \\\n",
    "        // Mean: {torch.mean(tensor)} // Mean Sum (per image): {torch.sum(tensor).item()/(tensor.shape[0]*tensor.shape[1])} // Sum (a single image): {torch.sum(tensor[0,0,:])}')\n",
    "\n",
    "    num_rows = len(image_tensors)\n",
    "    num_columns = len(image_tensors[0])\n",
    "    # Combine tensors into one & clamp #\n",
    "    combined_tensor = torch.cat(image_tensors, dim=0).detach().cpu()\n",
    "    combined_tensor = torch.clamp(combined_tensor, min=0)\n",
    "    # Make a grid of the tensors #\n",
    "    image_grid = make_grid(combined_tensor, nrow=num_columns) # Note: nrow is the number of images displayed in each row (i.e., the number of columns)\n",
    "\n",
    "    # Determine figure size #\n",
    "    print('num_rows:', num_rows)\n",
    "    fig, ax = plt.subplots(1,1, figsize=(30,1*num_rows))\n",
    "    #fig, ax = plt.subplots(1,1, figsize=(30,7))\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "    image_grid = image_grid[0,:].squeeze()\n",
    "    im = ax.imshow(image_grid, cmap=cmap)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ajsf7Iok0X2I"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NzzJJ4-70XE5"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "## Functions for Calculating Metrics Dataframes ##\n",
    "##################################################\n",
    "\n",
    "def update_tune_dataframe(tune_dataframe, model, config, mean_CNN_MSE, mean_CNN_SSIM, mean_CNN_CUSTOM):\n",
    "    '''\n",
    "    Function to update the tune_dataframe for each trial run that makes it partway through the tuning process.\n",
    "    '''\n",
    "    # Extract values from config dictionary\n",
    "    SI_dropout =        config['SI_dropout']\n",
    "    SI_exp_kernel =     config['SI_exp_kernel']\n",
    "    SI_gen_fill =       config['SI_gen_fill']\n",
    "    SI_gen_hidden_dim = config['SI_gen_hidden_dim']\n",
    "    SI_gen_neck =       config['SI_gen_neck']\n",
    "    SI_layer_norm =     config['SI_layer_norm']\n",
    "    SI_normalize =      config['SI_normalize']\n",
    "    SI_pad_mode =       config['SI_pad_mode']\n",
    "    batch_size =        config['batch_size']\n",
    "    gen_lr =            config['gen_lr']\n",
    "\n",
    "    # Calculate number of trainable weights in CNN\n",
    "    num_params = sum(map(torch.numel, model.parameters()))\n",
    "\n",
    "    # Concatenate Dataframe\n",
    "    add_frame = pd.DataFrame({'SI_dropout': SI_dropout, 'SI_exp_kernel': SI_exp_kernel, 'SI_gen_fill': SI_gen_fill, 'SI_gen_hidden_dim': SI_gen_hidden_dim,\n",
    "                            'SI_gen_neck': SI_gen_neck, 'SI_layer_norm': SI_layer_norm, 'SI_normalize': SI_normalize, 'SI_pad_mode': SI_pad_mode, 'batch_size': batch_size,\n",
    "                            'gen_lr': gen_lr, 'num_params': num_params, 'mean_CNN_MSE': mean_CNN_MSE, 'mean_CNN_SSIM': mean_CNN_SSIM, 'mean_CNN_CUSTOM': mean_CNN_CUSTOM}, index=[0])\n",
    "\n",
    "    tune_dataframe = pd.concat([tune_dataframe, add_frame], axis=0)\n",
    "\n",
    "    # Save Dataframe to File\n",
    "    tune_dataframe.to_csv(tune_dataframe_path, index=False)\n",
    "\n",
    "    return tune_dataframe\n",
    "\n",
    "## Construct Dataframes ##\n",
    "def update_test_dataframe_and_return_outputs(sino_tensor, image_size, CNN_output, ground_image, test_dataframe, config):\n",
    "    '''\n",
    "    Function which: A) performs reconstructions (FBP and possibly ML-EM)\n",
    "                    B) constructs a dataframe of metric values (MSE & SSIM) for these reconstructions, and also for the CNN output, with respect to the ground truth image.\n",
    "                    C) concatenates this with the test dataframe passed to this function\n",
    "                    D) returns the concatenated dataframe, mean metric values, and reconstructions\n",
    "\n",
    "    sino_tensor:    sinogram tensor of shape [num, chan, height, width]\n",
    "    image_size:     image_size\n",
    "    CNN_output:     CNN reconstruction\n",
    "    ground_image:   ground truth image\n",
    "    test_dataframe: dataframe to append metric values to\n",
    "    config:         general config dictionary\n",
    "    '''\n",
    "    # Construct Outputs #\n",
    "    FBP_output = reconstruct(sino_tensor, config, image_size=image_size, recon_type='FBP')\n",
    "    if compute_MLEM==True:\n",
    "        MLEM_output = reconstruct(sino_tensor, config, image_size=image_size, recon_type='MLEM')\n",
    "    else: # If not looking at ML-EM, don't waste time computing the MLEM images, which can take awhile.\n",
    "        MLEM_output = FBP_output\n",
    "\n",
    "    # Dataframes: build dataframes for every reconstruction technique/metric combination #\n",
    "    batch_CNN_MSE,  mean_CNN_MSE   = calculate_metric(ground_image, CNN_output, MSE,  return_dataframe=True, label='MSE (Network)')\n",
    "    batch_CNN_SSIM,  mean_CNN_SSIM = calculate_metric(ground_image, CNN_output, SSIM, return_dataframe=True, label='SSIM (Network)')\n",
    "    batch_FBP_MSE,  mean_FBP_MSE   = calculate_metric(ground_image, FBP_output, MSE,  return_dataframe=True, label='MSE (FBP)')\n",
    "    batch_FBP_SSIM,  mean_FBP_SSIM = calculate_metric(ground_image, FBP_output, SSIM, return_dataframe=True, label='SSIM (FBP)')\n",
    "    batch_MLEM_MSE, mean_MLEM_MSE  = calculate_metric(ground_image, MLEM_output, MSE, return_dataframe=True, label='MSE (ML-EM)')\n",
    "    batch_MLEM_SSIM, mean_MLEM_SSIM= calculate_metric(ground_image, MLEM_output, SSIM,return_dataframe=True, label='SSIM (ML-EM)')\n",
    "\n",
    "    # Concatenate batch dataframes and larger running test dataframe\n",
    "    add_frame = pd.concat([batch_CNN_MSE, batch_FBP_MSE, batch_MLEM_MSE, batch_CNN_SSIM, batch_FBP_SSIM, batch_MLEM_SSIM], axis=1)\n",
    "    test_dataframe = pd.concat([test_dataframe, add_frame], axis=0)\n",
    "\n",
    "    # Return a whole lot of stuff\n",
    "    return test_dataframe, mean_CNN_MSE, mean_CNN_SSIM, mean_FBP_MSE, mean_FBP_SSIM, mean_MLEM_MSE, mean_MLEM_SSIM, FBP_output, MLEM_output\n",
    "\n",
    "## Calculate Arbitrary Metric ##\n",
    "def calculate_metric(batch_A, batch_B, img_metric_function, return_dataframe=False, label='default', crop_factor=1):\n",
    "    '''\n",
    "    Function which calculates metric values for two batches of images.\n",
    "    Returns either the average metric value for the batch or a dataframe of individual image metric values.\n",
    "\n",
    "    batch_A:                tensor of images to compare [num, chan, height, width]\n",
    "    batch_B:                tensor of images to compare [num, chan, height, width]\n",
    "    img_metric_function:    a function which calculates a metric (MSE, SSIM, etc.) from two INDIVIDUAL images\n",
    "    return_dataframe:       If False, then the average is returned.\n",
    "                            Otherwise a dataframe containing the metric values of the images in the batch, and the average, are returned.\n",
    "    label:                  what to call dataframe, if it is created\n",
    "    crop_factor:            factor by which to crop the images. 1 = whole image retained.\n",
    "    '''\n",
    "\n",
    "    if crop_factor != 1:\n",
    "        A = crop_image_tensor_by_factor(batch_A, crop_factor=crop_factor)\n",
    "        B = crop_image_tensor_by_factor(batch_B, crop_factor=crop_factor)\n",
    "\n",
    "    length = len(batch_A)\n",
    "    metric_avg = 0\n",
    "    metric_list = []\n",
    "\n",
    "    for i in range(length):\n",
    "        image_A = batch_A[i:i+1,:,:,:] # I use i:i+1 instead of just i preserves the dimensionality of the array\n",
    "        image_B = batch_B[i:i+1,:,:,:]\n",
    "\n",
    "        metric_value = img_metric_function(image_A, image_B)\n",
    "        metric_avg += metric_value/length\n",
    "        if return_dataframe==True:\n",
    "            metric_list.append(metric_value)\n",
    "\n",
    "    metric_frame = pd.DataFrame({label : metric_list})\n",
    "\n",
    "    if return_dataframe==False:\n",
    "        return metric_avg\n",
    "    else:\n",
    "        return metric_frame, metric_avg\n",
    "\n",
    "######################\n",
    "## Metric Functions ##\n",
    "######################\n",
    "\n",
    "## Metrics which take only single images as inputs ##\n",
    "def SSIM(image_A, image_B, win_size=-1):\n",
    "    '''\n",
    "    Function to return the SSIM for two 2D images.\n",
    "\n",
    "    image_A:        pytorch tensor for a single image\n",
    "    image_B:        pytorch tensor for a single image\n",
    "    win_size:       window size to use when computing the SSIM. If =-1, the full size of the image is used.\n",
    "    '''\n",
    "\n",
    "    if win_size == -1:   # The default shape of the window size is the same size as the image.\n",
    "        x = image_A.shape[2]\n",
    "        win_size = (x if x % 2 == 1 else x-1) # Guarantees the window size is odd.\n",
    "\n",
    "        image_A_npy = image_A.detach().squeeze().cpu().numpy()\n",
    "        image_B_npy = image_B.detach().squeeze().cpu().numpy()\n",
    "\n",
    "        max_value = max([np.amax(image_A_npy, axis=(0,1)), np.amax(image_B_npy, axis=(0,1))])   # Find maximum among the images\n",
    "        min_value = min([np.amin(image_A_npy, axis=(0,1)), np.amin(image_B_npy, axis=(0,1))])   # Find minimum among the images\n",
    "        data_range = max_value-min_value\n",
    "\n",
    "        SSIM_image = structural_similarity(image_A_npy, image_B_npy, data_range=data_range, gaussian_weights=False, use_sample_covariance=False, win_size=win_size)\n",
    "\n",
    "    return SSIM_image\n",
    "\n",
    "## Metrics which take either batches or images as inputs ##\n",
    "def MSE(image_A, image_B):\n",
    "    '''\n",
    "    Function to return the mean square error for two 2D images (or two batches of images).\n",
    "\n",
    "    image_A:        pytorch tensor for a single image\n",
    "    image_B:        pytorch tensor for a single image\n",
    "    '''\n",
    "    image_A_npy = image_A.detach().squeeze().cpu().numpy()\n",
    "    image_B_npy = image_B.detach().squeeze().cpu().numpy()\n",
    "\n",
    "    return torch.mean((image_A-image_B)**2).item()\n",
    "\n",
    "def MAE(image_A, image_B):\n",
    "    '''\n",
    "    Function to return the mean absolute error for two 2D images (or two batches of images).\n",
    "\n",
    "    image_A:        pytorch tensor for a single image\n",
    "    image_B:        pytorch tensor for a single image\n",
    "    '''\n",
    "    image_A_npy = image_A.detach().squeeze().cpu().numpy()\n",
    "    image_B_npy = image_B.detach().squeeze().cpu().numpy()\n",
    "\n",
    "    return torch.mean(torch.abs(image_A-image_B)).item()\n",
    "\n",
    "def LDM_window(batch_A, batch_B, window_size = 10, stride=10, dataframe=False):\n",
    "    '''\n",
    "    Function to return the local distributions metrics for two image tensors.\n",
    "    '''\n",
    "    ## Nested Functions ##\n",
    "\n",
    "    def compare_moments(win_A, win_B, moment):\n",
    "        def compute_moment(win, moment, axis=1):\n",
    "            mean_column = np.mean(win, axis=axis)\n",
    "            if moment == 1:\n",
    "                return mean_column\n",
    "            else:\n",
    "                mean_array = np.array([mean_column] * win.shape[1]).T\n",
    "                moment = np.mean((win - mean_array)**moment, axis=1)\n",
    "                return moment\n",
    "\n",
    "        batch_size = win_A.shape[0]\n",
    "\n",
    "\n",
    "        reshape_A = (torch.reshape(win_A, (batch_size, -1))).detach().cpu().numpy()\n",
    "        reshape_B = (torch.reshape(win_B, (batch_size, -1))).detach().cpu().numpy()\n",
    "\n",
    "        moment_A = compute_moment(reshape_A, moment=moment)\n",
    "        moment_B = compute_moment(reshape_B, moment=moment)\n",
    "        moment_score = np.mean(np.absolute(moment_A-moment_B)/(np.absolute(moment_A)+0.1))\n",
    "\n",
    "        '''\n",
    "        print('===============================')\n",
    "        print('MOMENT: ', moment)\n",
    "        print('moment_A shape: ', moment_A.shape)\n",
    "        print('moment_A mean: ', np.mean(moment_A))\n",
    "        print('moment_B shape: ', moment_B.shape)\n",
    "        print('moment_B mean: ', np.mean(moment_B))\n",
    "        print('moment_score, |moment_A-moment_B|/(moment_A+0.1) : ', moment_score)\n",
    "        '''\n",
    "        return moment_score\n",
    "\n",
    "    ## Code ##\n",
    "    image_size = batch_A.shape[2]\n",
    "\n",
    "    num_windows = int((image_size)/stride) # Maximum number of windows occurs when: stride = window_size.\n",
    "    while (num_windows-1)*stride + window_size > image_size: # If stride < crop_size, we need fewer, and we need to solve for the number of crops\n",
    "        num_windows += -1\n",
    "\n",
    "    moment_1_running_score = 0\n",
    "    moment_2_running_score = 0\n",
    "    moment_3_running_score = 0\n",
    "\n",
    "    for i in range(0, num_windows):\n",
    "        for j in range(0, num_windows):\n",
    "            corner = (i*stride, j*stride)\n",
    "\n",
    "            win_A = crop_image_tensor_with_corner(batch_A, window_size, corner)\n",
    "            win_B = crop_image_tensor_with_corner(batch_B, window_size, corner)\n",
    "\n",
    "            moment_1_score = compare_moments(win_A, win_B, moment=1)\n",
    "            moment_2_score = compare_moments(win_A, win_B, moment=2)\n",
    "            moment_3_score = compare_moments(win_A, win_B, moment=3)\n",
    "\n",
    "            moment_1_running_score += moment_1_score\n",
    "            moment_2_running_score += moment_2_score\n",
    "            moment_3_running_score += moment_3_score\n",
    "\n",
    "    return moment_1_running_score, moment_2_running_score, moment_3_running_score\n",
    "\n",
    "def LDM(batch_A, batch_B):\n",
    "    score_1, score_2, score_3 = LDM_window(batch_A, batch_B, window_size=5, stride=5)\n",
    "\n",
    "    score_1 = score_1*1\n",
    "    score_2 = score_2*1\n",
    "    score_3 = score_3*1\n",
    "\n",
    "    '''\n",
    "    print('Scores')\n",
    "    print('====================')\n",
    "    print(score_1)\n",
    "    print(score_2)\n",
    "    print(score_3)\n",
    "    '''\n",
    "\n",
    "    return score_1+score_2+score_3\n",
    "\n",
    "def custom_metric(batch_A, batch_B):\n",
    "    return 0\n",
    "    #return MSE(batch_A, batch_B)\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "## Average or a Batch Metrics: Good for GANs ##\n",
    "###############################################\n",
    "\n",
    "# Range #\n",
    "def range_metric(real, fake):\n",
    "    '''\n",
    "    Computes a simple metric which penalizes \"fake\" images in a batch for having a range different than the \"real\" images in a batch.\n",
    "    Only a single metric number is returned.\n",
    "    '''\n",
    "    range_real = torch.max(real).item()-torch.min(real).item()\n",
    "    range_fake = torch.max(fake).item()-torch.min(fake).item()\n",
    "\n",
    "    return abs(range_real-range_fake)/(range_real+.1)\n",
    "\n",
    "# Average #\n",
    "def avg_metric(real, fake):\n",
    "    '''\n",
    "    Computes a simple metric which penalizes \"fake\" images in a batch for having an average value different than the \"real\" images in a batch.\n",
    "    Only a single metric number is returned.\n",
    "    '''\n",
    "    avg_metric = abs((torch.mean(real).item()-torch.mean(fake).item())/(torch.mean(real)+.1).item())\n",
    "    return avg_metric\n",
    "\n",
    "# Pixel Variation #\n",
    "def pixel_dist_metric(real, fake):\n",
    "    '''\n",
    "    Computes a metric which penalizes \"fake\" images for having a pixel distance different than the \"real\" images.\n",
    "\n",
    "    real: real image tensor\n",
    "    fake: fake image tensor\n",
    "    '''\n",
    "    def pixel_dist(image_tensor):\n",
    "        '''\n",
    "        Function for computing the pixel distance (standard deviation from mean) for a batch of images.\n",
    "        For simplicity, it only looks at the 0th channel.\n",
    "        '''\n",
    "        array = image_tensor[:,0,:,:].detach().cpu().numpy().squeeze()\n",
    "        sd = np.std(array, axis=0)\n",
    "        avg=np.mean(sd)\n",
    "        return(avg)\n",
    "\n",
    "    pix_dist_fake = pixel_dist(fake)\n",
    "    pix_dist_real = pixel_dist(real)\n",
    "\n",
    "    return abs((pix_dist_real-pix_dist_fake)/(pix_dist_real+.1)) # The +0.1 in the denominators guarantees we don't divide by zero\n",
    "\n",
    "###################\n",
    "## Old Functions ##\n",
    "###################\n",
    "\n",
    "def LDM_OLD(real, fake, crop_size = 10, stride=10):\n",
    "    '''\n",
    "    Function to return the local distributions metric for two images.\n",
    "\n",
    "    image_A:        pytorch tensor for a single image\n",
    "    image_B:        pytorch tensor for a single image\n",
    "    '''\n",
    "    image_size = real.shape[2]\n",
    "\n",
    "    i_max = int((image_size)/stride) # Maximum number of windows occurs when the stride equals the crop_size\n",
    "    while (i_max-1)*stride + crop_size > image_size: # If stride < crop_size, we need fewer need to solve for the number of crops\n",
    "        i_max += -1\n",
    "\n",
    "    def crop_image_tensor_with_corner(A, corner=(0,0), crop_size=1):\n",
    "        '''\n",
    "        Function which returns a small, cropped version of an image.\n",
    "\n",
    "        A           a batch of images with dimensions: (num_images, channel, height, width)\n",
    "        corner      upper-left corner of window\n",
    "        crop_size   size of croppiong window\n",
    "        '''\n",
    "        x_min = corner[1]\n",
    "        x_max = corner[1]+crop_size\n",
    "        y_min = corner[0]\n",
    "        y_max = corner[0]+crop_size\n",
    "        return A[:,:, y_min:y_max , x_min:x_max ]\n",
    "\n",
    "    running_dist_score = 0\n",
    "    running_avg_score = 0\n",
    "\n",
    "    for i in range(0, i_max):\n",
    "        for j in range(0, j_max):\n",
    "            corner = (i*crop_size, j*crop_size)\n",
    "            win_real = crop_image_tensor_with_corner(real, corner, crop_size)\n",
    "            win_fake = crop_image_tensor_with_corner(fake, corner, crop_size)\n",
    "\n",
    "            #range_score = range_metric(win_real, win_fake)\n",
    "            avg_score = avg_metric(win_real, win_fake)\n",
    "            pixel_dist_score = pixel_dist_metric(win_real, win_fake)\n",
    "\n",
    "            running_dist_score += pixel_dist_score\n",
    "            running_avg_score += avg_score\n",
    "\n",
    "    combined_score = running_dist_score + running_avg_score\n",
    "\n",
    "    return combined_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iN7zClTzVvMp"
   },
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CCFRLWqJVuY2"
   },
   "outputs": [],
   "source": [
    "def get_supervisory_loss(fake_X, real_X, sup_criterion):\n",
    "    #print('Calc supervisory loss')\n",
    "    sup_loss = sup_criterion(fake_X, real_X)\n",
    "    return sup_loss\n",
    "\n",
    "def get_disc_loss(fake_X, real_X, disc_X, adv_criterion):\n",
    "    disc_fake_pred = disc_X(fake_X.detach()) # Detach generator from fake batch\n",
    "    disc_fake_loss = adv_criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "    disc_real_pred = disc_X(real_X)\n",
    "    disc_real_loss = adv_criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "    return disc_loss\n",
    "\n",
    "def get_gen_adversarial_loss(real_X, gen_XY, disc_Y, adv_criterion):\n",
    "    #print('Calc generative adversarial loss')\n",
    "    fake_Y = gen_XY(real_X)\n",
    "    disc_fake_pred = disc_Y(fake_Y)\n",
    "    adversarial_loss = adv_criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "    return adversarial_loss, fake_Y\n",
    "\n",
    "def get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion):\n",
    "    #print('Calc cycle loss')\n",
    "    cycle_X = gen_YX(fake_Y)\n",
    "    cycle_loss = cycle_criterion(cycle_X, real_X)\n",
    "    return cycle_loss, cycle_X\n",
    "\n",
    "def get_gen_loss(real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, config):\n",
    "    supervisory_criterion = config['sup_criterion']\n",
    "    cycle_criterion = config['cycle_criterion']\n",
    "    gen_adversarial_criterion = config['gen_adv_criterion']\n",
    "    lambda_adv = config['lambda_adv']\n",
    "    lambda_sup = config['lambda_sup']\n",
    "    lambda_cycle = config['lambda_cycle']\n",
    "\n",
    "    # Adversarial Loss\n",
    "    if lambda_adv != 0: # To save resources, we only run this code if lambda_adv != 0\n",
    "        adv_loss_AB, fake_B = get_gen_adversarial_loss(real_A, gen_AB, disc_B, gen_adversarial_criterion)\n",
    "        adv_loss_BA, fake_A = get_gen_adversarial_loss(real_B, gen_BA, disc_A, gen_adversarial_criterion)\n",
    "        adv_loss = adv_loss_AB+adv_loss_BA\n",
    "    else: # Even if we don't compute adversarial losses, we still need fake_A and fake_B for later code\n",
    "        fake_A = gen_BA(real_B)\n",
    "        fake_B = gen_AB(real_A)\n",
    "\n",
    "    # Supervisory Loss\n",
    "    if lambda_sup != 0: # To save resources, we only run this code if lambda_sup != 0\n",
    "        sup_loss_AB = get_supervisory_loss(fake_B, real_B, supervisory_criterion)\n",
    "        sup_loss_BA = get_supervisory_loss(fake_A, real_A, supervisory_criterion)\n",
    "        sup_loss = sup_loss_AB+sup_loss_BA\n",
    "\n",
    "    # Cycle-consistency Loss -- get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion)\n",
    "    cycle_loss_AB, cycle_B = get_cycle_consistency_loss(real_B, fake_A, gen_AB, cycle_criterion)\n",
    "    cycle_loss_BA, cycle_A = get_cycle_consistency_loss(real_A, fake_B, gen_BA, cycle_criterion)\n",
    "    cycle_loss = cycle_loss_AB+cycle_loss_BA\n",
    "\n",
    "    # Total Generator Loss\n",
    "    if lambda_sup == 0:\n",
    "        gen_loss = lambda_adv*adv_loss+lambda_cycle*cycle_loss\n",
    "        return gen_loss, adv_loss.item(), 0, cycle_loss.item(), cycle_A, cycle_B\n",
    "    elif lambda_adv == 0:\n",
    "        gen_loss = lambda_sup*sup_loss+lambda_cycle*cycle_loss\n",
    "        return gen_loss, 0, sup_loss.item(), cycle_loss.item(), cycle_A, cycle_B\n",
    "    else:\n",
    "        gen_loss = lambda_adv*adv_loss+lambda_sup*sup_loss+lambda_cycle*cycle_loss\n",
    "        return gen_loss, adv_loss.item(), sup_loss.item(), cycle_loss.item(), cycle_A, cycle_B\n",
    "\n",
    "### Functons for Assymmetric/Separate (Older) ###\n",
    "'''\n",
    "def get_gen_adv_loss(fake_X, disc_X, adv_criterion):\n",
    "    print('Calc generative adversarial loss')\n",
    "    disc_fake_pred = disc_X(fake_X)\n",
    "    adversarial_loss = adv_criterion(disc_fake_pred, torch.ones_like(disc_fake_pred)) # Called only from get_gen_loss\n",
    "    return adversarial_loss\n",
    "\n",
    "def get_sup_loss(fake_X, real_X, sup_criterion):\n",
    "    print('Calc supervisory loss')\n",
    "    sup_loss = sup_criterion(fake_X, real_X)\n",
    "    return sup_loss\n",
    "\n",
    "def get_cycle_loss(fake_I, gen_IS, low_rez_S, cycle_criterion):\n",
    "    print('Calc cycle loss')\n",
    "    cycle_S = gen_IS(fake_I)\n",
    "    cycle_loss = cycle_criterion(cycle_S, low_rez_S)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIxCEyDO3Xdy"
   },
   "source": [
    "## Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "j0avff6f3WFF"
   },
   "outputs": [],
   "source": [
    "def display_times(label_string, init_time, show_times):\n",
    "    '''\n",
    "    Function to display the time it takes to perform individual steps in the code. This can be helpful when trying to streamline things.\n",
    "\n",
    "    init_time:      initiation time when the process started\n",
    "    label_string:   string to label the displayed time\n",
    "    show_times:     show times or not\n",
    "    '''\n",
    "    current_time = time.time()\n",
    "\n",
    "    if show_times == True:\n",
    "        print(f'{label_string} (ms): {(current_time-init_time)*1000}')\n",
    "\n",
    "    return current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxBvKZ2-80VH"
   },
   "source": [
    "# Train/Test Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iLmrMc7b20J"
   },
   "source": [
    "## SUP Loss Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "W4kG5oJcby7G"
   },
   "outputs": [],
   "source": [
    "def train_Supervisory_Sym(config, offset=0, num_examples=-1, sample_division=1):\n",
    "    '''\n",
    "    Function to train or test a network with supervisory loss only. Also used for visualizing data in the dataset.\n",
    "    '''\n",
    "    print('Dataset offset:', offset)\n",
    "    print('Dataset num_examples:', num_examples)\n",
    "    print('Dataset sample_division:', sample_division)\n",
    "\n",
    "    ############################\n",
    "    ### Initialize Variables ###\n",
    "    ############################\n",
    "\n",
    "    ## Grab some values and assign to local variables ##\n",
    "    batch_size=config['batch_size']                     #config['batch_size']=tune.choice([32, 64, 128, 256, 512, 1024])\n",
    "    sup_criterion=config['sup_criterion']\n",
    "    scale=config['SI_scale'] if train_SI==True else config['IS_scale']\n",
    "    display_step = global_display_step\n",
    "\n",
    "    ## If Tuning ##\n",
    "    if run_mode=='tune':\n",
    "        tune_dataframe = pd.read_csv(tune_dataframe_path)\n",
    "\n",
    "        if tune_even_reporting == True:\n",
    "            batch_mult = 512/batch_size\n",
    "            display_step = global_display_step*batch_mult # Larger batch size --> fewer training iterations per report to RayTune\n",
    "\n",
    "    ## If Testing ##\n",
    "    if run_mode=='test':\n",
    "        config['batch_size'] = test_batch_size\n",
    "        test_dataframe = pd.DataFrame({'MSE (Network)' : [],  'MSE (FBP)': [],  'MSE (ML-EM)': [],'SSIM (Network)' : [], 'SSIM (FBP)': [], 'SSIM (ML-EM)': []})\n",
    "\n",
    "    ## If Visualizeing ##\n",
    "    if run_mode=='visualize':\n",
    "        config['batch_size'] = visualize_batch_size\n",
    "\n",
    "    ## Define running variables ##\n",
    "    mean_gen_loss = 0; mean_CNN_SSIM = 0 ; mean_CNN_MSE = 0 ; mean_CNN_CUSTOM = 0; report_num = 0\n",
    "\n",
    "    ###########################\n",
    "    ### Instantiate Classes ###\n",
    "    ###########################\n",
    "\n",
    "    # Generator #\n",
    "    if train_SI==True:\n",
    "        gen =  Generator(config=config, gen_SI=True,  input_size=sino_size, input_channels=sino_channels,  output_channels=image_channels).to(device)\n",
    "    else:\n",
    "        gen =  Generator(config=config, gen_SI=False, input_size=sino_size, input_channels=image_channels, output_channels=sino_channels ).to(device)\n",
    "    gen_opt = torch.optim.Adam(gen.parameters(), lr=config['gen_lr'], betas=(config['gen_b1'], config['gen_b2']))\n",
    "\n",
    "    # Dataloader #\n",
    "    dataloader = DataLoader(\n",
    "        NpArrayDataSet(image_path=image_path, sino_path=sino_path, config=config, image_size=image_size, image_channels=image_channels,\n",
    "                       sino_size=sino_size, sino_channels=sino_channels, augment=augment, offset=offset, num_examples=num_examples, sample_division=sample_division),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    ##############################\n",
    "    ### Set Initial Conditions ###\n",
    "    ##############################\n",
    "\n",
    "    ## If loading checkpoint ##\n",
    "    if load_state==True:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        gen.load_state_dict(checkpoint['gen_state_dict'])\n",
    "        gen_opt.load_state_dict(checkpoint['gen_opt_state_dict'])\n",
    "\n",
    "        # If testing or visualizing #\n",
    "        if run_mode=='test' or run_mode=='visualize':\n",
    "            gen.eval()\n",
    "            start_epoch=0 ; batch_step = 0\n",
    "            end_epoch=1\n",
    "        # If training, pick up where we left off #\n",
    "        else:\n",
    "            start_epoch = checkpoint['epoch'] # Note: if interrupted, this epoch may be trained more than once\n",
    "            end_epoch = start_epoch + num_epochs\n",
    "            batch_step = checkpoint['batch_step']\n",
    "    ## If starting from scratch ##\n",
    "    else:\n",
    "        gen = gen.apply(weights_init)\n",
    "        start_epoch=0 ; batch_step = 0\n",
    "        end_epoch=num_epochs\n",
    "\n",
    "    ## Initialize timestamps to keep track of calculation times ##\n",
    "    time_init_full = time.time()\n",
    "    time_init_loader = time.time()  # This is reset at the end of the inner for loop.\n",
    "\n",
    "    #############\n",
    "    ### Loops ###\n",
    "    #############\n",
    "\n",
    "    ### Loop over Epochs ###\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "\n",
    "        ### Loop Over Batches ###\n",
    "        for sino_ground, sino_scaled, image_ground, image_scaled in iter(dataloader): # Dataloader returns the batches. Loop over batches within epochs.\n",
    "\n",
    "            # Show times #\n",
    "            current_time = display_times('loader time', time_init_loader, show_times)\n",
    "            time_init_full = display_times('FULL STEP TIME', time_init_full, show_times)\n",
    "\n",
    "            # Assign inputs and targets #\n",
    "            if train_SI==True:\n",
    "                target=image_scaled\n",
    "                input=sino_scaled\n",
    "            else:\n",
    "                target=sino_scaled\n",
    "                input=image_scaled\n",
    "\n",
    "            #######################\n",
    "            ## Calculate Outputs ##\n",
    "            #######################\n",
    "\n",
    "            ## If Tuning or Training, train one step ##\n",
    "            if run_mode=='tune' or run_mode=='train':\n",
    "                time_init_train = time.time() # Initialize timestamp for training duration\n",
    "\n",
    "                gen_opt.zero_grad()\n",
    "                CNN_output = gen(input)\n",
    "\n",
    "                if run_mode=='train' and torch.sum(CNN_output[1,0,:]) < 0: # Let's you know if the network starts putting out predominantly negative values.\n",
    "                    print('PIXEL VALUES SUM TO A NEGATIVE NUMBER. IF THIS CONTINUES FOR AWHILE, YOU MAY NEED TO RESTART')\n",
    "\n",
    "                # Update gradients\n",
    "                gen_loss = sup_criterion(CNN_output, target)\n",
    "                gen_loss.backward()\n",
    "                gen_opt.step()\n",
    "                # Keep track of the average generator loss\n",
    "                mean_gen_loss += gen_loss.item() / display_step\n",
    "\n",
    "                current_time = display_times('training time', time_init_train, show_times)\n",
    "\n",
    "            ## If Testing or Vizualizing, calculate output only ##\n",
    "            else:\n",
    "                CNN_output=gen(input).detach()\n",
    "\n",
    "            ####################################\n",
    "            ### Run-Type Specific Operations ###\n",
    "            ####################################\n",
    "            time_init_metrics=time.time()\n",
    "\n",
    "            ## If Tuning or Training ##\n",
    "            # We only calculate the mean value of the metrics, but not dataframes or reconstructions. Mean values are used to calculate the optimization metrics #\n",
    "            if (run_mode == 'tune') or (run_mode=='train'):\n",
    "                mean_CNN_MSE +=  calculate_metric(target, CNN_output, MSE) / display_step      #MSE(target, CNN_output) / display_step\n",
    "                mean_CNN_SSIM += calculate_metric(target, CNN_output, SSIM)/ display_step      #SSIM_2D(target, CNN_output) / display_step\n",
    "\n",
    "                time_init_LDM=time.time()\n",
    "                mean_CNN_CUSTOM += custom_metric(target, CNN_output) / display_step\n",
    "\n",
    "                current_time = display_times('LDM time', time_init_LDM, show_times)\n",
    "\n",
    "            ## If Testing ##\n",
    "            # We reconstruct images and we calculate metric dataframes #\n",
    "            if run_mode == 'test':\n",
    "                test_dataframe, mean_CNN_MSE, mean_CNN_SSIM, mean_FBP_MSE, mean_FBP_SSIM, mean_MLEM_MSE, mean_MLEM_SSIM, FBP_output, MLEM_output = update_test_dataframe_and_return_outputs(\n",
    "                    input, image_size, CNN_output, image_scaled, test_dataframe, config)\n",
    "\n",
    "            ## If Visualizing ##\n",
    "            # We calculate reconstructions but not metric values. #\n",
    "            if run_mode=='visualize':\n",
    "                FBP_output =  reconstruct(input, config, image_size=image_size, recon_type='FBP')\n",
    "                MLEM_output = reconstruct(input, config, image_size=image_size, recon_type='MLEM')\n",
    "\n",
    "\n",
    "            # Show metric calculation time #\n",
    "            current_time = display_times('metrics time', time_init_metrics, show_times)\n",
    "\n",
    "            ######################################\n",
    "            ### VISUALIZATION / REPORTING CODE ###\n",
    "            ######################################\n",
    "\n",
    "            if batch_step % display_step == 0 and (batch_step > 0 or run_mode != 'tune'): # Leaving batch_step>0 is IMPORTANT for when you're tuning.\n",
    "                                                                                          # Otherwise your first report will report nearly nothing\n",
    "                # Visualization timestamp #\n",
    "                time_init_visualization=time.time()\n",
    "\n",
    "                ## If Tuning ##\n",
    "                if run_mode=='tune':\n",
    "\n",
    "                    report_num +=1 # The first report to RayTune has report_num = 1\n",
    "                    if int(tune_dataframe_fraction*tune_max_t) == report_num:\n",
    "                        tune_dataframe = update_tune_dataframe(tune_dataframe, gen, config, mean_CNN_MSE, mean_CNN_SSIM, mean_CNN_CUSTOM)\n",
    "                        #print(tune_dataframe)\n",
    "\n",
    "                    example_num = (batch_step+1) * batch_size\n",
    "                    session.report({'MSE':mean_CNN_MSE, 'SSIM':mean_CNN_SSIM, 'CUSTOM':mean_CNN_CUSTOM, 'example_number': example_num, 'batch_step':batch_step, 'epoch':epoch})\n",
    "\n",
    "                ## If Training ##\n",
    "                if run_mode == 'train':\n",
    "                    # Display Batch Metrics #\n",
    "                    print('================Training===================')\n",
    "                    print(f'CURRENT PROGRESS: epoch: {epoch} / batch_step: {batch_step} / image #: {batch_step*batch_size}')\n",
    "                    print(f'mean_gen_loss:', mean_gen_loss)\n",
    "                    print(f'mean_CNN_MSE :', mean_CNN_MSE)\n",
    "                    print(f'mean_CNN_SSIM:', mean_CNN_SSIM)\n",
    "                    print(f'mean-CNN_CUSTOM', mean_CNN_CUSTOM)\n",
    "                    print('===========================================')\n",
    "                    print('Last Batch MSE: ', calculate_metric(target, CNN_output, MSE))\n",
    "                    print('Last Batch SSIM: ', calculate_metric(target, CNN_output, SSIM))\n",
    "\n",
    "                    # Display Inputs & Reconstructions#\n",
    "                    show_single_unmatched_tensor(input)\n",
    "                    print('Target/Output:')\n",
    "                    show_multiple_matched_tensors(target[0:9], CNN_output[0:9])\n",
    "\n",
    "                ## If Testing ##\n",
    "                if run_mode == 'test':\n",
    "                    # Display Batch Metrics #\n",
    "                    print('==================Testing==================')\n",
    "                    print(f'mean_CNN_MSE/mean_MLEM_MSE/mean_FBP_MSE : {mean_CNN_MSE}/{mean_MLEM_MSE}/{mean_FBP_MSE}')\n",
    "                    print(f'mean_CNN_SSIM/mean_MLEM_SSIM/mean_FBP_SSIM: {mean_CNN_SSIM}/{mean_MLEM_SSIM}/{mean_FBP_SSIM}')\n",
    "                    print('===========================================')\n",
    "\n",
    "                    # Display Inputs & Reconstructions #\n",
    "                    show_single_unmatched_tensor(input)\n",
    "                    print('Target/Output/MLEM/FBP:')\n",
    "                    show_multiple_matched_tensors(target[0:visualize_batch_size], CNN_output[0:visualize_batch_size],\n",
    "                                                  MLEM_output[0:visualize_batch_size], FBP_output[0:visualize_batch_size])\n",
    "\n",
    "                ## If Visualizing ##\n",
    "                if run_mode == 'visualize':\n",
    "                    if visualize_batch_size==120:\n",
    "                        print(f'visualize_offset: {visualize_offset}, Image Number (batch_step*120): {batch_step*120}')\n",
    "                        show_single_unmatched_tensor(target, grid=True, cmap='jet', fig_size=1)\n",
    "                    else:\n",
    "                        print('Input')\n",
    "                        show_single_unmatched_tensor(input)\n",
    "                        print('Target/ML-EM/FBP/Output:')\n",
    "                        show_multiple_matched_tensors(target[0:visualize_batch_size], MLEM_output[0:visualize_batch_size],\n",
    "                                                     FBP_output[0:visualize_batch_size],CNN_output[0:visualize_batch_size])\n",
    "\n",
    "                # Save State -- occurs only during a visualization step in order to save resources #\n",
    "                if save_state:\n",
    "                    print('Saving model!')\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'batch_step': batch_step,\n",
    "                        'gen_state_dict': gen.state_dict(),\n",
    "                        'gen_opt_state_dict': gen_opt.state_dict(),\n",
    "                        }, checkpoint_path)\n",
    "\n",
    "                # Zero running stats -- occurs once per visualization step #\n",
    "                mean_gen_loss = 0 ; mean_CNN_SSIM = 0 ; mean_CNN_MSE = 0 ; mean_CNN_CUSTOM=0\n",
    "\n",
    "                # Show visualization time #\n",
    "                current_time = display_times('visualization time', time_init_visualization, show_times)\n",
    "\n",
    "            # Update variables before repeating loop -- occurs for every step #\n",
    "            batch_step += 1 #updates with every batch\n",
    "            time_init_loader = time.time()\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    ### Complete end of Train Function Tasks ###\n",
    "    ############################################\n",
    "\n",
    "    # Save Network State (Training) #\n",
    "    if save_state:\n",
    "        print('Saving model!')\n",
    "        path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "        torch.save({\n",
    "            'epoch': epoch+1, # If we are saving after an epoch is completed, we have to go to the next epoch\n",
    "            'batch_step': batch_step,\n",
    "            'gen_state_dict': gen.state_dict(),\n",
    "            'gen_opt_state_dict': gen_opt.state_dict(),\n",
    "            }, path)\n",
    "\n",
    "    # If testing, return dataframe #\n",
    "    if run_mode=='test':\n",
    "        return test_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LRDr105TDxk"
   },
   "source": [
    "## GAN / CYCLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cs3heiSl-JyD"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Note: It makes no sense to \"test\" a GAN or use SSIM since there is nothing to compare it to. Therefore, this functionality is left out here.\n",
    "Also, now that you've defined assigned the checkpoint_dir and test_dataframe_dir in the \"User Parameters cell\", you can get rid of the path constructions below.\n",
    "\n",
    "'''\n",
    "def train_test_GAN(config, checkpoint_dir=None, load_state=False, save_state=False):\n",
    "    '''\n",
    "    Note: Arguments are set to False/None to ensure that when RayTune calles train(), states are not saved/loaded\n",
    "    Note: you may want to use 'model.train()' to put model back into training mode if you put it into eval mode at some point...\n",
    "    '''\n",
    "    print('Training GAN only!!')\n",
    "\n",
    "    ## Grab from Config ##\n",
    "\n",
    "    batch_size=config['batch_size']\n",
    "    gen_adv_criterion=config['gen_adv_criterion']\n",
    "    scale=config['SI_scale'] if train_SI==True else config['IS_scale']\n",
    "\n",
    "    ## Tensorboard ##\n",
    "    writer=SummaryWriter(tensorboard_dir)\n",
    "\n",
    "    # Generators/Discriminators #\n",
    "\n",
    "    ## These are the original networks, and work great with 71x71 images ##\n",
    "    #disc = Disc_I_Orig(config=config).to(device)\n",
    "    #gen =  Gen_SI_Orig(config=config).to(device)\n",
    "\n",
    "    ## These are the modified networks, for 90x90, and also work great ##\n",
    "    #disc = Disc_I_Orig_90(config=config).to(device)\n",
    "    #gen = Gen_SI_Orig_90(config=config).to(device)\n",
    "\n",
    "    if train_SI==True:\n",
    "        ## Now let's try a flex generator and Gen_SI_Orig_90 discriminator ##\n",
    "        disc_adv_criterion=config['SI_disc_adv_criterion']\n",
    "        disc = Disc_I_90(config=config, input_channels=image_channels).to(device)\n",
    "        gen =  Gen_90(config=config, gen_SI=True, input_channels=sino_channels, output_channels=image_channels).to(device)\n",
    "        gen_opt = torch.optim.Adam(gen.parameters(), lr=config['gen_lr'], betas=(config['gen_b1'], config['gen_b2'])) #betas are optional inputs\n",
    "        disc_opt = torch.optim.Adam(disc.parameters(), lr=config['SI_disc_lr'], betas=(config['SI_disc_b1'], config['SI_disc_b2']))\n",
    "    else:\n",
    "        disc_adv_criterion=config['IS_disc_adv_criterion']\n",
    "        disc = Disc_S_90(config=config, input_channels=sino_channels).to(device)\n",
    "        gen =  Gen_90(config=config, gen_SI=False, input_channels=image_channels, output_channels=sino_channels).to(device)\n",
    "        gen_opt = torch.optim.Adam(gen.parameters(), lr=config['gen_lr'], betas=(config['gen_b1'], config['gen_b2'])) #betas are optional inputs\n",
    "        disc_opt = torch.optim.Adam(disc.parameters(), lr=config['IS_disc_lr'], betas=(config['IS_disc_b1'], config['IS_disc_b2']))\n",
    "\n",
    "    ## Load Data ##\n",
    "    dataloader = DataLoader(\n",
    "        NpArrayDataSet(image_path=image_path, sino_path=sino_path, config=config, resize_size=resize_size, image_channels=image_channels, sino_channels=sino_channels, offset=True),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    ## Load Checkpoint ##\n",
    "    if checkpoint_dir and load_state:\n",
    "        # Load dictionary\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        # Load values from dictionary\n",
    "        start_epoch = checkpoint['epoch'] #If interrupted, this epoch may be trained more than once\n",
    "        end_epoch = start_epoch + num_epochs\n",
    "        batch_step = checkpoint['batch_step']\n",
    "        gen.load_state_dict(checkpoint['gen_state_dict'])\n",
    "        gen_opt.load_state_dict(checkpoint['gen_opt_state_dict'])\n",
    "        disc.load_state_dict(checkpoint['disc_state_dict'])\n",
    "        disc_opt.load_state_dict(checkpoint['disc_opt_state_dict'])\n",
    "    else:\n",
    "        print('Starting from scratch')\n",
    "        start_epoch=0\n",
    "        end_epoch=num_epochs\n",
    "        batch_step = 0\n",
    "        gen = gen.apply(weights_init)\n",
    "        disc = disc.apply(weights_init) # Both gen & disc inherit nn.Module functionality (.apply())\n",
    "\n",
    "    ## Loop Over Epochs ##\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        pix_dist_real_array = np.array([]) # Reset every epoch\n",
    "        mean_gen_loss = 0  # Reset every display step, but I define it here so it's available later\n",
    "        mean_disc_loss = 0 # Reset every display step\n",
    "        mean_pix_metric = 0  # Reset every display step\n",
    "        time_init_full = time.time()\n",
    "\n",
    "        ## Loop Over Batches ##\n",
    "        for sino, sino_scaled, image, image_scaled in iter(dataloader): # Dataloader returns the batches. Loop over batches within epochs.\n",
    "\n",
    "            print(f'FULL step (time): {(time.time()-time_init_full)*1000}')\n",
    "            time_init_full = time.time()\n",
    "\n",
    "            if train_SI==True:\n",
    "                real=image_scaled\n",
    "                noise=sino_scaled\n",
    "            else:\n",
    "                real=sino_scaled\n",
    "                noise=image_scaled\n",
    "\n",
    "            #print(f'Real Type: {real.dtype}, Real Shape:  {real.shape}')\n",
    "            #print(f'Noise Type: {noise.dtype}, Noise Shape:  {noise.shape}')\n",
    "            #cur_batch_size = len(real)\n",
    "\n",
    "            ## UPDATE DISCRIMINATOR ##\n",
    "            disc_opt.zero_grad()                    # Zero gradients before every batch #\n",
    "            disc_real_pred = disc(real)             # Predictions on Real Images #\n",
    "\n",
    "            with torch.no_grad(): # We won't be optmizing generator here, so disabling gradients saves on resources\n",
    "                fake = gen(noise)\n",
    "            disc_fake_pred = disc(fake.detach())\n",
    "\n",
    "            a = torch.ones_like(disc_real_pred)\n",
    "\n",
    "            disc_real_loss = disc_adv_criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "            disc_fake_loss = disc_adv_criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "\n",
    "            disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "            disc_loss.backward(retain_graph=True) # retain_graph=True is set so that we can perform gradient calculations using \"backward\" twice:\n",
    "                                                  # you need to compute gradients of discriminator in order to obtain gradients of generator, later.\n",
    "                                                  # Otherwise, for performance reasons, you can't do this.\n",
    "            disc_opt.step()\n",
    "\n",
    "            # Keep track of the average discriminator loss\n",
    "            mean_disc_loss += disc_loss.item() / display_step\n",
    "\n",
    "            ## UPDATE GENERATOR ##\n",
    "            gen_opt.zero_grad()\n",
    "            # Generator adversarial loss\n",
    "            disc_fake_pred = disc(gen(noise))\n",
    "            gen_loss = gen_adv_criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "            # Update gradients\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "            # Keep track of the average generator loss\n",
    "            mean_gen_loss += gen_loss.item() / display_step #gen_loss.item() reduces tensor to scalar. It updates loss per display step\n",
    "\n",
    "            ## PIXEL DISTANCE METRIC ##\n",
    "            pix_dist_fake = pixel_dist(fake)\n",
    "            pix_dist_real = pixel_dist(real)\n",
    "            pix_dist_real_array = np.append(pix_dist_real_array, pix_dist_real)\n",
    "            pix_dist_real_avg = np.mean(pix_dist_real_array)\n",
    "            #pix_dist_real_avg = 0.00029 # determined experimentally\n",
    "            pix_metric = abs((pix_dist_real_avg-pix_dist_fake)/pix_dist_real_avg)\n",
    "            mean_pix_metric += pix_metric / display_step\n",
    "\n",
    "            ## visualization CODE ##\n",
    "            if batch_step % display_step == 0 and batch_step > 0: # runs if batch_step is a multiple of the display step\n",
    "\n",
    "                # Calculate Individual Loss Terms #\n",
    "                loss_balance=abs(mean_gen_loss-mean_disc_loss)\n",
    "                r_metric= range_metric(real, fake)\n",
    "                a_metric= avg_metric(real, fake)\n",
    "\n",
    "                # Metric Loss #\n",
    "                optim_metric=0.5*loss_balance+mean_pix_metric+a_metric #+r_metric\n",
    "\n",
    "                ## REPORT AND SAVE STATE ##\n",
    "                # Report #\n",
    "                if run_mode=='tune':\n",
    "                    tune.report(batch_step=batch_step, epoch=epoch,\n",
    "                                mean_gen_loss=mean_gen_loss, mean_disc_loss=mean_disc_loss, loss_balance=loss_balance,\n",
    "                                range_metric=r_metric, avg_metric = a_metric, mean_pix_metric=mean_pix_metric, optim_metric=optim_metric\n",
    "                                )\n",
    "                else:\n",
    "                    # Display Stats #\n",
    "                    print(f'===========================================\\nEPOCH: {epoch}, STEP: {batch_step}')\n",
    "\n",
    "                    print(f'Real Image Batch Min: {torch.min(real)} // Max: {torch.max(real)} // Mean: {torch.mean(real)} // Sum: {torch.sum(real).item()}')\n",
    "                    print(f'Fake Image Batch Min: {torch.min(fake)} // Max: {torch.max(fake)} // Mean: {torch.mean(fake)} // Sum: {torch.sum(fake).item()}')\n",
    "                    print(f'mean_gen_loss: {mean_gen_loss} // mean_disc_loss: {mean_disc_loss}')\n",
    "                    print(f'loss_balance: {loss_balance}')\n",
    "                    print(f'mean_pixel_metric: {mean_pix_metric}')\n",
    "                    print(f'range_metric: {r_metric}')\n",
    "                    print(f'avg_metric: {a_metric}')\n",
    "                    print(f'optim_metric: {optim_metric}')\n",
    "\n",
    "                    # visualize Images #\n",
    "                    print('Reals: ')\n",
    "                    show_single_unmatched_tensor(real)\n",
    "                    print('Fakes: ')\n",
    "                    show_single_unmatched_tensor(fake)\n",
    "\n",
    "                    writer.add_scalar('generator loss', mean_gen_loss, batch_step)\n",
    "                    writer.add_scalar('discriminator loss', mean_disc_loss, batch_step)\n",
    "                    writer.add_scalar('loss balance', loss_balance, batch_step)\n",
    "                    writer.add_scalar('pixel distance loss', mean_pix_metric, batch_step)\n",
    "                    #writer.add_image(\"real\", make_grid(real_image_tensor[:25], nrow=5, normalize=True)) # [:num_images]=[0:num_images]\n",
    "                    #writer.add_image(\"fake\", make_grid(fake_image_tensor[:25], nrow=5, normalize=True))\n",
    "                    writer.flush()\n",
    "\n",
    "                # Save State #\n",
    "                if checkpoint_dir and save_state:\n",
    "                    path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'batch_step': batch_step,\n",
    "                        'gen_state_dict': gen.state_dict(),\n",
    "                        'gen_opt_state_dict': gen_opt.state_dict(),\n",
    "                        'disc_state_dict': disc.state_dict(),\n",
    "                        'disc_opt_state_dict': disc_opt.state_dict(),\n",
    "                        }, path)\n",
    "\n",
    "                # Zero Stats #\n",
    "                mean_disc_loss = 0\n",
    "                mean_gen_loss = 0\n",
    "                mean_pix_metric = 0\n",
    "\n",
    "    ## And the end of the epoch loop, we do a final save of the model ##\n",
    "    if checkpoint_dir and save_state:\n",
    "        path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'batch_step': batch_step,\n",
    "            'gen_state_dict': gen.state_dict(),\n",
    "            'gen_opt_state_dict': gen_opt.state_dict(),\n",
    "            'disc_state_dict': disc.state_dict(),\n",
    "            'disc_opt_state_dict': disc_opt.state_dict(),\n",
    "            }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9dOrxzbuEV5N"
   },
   "outputs": [],
   "source": [
    "## Note: This function still needs to be updated for SSIM and testing with the test set. See 'START HERE' comment below.\n",
    "\n",
    "def train_test_CYCLE(config, checkpoint_dir=None, load_state=False, save_state=False):\n",
    "    '''\n",
    "    Note: Arguments are set to None/False to ensure that when RayTune calles train(), states are not saved/loaded. This uses up way too much hard drive space.\n",
    "    Note: you may want to use 'model.train()' to put model back into training mode if you put it into eval mode at some point...\n",
    "    '''\n",
    "\n",
    "    ## Grab Stuff from Config Dict. ##\n",
    "    batch_size = config['batch_size']\n",
    "    gen_b1 = config['gen_b1']\n",
    "    gen_b2 = config['gen_b2']\n",
    "    gen_lr = config['gen_lr']\n",
    "    scale=config['SI_scale'] if train_SI==True else config['IS_scale']\n",
    "\n",
    "    ## Tensorboard ##\n",
    "    writer=SummaryWriter(tensorboard_dir)\n",
    "\n",
    "    ## Initialize Generators/Discriminator/Summary Writer ##\n",
    "    disc_I = Disc_S_90(config=config, input_channels=image_channels).to(device)\n",
    "    disc_S = Disc_S_90(config=config, input_channels=sino_channels).to(device)\n",
    "    gen_SI = Gen_90(config=config, gen_SI=True, input_channels=sino_channels, output_channels=image_channels).to(device)\n",
    "    gen_IS = Gen_90(config=config, gen_SI=False, input_channels=image_channels, output_channels=sino_channels).to(device)\n",
    "\n",
    "    gen_both_opt = torch.optim.Adam(list(gen_SI.parameters()) + list(gen_IS.parameters()), lr=gen_lr, betas=(gen_b1, gen_b2)) # Common optimizer\n",
    "    disc_I_opt = torch.optim.Adam(disc_I.parameters(), lr=config['SI_disc_lr'], betas=(config['SI_disc_b1'], config['SI_disc_b2']))\n",
    "    disc_S_opt = torch.optim.Adam(disc_S.parameters(), lr=config['IS_disc_lr'], betas=(config['IS_disc_b1'], config['IS_disc_b2']))\n",
    "\n",
    "    ## Load Data ##\n",
    "    dataloader = DataLoader(\n",
    "        NpArrayDataSet(image_path=image_path, sino_path=sino_path, config=config, resize_size=resize_size, image_channels=image_channels, sino_channels=sino_channels),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    ## Load Checkpoint ##\n",
    "    if checkpoint_dir and load_state:\n",
    "        # Load dictionary\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoint_file))\n",
    "        # Load values from dictionary\n",
    "        start_epoch = checkpoint['epoch'] #If interrupted, this epoch may be trained more than once\n",
    "        end_epoch = start_epoch + num_epochs\n",
    "        batch_step = checkpoint['batch_step']\n",
    "        gen_SI.load_state_dict(checkpoint['gen_SI_state_dict'])\n",
    "        gen_IS.load_state_dict(checkpoint['gen_IS_state_dict'])\n",
    "        gen_both_opt.load_state_dict(checkpoint['gen_both_opt_state_dict'])\n",
    "        disc_I.load_state_dict(checkpoint['disc_I_state_dict'])\n",
    "        disc_S.load_state_dict(checkpoint['disc_S_state_dict'])\n",
    "        disc_I_opt.load_state_dict(checkpoint['disc_I_opt_state_dict'])\n",
    "        disc_S_opt.load_state_dict(checkpoint['disc_S_opt_state_dict'])\n",
    "        if run_mode=='test':\n",
    "            gen_SI.eval()\n",
    "            gen_IS.eval()\n",
    "\n",
    "    ## START HERE WITH UPDATING THIS FUNCION FOR SSIM AND TEST SET FUNCTIONALITY\n",
    "\n",
    "    else:\n",
    "        print('Starting from scratch')\n",
    "        start_epoch=0\n",
    "        end_epoch=num_epochs\n",
    "        batch_step = 0\n",
    "        gen_SI = gen_SI.apply(weights_init)\n",
    "        gen_IS = gen_IS.apply(weights_init)\n",
    "        disc_I = disc_I.apply(weights_init)\n",
    "        disc_S = disc_S.apply(weights_init)\n",
    "\n",
    "    ## Loop Over Epochs ##\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "\n",
    "        # Following variables reset every display step. The line below only establishes these variables, it does not reset them.\n",
    "        mean_disc_loss, mean_adv_loss, mean_sup_loss, mean_cycle_loss, mean_pix_metric, mean_range_metric, mean_avg_metric = 0,0,0,0,0,0,0\n",
    "\n",
    "        ## Loop Over Batches ##\n",
    "\n",
    "        time_init_full = time.time()\n",
    "        #time_init_loader = time.time()\n",
    "\n",
    "        for sino, sino_scaled, image, image_scaled in iter(dataloader): # Dataloader returns the batches. Loop over batches within epochs.\n",
    "\n",
    "            #print(f'iter dataloader (time): {(time.time()-time_init_loader)*1000}')\n",
    "            #print(f'FULL step (time): {(time.time()-time_init_full)*1000}')\n",
    "            time_init_full = time.time()\n",
    "\n",
    "            real_S = sino_scaled\n",
    "            real_I = image_scaled\n",
    "\n",
    "            ## Update Networks ##\n",
    "\n",
    "            # Update Discriminators #\n",
    "            # Image Discriminator #\n",
    "            disc_I_opt.zero_grad() # Zero out the gradient before backpropagation\n",
    "            with torch.no_grad(): # We won't be optmizing the generator here, so disabling gradients saves on resources\n",
    "                fake_I = gen_SI(real_S)\n",
    "\n",
    "            disc_I_loss = get_disc_loss(fake_I, real_I, disc_I, config['SI_disc_adv_criterion'])\n",
    "            disc_I_loss.backward(retain_graph=True) # Update gradients\n",
    "            disc_I_opt.step() # Update optimizer\n",
    "\n",
    "            # Sinogram Discriminator #\n",
    "            disc_S_opt.zero_grad() # Zero out the gradient before backpropagation\n",
    "            with torch.no_grad(): # We won't be optmizing the generator here, so disabling gradients saves on resources\n",
    "                fake_S = gen_IS(real_I)\n",
    "            disc_S_loss = get_disc_loss(fake_S, real_S, disc_S, config['IS_disc_adv_criterion'])\n",
    "            disc_S_loss.backward(retain_graph=True) # Update gradients\n",
    "            disc_S_opt.step() # Update optimizer\n",
    "\n",
    "            # Generators #\n",
    "            gen_both_opt.zero_grad()\n",
    "            gen_loss, adv_loss, sup_loss, cycle_loss, cycle_I, cycle_S = get_gen_loss(real_I, real_S, gen_IS, gen_SI, disc_I, disc_S, config)\n",
    "            gen_loss.backward() # Update gradients\n",
    "            gen_both_opt.step() # Update optimizer\n",
    "\n",
    "            #print(f'update generator (time)): {(time.time()-time_init_gen)*1000}')\n",
    "\n",
    "            ## Metrics ##\n",
    "            # Pixel Distance #\n",
    "            pix_metric_I = pixel_metric(real_I, fake_I)\n",
    "            pix_metric_S = pixel_metric(real_S, fake_S)\n",
    "            p_metric = pix_metric_I + pix_metric_S\n",
    "\n",
    "            # Range Metric #\n",
    "            range_metric_I = range_metric(real_I, fake_I)\n",
    "            range_metric_S = range_metric(real_S, fake_S)\n",
    "            r_metric = range_metric_I+range_metric_S\n",
    "\n",
    "            # Average Metric #\n",
    "            avg_metric_I = avg_metric(real_I, fake_I)\n",
    "            avg_metric_S = avg_metric(real_S, fake_S)\n",
    "            a_metric = avg_metric_I + avg_metric_S\n",
    "\n",
    "            ## Running Statistics ##\n",
    "            # Mean loss terms #\n",
    "            mean_disc_loss    += (abs(disc_I_loss.item()) + abs(disc_S_loss.item())) / display_step\n",
    "            mean_adv_loss     += abs(adv_loss) / display_step\n",
    "            mean_sup_loss     += abs(sup_loss) / display_step\n",
    "            mean_cycle_loss   += abs(cycle_loss) / display_step\n",
    "            mean_pix_metric   += p_metric / display_step\n",
    "            mean_range_metric += r_metric / display_step\n",
    "            mean_avg_metric   += a_metric / display_step\n",
    "\n",
    "            ## visualization CODE ##\n",
    "            if batch_step % display_step == 1 and batch_step > 0: # runs if batch_step is a multiple of the display step\n",
    "\n",
    "                # Optim_Metric #\n",
    "                MS_Error = MSE(real_I, fake_I)\n",
    "                loss_balance=abs(mean_adv_loss-mean_disc_loss)\n",
    "                #optim_metric = 0.5*loss_balance+mean_cycle_loss+mean_pix_metric #+mean_avg_metric #+mean_range_metric\n",
    "                optim_metric = MS_Error\n",
    "\n",
    "                # Prune #\n",
    "                #gen_SI = prune_gen(gen_SI)\n",
    "                #gen_IS = prune_gen(gen_IS)\n",
    "\n",
    "                ## Report  to Ray Tune ##\n",
    "                if run_mode=='tune':\n",
    "                    tune.report(batch_step=batch_step, epoch=epoch,\n",
    "                                mean_adv_loss=mean_adv_loss, mean_disc_loss=mean_disc_loss, loss_balance=loss_balance,\n",
    "                                mean_sup_loss=mean_sup_loss,\n",
    "                                mean_cycle_loss=mean_cycle_loss,\n",
    "                                mean_pix_metric=mean_pix_metric,\n",
    "                                mean_avg_metric=mean_avg_metric,\n",
    "                                optim_metric=optim_metric\n",
    "                                )\n",
    "                ## Display Stats & Images ##\n",
    "                else:\n",
    "                    print(f'================================================================================\\nEPOCH: {epoch}, STEP: {batch_step}, Batch Size: {batch_size}')\n",
    "\n",
    "                    lambda_adv, lambda_sup, lambda_cycle = config['lambda_adv'], config['lambda_sup'], config['lambda_cycle']\n",
    "\n",
    "                    print(f'MSE (Images):  {MS_Error}')\n",
    "                    print(f'lambda * Mean Adversarial Loss: {lambda_adv*mean_adv_loss}')\n",
    "                    print(f'lambda * Mean Supervisory Loss: {lambda_sup*mean_sup_loss}')\n",
    "                    print(f'lambda * Mean Cycle Loss      : {lambda_cycle*mean_cycle_loss}')\n",
    "                    print(f'mean_disc_loss: {mean_disc_loss} // mean_adv_loss: {mean_adv_loss} // loss_balance (M) {loss_balance}')\n",
    "                    print(f'mean_pix_metric (M): {mean_pix_metric}')\n",
    "                    print(f'range_metric (M): {mean_range_metric}')\n",
    "                    print(f'avg_metric: {mean_avg_metric}')\n",
    "                    print(f'optim_metric: {optim_metric}')\n",
    "\n",
    "                    ## visualize Images ##\n",
    "                    # Images #\n",
    "                    print('Ground Truth Images:')\n",
    "                    show_single_unmatched_tensor(real_I)\n",
    "                    print('Generated PET Images:')\n",
    "                    show_single_unmatched_tensor(fake_I)\n",
    "                    print('Cycle PET Images:')\n",
    "                    show_single_unmatched_tensor(cycle_I)\n",
    "\n",
    "                    # Sinograms #\n",
    "                    print('Grount Truth Sinograms:')\n",
    "                    show_single_unmatched_tensor(real_S) # low_rez_S = real\n",
    "                    print('Generated Sinograms:')\n",
    "                    show_single_unmatched_tensor(fake_S)\n",
    "                    print('Cycle Sinograms:')\n",
    "                    show_single_unmatched_tensor(cycle_S)\n",
    "\n",
    "                    # Less interesting #\n",
    "                    '''\n",
    "                    print('Resized Model Images:')\n",
    "                    show_single_unmatched_tensor(resized_I[0:9])\n",
    "                    print('FBP, Full-Rez Sinograms, resized (90x90):')\n",
    "                    show_single_unmatched_tensor(FBP_I[0:9])\n",
    "\n",
    "                    print('Hi-Rez Sinograms:')\n",
    "                    show_single_unmatched_tensor(high_rez_S)\n",
    "                    print('Sinogram of Ground Truth Images:')\n",
    "                    show_single_unmatched_tensor(project(ground_I))\n",
    "                    print('Sinogram of Generated PET:')\n",
    "                    show_single_unmatched_tensor(project(fake_I))\n",
    "                    print('FBP, Low-Rez Sinograms:')\n",
    "                    show_single_unmatched_tensor(reconstruct(low_rez_S))\n",
    "                    '''\n",
    "\n",
    "                    writer.add_scalar('mean adversarial loss', mean_adv_loss, batch_step)\n",
    "                    writer.add_scalar('discriminator loss', mean_disc_loss, batch_step)\n",
    "                    writer.add_scalar('loss balance', loss_balance, batch_step)\n",
    "                    writer.add_scalar('pixel distance loss', mean_pix_metric, batch_step)\n",
    "                    writer.add_scalar('cycle loss', mean_cycle_loss)\n",
    "                    writer.add_scalar('supervisory loss (ground)', mean_sup_loss)\n",
    "                    writer.flush()\n",
    "\n",
    "                # Save State #\n",
    "                if checkpoint_dir and save_state:\n",
    "                    path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'batch_step': batch_step,\n",
    "                        'gen_SI_state_dict': gen_SI.state_dict(),\n",
    "                        'gen_IS_state_dict': gen_IS.state_dict(),\n",
    "                        'gen_both_opt_state_dict': gen_both_opt.state_dict(),\n",
    "                        'disc_I_state_dict': disc_I.state_dict(),\n",
    "                        'disc_S_state_dict': disc_S.state_dict(),\n",
    "                        'disc_I_opt_state_dict': disc_I_opt.state_dict(),\n",
    "                        'disc_S_opt_state_dict': disc_S_opt.state_dict(),\n",
    "                        }, path)\n",
    "\n",
    "                # Zero Stats #\n",
    "                mean_adv_loss = 0  # Should balance with mean_disc_loss (below)\n",
    "                mean_disc_loss = 0 # Should balance with mean_adv_loss (above)\n",
    "                mean_sup_loss_model = 0\n",
    "                mean_sup_loss_ground = 0 #\n",
    "                mean_cycle_loss = 0 # Better performing models will minimize this\n",
    "                mean_pix_metric = 0 # Reasonable to minimize this for tuning purposes\n",
    "                mean_range_metric=0\n",
    "                mean_avg_metric=0\n",
    "\n",
    "            batch_step += 1 #updates with every batch\n",
    "\n",
    "\n",
    "            time_init_loader=time.time()\n",
    "#call model.eval() before test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jMA9no37mJa"
   },
   "source": [
    "## Test CNN (by chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jImHfP4V7lle"
   },
   "outputs": [],
   "source": [
    "def test_by_chunks(begin_at=0, chunk_size=5000, testset_size = 35000, sample_division=1, part_name='batch_dataframe_part_',\n",
    "         merge_dataframes=False, test_csv_file='combined_dataframe'):\n",
    "    '''\n",
    "    Splits up testing the CNN (on a test set) into smaller chunks so that computer time-outs don't result in lost work.\n",
    "\n",
    "    begin_at:           Where to begin the testing. You set this to >0 if the test terminates early and you need to pick up partway through the test set.\n",
    "    chunk_size:         How many examples to test in each chunk\n",
    "    testset_size:       Number of examples that you wish to test. This can be less than the number of examples in the dataset file, but not more.\n",
    "    sample_division:    To test every example, set to 1. To test every other example, set to 2, and so forth.\n",
    "    part_name:          Roots of dataframe parts files (containing testing results) that will be saved. These will have a number appended to them when saved.\n",
    "    merge_dataframes:   Set to True to merge the smaller parts of the dataframes into a larger dataframe once the smaller parts have finished calculating.\n",
    "                        Otherwise, you can use the MergeTests function below at a later time.\n",
    "    '''\n",
    "    label_num=int(begin_at/chunk_size) # Which numbered dataframe parts file you start at.\n",
    "\n",
    "    for index in range(begin_at, testset_size, chunk_size):\n",
    "        save_filename = part_name+str(label_num)+'.csv'\n",
    "        print('###############################################')\n",
    "        print(f'################# Working on:', save_filename)\n",
    "        print(f'################# Starting at example: ', index)\n",
    "        print('###############################################')\n",
    "\n",
    "        chunk_dataframe = train_Supervisory_Sym(config, offset=index, num_examples=chunk_size, sample_division=sample_division)\n",
    "        chunk_dataframe_path = os.path.join(test_dataframe_dir, save_filename)\n",
    "        chunk_dataframe.to_csv(chunk_dataframe_path, index=False)\n",
    "        label_num += 1\n",
    "\n",
    "    if merge_dataframes==True:\n",
    "        max_index = int(testset_size/chunk_size)-1\n",
    "        merge_test_chunks(max_index, part_name=part_name, test_csv_file=test_csv_file)\n",
    "\n",
    "def merge_test_chunks(max_index, part_name='batch_dataframe_part_', test_csv_file='combined_dataframe'):\n",
    "    '''\n",
    "    Function for merging smaller dataframes (which contain metrics for individual images) in to a single larger dataframe.\n",
    "\n",
    "    max_index:      number of largest index\n",
    "    part_name:      root of part filenames (not including the numbers appended to the end)\n",
    "    test_csv_file:       filename for the combined dataframe\n",
    "    '''\n",
    "\n",
    "    ## Build list of filenames ##\n",
    "    names = []\n",
    "    for i in range(0, max_index+1):\n",
    "        save_filename = part_name+str(i)+'.csv'\n",
    "        names.append(save_filename)\n",
    "\n",
    "    ## Concatenate ##\n",
    "    first = True\n",
    "    for name in names:\n",
    "        add_path = os.path.join(test_dataframe_dir, name)\n",
    "        print('Concatenating: ', add_path)\n",
    "        add_frame = pd.read_csv(add_path)\n",
    "\n",
    "        if first==True:\n",
    "            test_dataframe = add_frame\n",
    "            first=False\n",
    "        else:\n",
    "            test_dataframe = pd.concat([test_dataframe, add_frame], axis=0)\n",
    "\n",
    "    ## Save Result ##\n",
    "    test_dataframe.to_csv(test_dataframe_path, index=False)\n",
    "    test_dataframe.describe()\n",
    "\n",
    "#merge_test_chunks(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jw52guvr8KVm"
   },
   "source": [
    "# Tune Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pRUCef12dLWG"
   },
   "outputs": [],
   "source": [
    "def Tune(tune_max_t=40, trainable='SUP', grace_period=1):\n",
    "    '''\n",
    "    This function is called to tune the trainable function, given:\n",
    "\n",
    "    tune_max_t:     maximum number of time units (in this case, number of reports) per trial.\n",
    "    grace_period:   minimum number of raytune reports to run before aborting a trial due to poor performance\n",
    "    '''\n",
    "    ## What am I tuning for? ##\n",
    "    if tune_for=='MSE':\n",
    "        optim_metric='MSE'\n",
    "        min_max='min'\n",
    "    elif tune_for=='SSIM':\n",
    "        optim_metric='SSIM'\n",
    "        min_max='max'\n",
    "    elif tune_for=='CUSTOM':\n",
    "        optim_metric='CUSTOM'\n",
    "        min_max='min'\n",
    "\n",
    "    print('===================')\n",
    "    print('tune_max_t:', tune_max_t)\n",
    "    print('optim_metric:',optim_metric)\n",
    "    print('min_max:', min_max)\n",
    "    print('grace_period:', grace_period)\n",
    "    print('tune_minutes', tune_minutes)\n",
    "    print('===================')\n",
    "\n",
    "    ## Reporters ##\n",
    "    reporter1 = CLIReporter(\n",
    "        metric_columns=[optim_metric,'batch_step'])\n",
    "\n",
    "    reporter = JupyterNotebookReporter(\n",
    "        overwrite=True,                 # Overwrite subsequent reporter tables in output (no scrolling)\n",
    "        metric_columns=[optim_metric,'batch_step'],\n",
    "        metric=[optim_metric],          # Which metric is used to determine best trial?\n",
    "        #mode=['min'],\n",
    "        sort_by_metric=True,            # Order reporter table by metric\n",
    "    )\n",
    "\n",
    "    ## Trial Scheduler and Run Config ##\n",
    "    if tune_scheduler == 'ASHA':\n",
    "        scheduler = ASHAScheduler(\n",
    "            time_attr='training_iteration', # Time is measured in training iterations (which is actually reports to Ray Tune)\n",
    "            max_t=tune_max_t, # (default=40). Maximum time units per trial (units = time_attr). Note: Ray Tune will by default run a maximum of 100 display steps (reports) per trial\n",
    "            metric=optim_metric, # 'This is defined in the Train function\n",
    "            mode=min_max,\n",
    "            grace_period=grace_period, # Train for a minumum number of time_attr. Set in Tune() arguments.\n",
    "            #reduction_factor=2\n",
    "            )\n",
    "        run_config=air.RunConfig(       # How to perform the run\n",
    "            name=tune_exp_name,              # Ray checkpoints saved to this file, relative to local_dir\n",
    "            storage_path=local_dir,     # Local directory\n",
    "            progress_reporter=reporter,\n",
    "            failure_config=air.FailureConfig(fail_fast=False), # default = False\n",
    "            checkpoint_config=air.CheckpointConfig(\n",
    "                num_to_keep=10,         # Maximum number of checkpoints that are kept per run.\n",
    "                checkpoint_score_attribute=optim_metric,  # Determines which checkpoints are kept on disk.\n",
    "                checkpoint_score_order=min_max\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        scheduler = FIFOScheduler()\n",
    "        run_config=train.RunConfig(\n",
    "            stop={'training_iteration': tune_max_t}, # When using the FIFO scheduler, we must explicitly specify the stoppoing criterian.\n",
    "            name=tune_exp_name,              # Ray checkpoints saved to this file, relative to local_dir\n",
    "            storage_path=local_dir,     # Local directory\n",
    "            progress_reporter=reporter,\n",
    "            failure_config=air.FailureConfig(fail_fast=False), # default = False\n",
    "            checkpoint_config=air.CheckpointConfig(\n",
    "                num_to_keep=10,         # Maximum number of checkpoints that are kept per run.\n",
    "                checkpoint_score_attribute=optim_metric,  # Determines which checkpoints are kept on disk.\n",
    "                checkpoint_score_order=min_max)\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        run_config=train.RunConfig(       # How to perform the run\n",
    "            name=tune_exp_name,              # Ray checkpoints saved to this file, relative to local_dir\n",
    "            storage_path=local_dir,     # Local directory\n",
    "            progress_reporter=reporter,\n",
    "            failure_config=air.FailureConfig(fail_fast=False), # default = False\n",
    "            checkpoint_config=air.CheckpointConfig(\n",
    "                num_to_keep=10,         # Maximum number of checkpoints that are kept per run.\n",
    "                checkpoint_score_attribute=optim_metric,  # Determines which checkpoints are kept on disk.\n",
    "                checkpoint_score_order=min_max,\n",
    "                stop={\"time_total_s\": 5})\n",
    "            #    stop={\"training_iteration\": tune_max_t}) # The FIFO scheduler does not have a stopping criterian, so this stops the trial.\n",
    "            )\n",
    "        '''\n",
    "\n",
    "\n",
    "    ## HyperOpt Search Algorithm ##\n",
    "    search_alg = HyperOptSearch(metric=optim_metric, mode=min_max)  # It's also possible to pass the search space directly to the search algorithm here.\n",
    "                                                                    # But then the search space needs to be defined in terms of the specific search algorithm methods, rather than\n",
    "                                                                    # letting RayTune translate.\n",
    "    ## Which trainable do you want to use? ##\n",
    "    if trainable=='SUP':\n",
    "        trainable_with_resources = tune.with_resources(train_Supervisory_Sym, {\"CPU\":4,\"GPU\":1})\n",
    "    elif trainable=='GAN':\n",
    "        trainable_with_resources = tune.with_resources(train_test_GAN, {\"CPU\":4,\"GPU\":1})\n",
    "    elif trainable=='CYCLE':\n",
    "        trainable_with_resources = tune.with_resources(train_test_CYCLE, {\"CPU\":4,\"GPU\":1})\n",
    "\n",
    "    ## If starting from scratch ##\n",
    "    if tune_restore==False:\n",
    "\n",
    "        # Initialize a blank tuner object\n",
    "        tuner = tune.Tuner(\n",
    "                trainable_with_resources,       # The objective function w/ resources\n",
    "                param_space=config,             # What to search over\n",
    "                tune_config=tune.TuneConfig(    # How to perform the search\n",
    "                    num_samples=-1,\n",
    "                    time_budget_s=tune_minutes*60,\n",
    "                    scheduler=scheduler,\n",
    "                    search_alg=search_alg,\n",
    "                    ),\n",
    "                run_config=run_config\n",
    "                )\n",
    "\n",
    "    ## If loading from a checkpoint ##\n",
    "    else:\n",
    "        # Load the tuner\n",
    "        tuner = tune.Tuner.restore(\n",
    "            path=os.path.join(local_dir, tune_exp_name), # Path where previous run is checkpointed\n",
    "            trainable=trainable_with_resources,\n",
    "            resume_unfinished = False\n",
    "            )\n",
    "\n",
    "    result_grid: ResultGrid = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TmoNMP8UO8j"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "U2Qi-EkPUQaD"
   },
   "outputs": [],
   "source": [
    "if run_mode=='tune':\n",
    "    if train_type==\"SUP\":\n",
    "        print('Tuning w/ Supervisory Only!')\n",
    "        time.sleep(3)\n",
    "        Tune(tune_max_t=tune_max_t, trainable='SUP', grace_period=1) # for 90-90, tune_max_t=35 | 180-71, tune_max_t=25p | for LDM, tune_max_T=25\n",
    "    if train_type=='GAN':\n",
    "        print('Tuning a GAN!')\n",
    "        time.sleep(3)\n",
    "        Tune(tune_max_t=tune_max_t, trainable='GAN', grace_period=1)\n",
    "    if train_type=='CYCLESUP' or train_type=='CYCLEGAN':\n",
    "        print('Tuning a Cycle!')\n",
    "        time.sleep(3)\n",
    "        Tune(tune_max_t=tune_max_t, trainable='CYCLE', grace_period=1)\n",
    "elif (run_mode=='train') or (run_mode=='visualize'):\n",
    "    if train_type==\"SUP\":\n",
    "        train_Supervisory_Sym(config=config, offset=offset, num_examples=-1, sample_division=sample_division)\n",
    "    if train_type=='GAN':\n",
    "        train_test_GAN(config=config)\n",
    "    if train_type=='CYCLESUP' or train_type=='CYCLEGAN':\n",
    "        train_test_CYCLE(config=config)\n",
    "elif run_mode=='test':\n",
    "    test_by_chunks(begin_at=begin_at, chunk_size=chunk_size, testset_size=testset_size, sample_division=sample_division, part_name='batch_dataframe_part_',\n",
    "         merge_dataframes=merge_dataframes, test_csv_file=test_csv_file)\n",
    "\n",
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tunHHb885PKf"
   },
   "source": [
    "# Analysis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jkVa95flUdo"
   },
   "source": [
    "## Plot: Tuning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DiM_F1KL1cLd"
   },
   "outputs": [],
   "source": [
    "def PlotFrame(experiment_path, ax, x_ticks, x_label, y_ticks, y_label, xlim=None, ylim=None, logy=False, max_plot_num=-1):\n",
    "    '''\n",
    "    This function plots the dataframes for each tuning (experiment).\n",
    "\n",
    "    experiment_path:    path to the experiment file\n",
    "    ax:                 Matplotlib axis object to plot the dataframes\n",
    "    logy:               use a logarithmic scale for the y-axis?\n",
    "    ylim:               upper limit for the y-axis. Set to None to set no limit.\n",
    "    max_plot_num        maximum number of dataframes to plot. Set to -1 to plot all dataframes.\n",
    "    '''\n",
    "    restored_tuner = tune.Tuner.restore(experiment_path,\n",
    "                                        trainable = tune.with_resources(train_Supervisory_Sym, {\"CPU\":4,\"GPU\":1}))\n",
    "    result_grid = restored_tuner.get_results()\n",
    "\n",
    "    for i, result in enumerate(result_grid):\n",
    "        #print(i)\n",
    "        #label = f\"lr={result.config['lr']:.3f}, momentum={result.config['momentum']}\"\n",
    "        try: # Keeps plotting even if there is an error with one of the plots\n",
    "            result.metrics_dataframe.plot(x=x_ticks, y=y_ticks, ax=ax, label='test', legend=False, xlim=xlim, ylim=ylim,\n",
    "                                          logy=logy, fontsize=ticksize)\n",
    "        except:\n",
    "            print('Error Plotting')\n",
    "        if i==max_plot_num:\n",
    "            break\n",
    "    ax.set_ylabel(y_label, fontsize=fontsize)\n",
    "    ax.set_xlabel(x_label, fontsize=fontsize)\n",
    "\n",
    "    return result_grid\n",
    "\n",
    "#####################\n",
    "## Plot Appearance ##\n",
    "#####################\n",
    "\n",
    "## Paths ##\n",
    "#tune_exp_name='search-Full-tunedMSE-SPIE'\n",
    "#tune_exp_name='search-Full-tunedLDM_w5s2_meanWeighted'\n",
    "tune_exp_name='search-Full-tunedLDM_w5s5_evenWeighted'\n",
    "#tune_exp_name='search-Full-tunedMSE-AHSA_scheduler'\n",
    "#tune_exp_name='search-Quartile-lowSSIM-tunedSSIM-D'\n",
    "\n",
    "plot_save_name='figure-tuning'    # Save tuning plot to this filename (do not include extension)\n",
    "plot_dir= '/content/drive/MyDrive/Colab/Working/Plots/'\n",
    "\n",
    "## Defaults ##\n",
    "local_dir='/content/drive/MyDrive/Colab/Working/'\n",
    "experiment_path = f\"{local_dir}{tune_exp_name}\"\n",
    "\n",
    "## Figure ##\n",
    "'''\n",
    "titlesize=14\n",
    "fontsize=10\n",
    "ticksize=8 # font for ticks. Set to None for default\n",
    "dpi=800\n",
    "fig_size=(10,2) # Figure Size\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=fig_size, dpi=dpi)\n",
    "ax1 = axs[0] ; ax2 = axs[1]\n",
    "'''\n",
    "titlesize=13\n",
    "fontsize=12\n",
    "ticksize=10\n",
    "dpi=800\n",
    "\n",
    "figsize=(10,8)\n",
    "fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "gs = gridspec.GridSpec(ncols=100, nrows=100)\n",
    "\n",
    "# Top Row Axes #\n",
    "ax1 = fig.add_subplot(gs[0:25,   0:100])\n",
    "ax2 = fig.add_subplot(gs[38:62,   0:100])\n",
    "ax3 = fig.add_subplot(gs[75:100,  0:100])\n",
    "\n",
    "###########\n",
    "## Plots ##\n",
    "###########\n",
    "\n",
    "#result_grid = PlotFrame(experiment_path, axs[0], 'example_number', 'Example Number', 'MSE', 'MSE', ylim=ylim_MSE, logy=True)\n",
    "result_grid = PlotFrame(experiment_path, ax1, 'batch_step', 'Batch Step', 'MSE', 'MSE', ylim=(4,20), logy=True)\n",
    "ax1.set_title('(A) MSE Learning Curves', fontsize=titlesize)\n",
    "\n",
    "\n",
    "result_grid = PlotFrame(experiment_path, ax2, 'batch_step', 'Batch Step', 'SSIM', 'SSIM', ylim=(0,0.8), logy=False)\n",
    "ax2.set_title('(B) SSIM Learning Curves', fontsize=titlesize)\n",
    "\n",
    "result_grid = PlotFrame(experiment_path, ax3, 'batch_step', 'Batch Step', 'CUSTOM', 'Local Distributions Metric', ylim=(300,500))\n",
    "ax3.set_title('(A) LDM Learning Curves', fontsize=titlesize)\n",
    "\n",
    "\n",
    "#save_path = plot_dir+plot_save_name+'.svg'\n",
    "#savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "##########################\n",
    "## Pick out Best Result ##\n",
    "##########################\n",
    "\n",
    "logdir = result_grid.get_best_result(\"SSIM\", mode=\"max\")\n",
    "print('##################')\n",
    "print('## Best Result! ##')\n",
    "print('##################')\n",
    "print(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-RDucbz-NOi"
   },
   "source": [
    "## Plot: Tuning Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hW_RJkIi-XaT"
   },
   "outputs": [],
   "source": [
    "tune_dataframe_dir= '/content/drive/MyDrive/Colab/Working/Dataframes-Tune-Full'\n",
    "tune_csv_file = 'frame-tunedMSE-ASHA'\n",
    "\n",
    "tune_dataframe_path = os.path.join(tune_dataframe_dir, tune_csv_file+'.csv')\n",
    "tune_dataframe = pd.read_csv(tune_dataframe_path)\n",
    "\n",
    "## Describe Dataframes ##\n",
    "\n",
    "#plt.scatter(tune_dataframe['num_params'], tune_dataframe['mean_CNN_MSE'])\n",
    "#plt.scatter(tune_dataframe['num_params'][1:], tune_dataframe['mean_CNN_MSE'][1:])\n",
    "\n",
    "tune_dataframe.plot.scatter('num_params', 'mean_CNN_MSE', ylim=(0,5))\n",
    "tune_dataframe.plot.scatter('gen_lr', 'mean_CNN_MSE', ylim=(0,5))\n",
    "tune_dataframe.plot.scatter('batch_size', 'mean_CNN_MSE', ylim=(0,5))\n",
    "\n",
    "'''\n",
    "plt.scatter(tune_dataframe['num_params'], tune_dataframe['mean_CNN_MSE'], ylim=(0,1))\n",
    "plt.xlabel('Number of Parameters')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "tune_dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFWg1RoN4NVJ"
   },
   "source": [
    "## Load: Test Dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5mMOMILr4NAr"
   },
   "outputs": [],
   "source": [
    "# tunedMSE #\n",
    "test_dataframe_dir1= '/content/drive/MyDrive/Colab/Working/Dataframes-TestOnFull'\n",
    "#test_csv_file1 = 'combined-tunedFullMSE-trainedFull-onTrainingSet-noMLEM'   # Use this dataframe to determine thresholds for sorting training set by metrics\n",
    "#test_csv_file1 = 'combined-tunedFullMSE-trainedFull-onTestSet-wMLEM'       # Use this dataframe to determine thresholds for sorting test set by metrics\n",
    "#test_csv_file1 = 'combined-tunedFullSSIM-trainedFull-onTestSet-wMLEM'\n",
    "test_csv_file1 = 'combined-tunedHighMSE-trainedHighMSE-onTestSet-wMLEM'\n",
    "\n",
    "#test_dataframe_dir2= '/content/drive/MyDrive/Colab/Working/Dataframes-Test-Quartile-MSE'\n",
    "#test_dataframe_dir2= '/content/drive/MyDrive/Colab/Working/Dataframes-TestOnFull'\n",
    "#test_csv_file2 = 'combined-tunedFullSSIM-trainedFull-onTestSet-wMLEM'\n",
    "#test_csv_file2 = 'combined-tunedHighMSE-trainedHighMSE-onTestSet-wMLEM'\n",
    "test_csv_file2 = 'combined-tunedLowSSIM-trainedLowSSIM-onTestSet-wMLEM'\n",
    "\n",
    "# Read Dataframes from File #\n",
    "dataframe_path1 = os.path.join(test_dataframe_dir1, test_csv_file1+'.csv')\n",
    "dataframe1 = pd.read_csv(dataframe_path1)\n",
    "dataframe_path2 = os.path.join(test_dataframe_dir2, test_csv_file2+'.csv')\n",
    "dataframe2 = pd.read_csv(dataframe_path2)\n",
    "\n",
    "## Describe Dataframes ##\n",
    "\n",
    "#frame_picked = dataframe[dataframe[\"SSIM (ML-EM)\"]>dataframe[\"SSIM (FBP)\"]]\n",
    "#frame_picked = dataframe[dataframe[\"SSIM (Network)\"]>dataframe[\"SSIM (ML-EM)\"]]\n",
    "\n",
    "#frame_picked = dataframe[dataframe[\"MSE (Network)\"]<dataframe[\"MSE (ML-EM)\"]]\n",
    "#frame_picked = dataframe[dataframe[\"MSE (ML-EM)\"]<dataframe[\"MSE (FBP)\"]]\n",
    "\n",
    "#frame_picked = dataframe1[dataframe1[\"MSE (FBP)\"]>0.95908]\n",
    "#frame_picked = dataframe1[dataframe1[\"MSE (FBP)\"]<0.330922]\n",
    "frame_picked = dataframe1[dataframe1[\"SSIM (FBP)\"]<0.837850]\n",
    "\n",
    "#dataframe1.describe()\n",
    "dataframe2.describe()\n",
    "#frame_picked.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAmVEnJChNGK"
   },
   "source": [
    "### Plot: Test Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mJbGBeJbKYt7"
   },
   "outputs": [],
   "source": [
    "# Define Plotting Functions #\n",
    "def plot_hist_1D(ax, dataframe, title, x_label, y_label, column_1, column_2, xlim, ylim, bins=400, alpha=0.5):\n",
    "    '''\n",
    "    Plots a histogram of a columns in a dataframe.\n",
    "    '''\n",
    "    dataframe = dataframe[dataframe[column_1]>xlim[0]]\n",
    "    dataframe = dataframe[dataframe[column_1]<xlim[1]]\n",
    "    dataframe = dataframe[dataframe[column_2]>xlim[0]]\n",
    "    dataframe = dataframe[dataframe[column_2]<xlim[1]]\n",
    "\n",
    "    dataframe[[column_1, column_2]].plot.hist(xlim=xlim, ylim=ylim, bins=bins, alpha=alpha, ax=ax, fontsize=fontsize)\n",
    "    ax.set_title(title, fontsize=titlesize)  # Add a title to the axis.\n",
    "    ax.set_xlabel(x_label, fontsize=fontsize)  # Add an x-label to the axis.\n",
    "    ax.set_ylabel(y_label, fontsize=fontsize)  # Add a y-label to the axis.\n",
    "\n",
    "def plot_hist_2D(ax, dataframe, title, x_label, y_label, x_column, y_column, xlim=(0,1), ylim=(0,1), gridsize=None):\n",
    "    '''\n",
    "    Plots hexagonal bin plot of a two columns in a dataframe.\n",
    "\n",
    "    dataframe   the dataframe from which to grab the data\n",
    "    x_label     label of data to plot on the x-axis. This must match a column label in the dataframe.\n",
    "    y_label     label of data to plot on the y-axis. This must match a column label in the dataframe.\n",
    "    gridsize    how large to make the grid on the gridplot\n",
    "    '''\n",
    "    dataframe = dataframe[dataframe[x_column]>xlim[0]]\n",
    "    dataframe = dataframe[dataframe[x_column]<xlim[1]]\n",
    "    dataframe = dataframe[dataframe[y_column]>ylim[0]]\n",
    "    dataframe = dataframe[dataframe[y_column]<ylim[1]]\n",
    "    dataframe.plot.hexbin(ax=ax, x=x_column, y=y_column, xlim=xlim, ylim=ylim, gridsize=gridsize, fontsize=ticksize)\n",
    "\n",
    "    ax.set_title(title, fontsize=titlesize)  # Add a title to the axis.\n",
    "    ax.set_xlabel(x_label, fontsize=fontsize)  # Add an x-label to the axis.\n",
    "    ax.set_ylabel(y_label, fontsize=fontsize)  # Add a y-label to the axis.\n",
    "    ax.plot(xlim, ylim, linestyle='--') # plot dividing line\n",
    "\n",
    "## Specify Plotting Parameters ##\n",
    "plot_type = 2 # 1 = histograms, 2 = bin plots, 3 = both\n",
    "\n",
    "column_MSE_1 = 'MSE (ML-EM)'\n",
    "#column_MSE_1 = 'MSE (FBP)'\n",
    "column_MSE_2 = 'MSE (Network)'\n",
    "#column_MSE_2 = 'MSE (FBP)'\n",
    "\n",
    "column_SSIM_1 = 'SSIM (ML-EM)'\n",
    "#column_SSIM_1 = 'SSIM (FBP)'\n",
    "column_SSIM_2 = 'SSIM (Network)'\n",
    "#column_SSIM_2 = 'SSIM (FBP)'\n",
    "\n",
    "\n",
    "titlesize=12\n",
    "fontsize=9\n",
    "ticksize=7\n",
    "dpi=800\n",
    "\n",
    "if plot_type == 1 or plot_type == 2:\n",
    "    figsize=(8,6) # 17,5\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    gs = gridspec.GridSpec(ncols=100, nrows=100)\n",
    "\n",
    "    # Top Row Axes #\n",
    "    ax1 = fig.add_subplot(gs[0:42,   0:43])\n",
    "    ax2 = fig.add_subplot(gs[0:42,   57:100])\n",
    "\n",
    "    # Bottom Row Axes #\n",
    "    ax3 = fig.add_subplot(gs[58:100, 0:43])\n",
    "    ax4 = fig.add_subplot(gs[58:100, 57:100])\n",
    "\n",
    "    if plot_type == 1:\n",
    "        plot_hist_1D(ax1, dataframe1, '(1) CNN-A: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2, xlim=(0,4), ylim=(0,5000), bins=40)\n",
    "        plot_hist_1D(ax2, dataframe1, '(2) CNN-A: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n",
    "        plot_hist_1D(ax3, dataframe2, '(3) CNN-B: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2,  xlim=(0,4), ylim=(0,5000),  bins=40)\n",
    "        plot_hist_1D(ax4, dataframe2, '(4) CNN-B: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n",
    "    if plot_type == 2:\n",
    "        plot_hist_2D(ax1, dataframe1, '(1) CNN-A: MSE Bin Plot', column_MSE_1, 'MSE (CNN-A)', column_MSE_1 , column_MSE_2,(0,1.5), (0,1.5), gridsize=60)\n",
    "        plot_hist_2D(ax2, dataframe1, '(2) CNN-A: SSIM Bin Plot',column_SSIM_1, 'SSIM (CNN-A)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n",
    "        plot_hist_2D(ax3, dataframe2, '(3) CNN-B: MSE Bin Plot', column_MSE_1, 'MSE (CNN-B)', column_MSE_1 , column_MSE_2, (0,1.5), (0,1.5), gridsize=60)\n",
    "        plot_hist_2D(ax4, dataframe2, '(4) CNN-B: SSIM Bin Plot', column_SSIM_1, 'SSIM (CNN-B)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n",
    "\n",
    "if plot_type == 3:\n",
    "    figsize=(15,6) # 17,5\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    gs = gridspec.GridSpec(ncols=100, nrows=100)\n",
    "\n",
    "    # Top Row Axes #\n",
    "    ax1 = fig.add_subplot(gs[0:42,   0:18]) # 20\n",
    "    ax2 = fig.add_subplot(gs[0:42,   25:47]) # 22\n",
    "    ax3 = fig.add_subplot(gs[0:42,   53:74]) # 20\n",
    "    ax4 = fig.add_subplot(gs[0:42,   80:100]) # 22\n",
    "\n",
    "    # Bottom Row Axes #\n",
    "    ax5 = fig.add_subplot(gs[58:100, 0:18]) # -5-\n",
    "    ax6 = fig.add_subplot(gs[58:100, 25:47]) # -3 - -3-\n",
    "    ax7 = fig.add_subplot(gs[58:100, 53:74]) # -5-\n",
    "    ax8 = fig.add_subplot(gs[58:100, 80:100])\n",
    "\n",
    "    plot_hist_1D(ax1, dataframe1, '(1) CNN-A: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2, xlim=(0,4), ylim=(0,5000), bins=40)\n",
    "    plot_hist_1D(ax2, dataframe1, '(3) CNN-A: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n",
    "    plot_hist_2D(ax3, dataframe1, '(5) CNN-A: MSE Bin Plot', column_MSE_1, 'MSE (CNN-A)', column_MSE_1 , column_MSE_2,(0,1.5), (0,1.5), gridsize=60)\n",
    "    plot_hist_2D(ax4, dataframe1, '(7) CNN-A: SSIM Bin Plot',column_SSIM_1, 'SSIM (CNN-A)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n",
    "\n",
    "    plot_hist_1D(ax5, dataframe2, '(2) CNN-B: MSE Histogram',  'MSE', 'frequency', column_MSE_1 , column_MSE_2,  xlim=(0,4), ylim=(0,5000),  bins=40)\n",
    "    plot_hist_1D(ax6, dataframe2, '(4) CNN-B: SSIM Histogram', 'SSIM','frequency', column_SSIM_1, column_SSIM_2, xlim=(0.6,1), ylim=(0,4000), bins=40)\n",
    "    plot_hist_2D(ax7, dataframe2, '(6) CNN-B: MSE Bin Plot', column_MSE_1, 'MSE (CNN-B)', column_MSE_1 , column_MSE_2, (0,1.5), (0,1.5), gridsize=60)\n",
    "    plot_hist_2D(ax8, dataframe2, '(8) CNN-B: SSIM Bin Plot', column_SSIM_1, 'SSIM (CNN-B)', column_SSIM_1, column_SSIM_2, (.7,1), (.7,1), gridsize=100)\n",
    "\n",
    "save_path = plot_dir+'figure-histograms.png'\n",
    "savefig(save_path, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5qkU0fpHsKa"
   },
   "source": [
    "## Plot: Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "J9PCpxBnHo07"
   },
   "outputs": [],
   "source": [
    "### User Parameters ###\n",
    "#######################\n",
    "\n",
    "## Indexes of Example Images ##\n",
    "#-----------------------------#\n",
    "# Panel 1: Network performs much better for all images\n",
    "#indexes = [2280+4, 13*120, 187*120+13, 151 * 120, 240+37, 147*120, 187*120+17, 239*120]\n",
    "\n",
    "# Panel 2: Network performs somewhat better:\n",
    "#indexes = [1073, 840+48, 108 * 120,  147*120+33, 153*120, 1440+71, 13560+17, 153*120, 224*120+161]\n",
    "\n",
    "# Panel 3: ML-EM performs better\n",
    "#indexes = [960+97, 1268*120, 111*120]\n",
    "\n",
    "#Panel 5: Panel 1 + Panel 2 --> image 0-8 (network much better)\n",
    "#indexes = [2280+4, 13*120, 151 * 120, 240+37, 147*120, 187*120+17, 239*120, 1073]\n",
    "#indexes = [840+48+1, 108 * 120,  147*120+33, 153*120+2, 1440+71, 13560+17, 224*120+161+5]\n",
    "\n",
    "# Panel 6: Used in SPIE Paper\n",
    "#indexes = [2280+4, 240+37, 187*120+17, 239*120, 1073, 840+48+1, 153*120+2,  13*120, 224*120+161+5]\n",
    "#indexes = [240+37, 187*120+17, 239*120, 1073, 840+48+1, 224*120+161+5, 153*120+2, 13*120] # Final Cut\n",
    "\n",
    "# Final Panel (nine images max)\n",
    "\n",
    "## Paths ##\n",
    "indexes = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]\n",
    "\n",
    "image_path = '/content/drive/MyDrive/Repository/PET_Data/test_image-35k.npy'\n",
    "sino_path =   '/content/drive/MyDrive/Repository/PET_Data/test_sino-35k.npy'\n",
    "\n",
    "\n",
    "checkpoint_dir = checkpoint_dir='/content/drive/MyDrive/Colab/Working/Checkpoints-trainOnFull'\n",
    "\n",
    "checkpoint_file_SSIM= 'checkpoint-tunedSSIM-14d-6epochs'\n",
    "checkpoint_file_MSE = 'checkpoint-tunedMSE-fc6-6epochs'\n",
    "checkpoint_file_MAE = 'checkpoint-tunedMAE-b08-6epochs'\n",
    "checkpoint_file_LDM = 'checkpoint-tunedLDM-w10s8-b5c-6epochs'\n",
    "checkpoint_file_LDM_batch='checkpoint-tunedLDM_batch-f9f-6epochs'\n",
    "\n",
    "## Dimensions ##\n",
    "image_size=90\n",
    "sino_size=90\n",
    "image_channels=1\n",
    "sino_channels=1\n",
    "\n",
    "## CNNs ##\n",
    "config_MSE= { # 1x90x90, Tuned for MSE - fc6 #\n",
    "    \"SI_dropout\": False, \"SI_exp_kernel\": 4, \"SI_gen_fill\": 0, \"SI_gen_final_activ\": None, \"SI_gen_hidden_dim\": 14, \"SI_gen_mult\": 2.3737518721494038,\n",
    "    \"SI_gen_neck\": 5, \"SI_gen_z_dim\": 300, \"SI_layer_norm\": \"instance\", \"SI_normalize\": True, \"SI_pad_mode\": \"zeros\", \"SI_scale\": 8100,\n",
    "    \"batch_size\": 266, \"gen_b1\": 0.5194977285709309, \"gen_b2\": 0.4955647195661826, \"gen_lr\": 0.0006569034263698925, \"sup_criterion\": nn.MSELoss() }\n",
    "\n",
    "config_SSIM = { # 1x90x90, Tuned for SSIM - 14d #\n",
    "    \"SI_dropout\": False, \"SI_exp_kernel\": 4, \"SI_gen_fill\": 0, \"SI_gen_final_activ\": nn.Tanh(), \"SI_gen_hidden_dim\": 23, \"SI_gen_mult\": 1.6605902406330195,\n",
    "    \"SI_gen_neck\": 5, \"SI_gen_z_dim\": 789, \"SI_layer_norm\": \"instance\", \"SI_normalize\": True, \"SI_pad_mode\": \"zeros\", \"SI_scale\": 8100, \"batch_size\": 71, \"gen_b1\": 0.2082092731474774,\n",
    "    \"gen_b2\": 0.27147903136187507, \"gen_lr\": 0.0005481469822215635, \"sup_criterion\": nn.MSELoss()}\n",
    "\n",
    "config_MAE = { # 1x90x90, Tuned for MAE, - b08 #\n",
    "    \"SI_dropout\": True, \"SI_exp_kernel\": 3, \"SI_gen_fill\": 0, \"SI_gen_final_activ\": nn.Tanh(), \"SI_gen_hidden_dim\": 29, \"SI_gen_mult\": 3.4493572412953926,\n",
    "    \"SI_gen_neck\": 5, \"SI_gen_z_dim\": 92, \"SI_layer_norm\": \"instance\", \"SI_normalize\": True, \"SI_pad_mode\": \"zeros\", \"SI_scale\": 8100, \"batch_size\": 184,\n",
    "    \"gen_b1\": 0.41793988944151467, \"gen_b2\": 0.15133808988276928, \"gen_lr\": 0.0012653525173041019, \"sup_criterion\": nn.L1Loss() }\n",
    "\n",
    "config_LDM={ # 1x90x90, Tuned for CUSTOM = LDM (image statistics)\n",
    "    \"SI_dropout\": False, \"SI_exp_kernel\": 4, \"SI_gen_fill\": 0, \"SI_gen_final_activ\": None, \"SI_gen_hidden_dim\": 9, \"SI_gen_mult\": 2.1547197646081444,\n",
    "    \"SI_gen_neck\": 5, \"SI_gen_z_dim\": 344, \"SI_layer_norm\": \"batch\", \"SI_normalize\": True, \"SI_pad_mode\": \"zeros\", \"SI_scale\": 8100, \"batch_size\": 47,\n",
    "    \"gen_b1\": 0.31108788447029295, \"gen_b2\": 0.3445239707919786, \"gen_lr\": 0.0007561178182660596, \"sup_criterion\": nn.L1Loss()}\n",
    "\n",
    "config_LDM_batch={ # 1x90x90, Tuned for CUSTOM = LDM (batch statistics)\n",
    "    \"SI_dropout\": False, \"SI_exp_kernel\": 3, \"SI_gen_fill\": 0, \"SI_gen_final_activ\": nn.Tanh(), \"SI_gen_hidden_dim\": 19, \"SI_gen_mult\": 2.70340867805694,\n",
    "    \"SI_gen_neck\": 1, \"SI_gen_z_dim\": 1616, \"SI_layer_norm\": \"instance\", \"SI_normalize\": True, \"SI_pad_mode\": \"zeros\", \"SI_scale\": 8100, \"batch_size\": 363,\n",
    "    \"gen_b1\": 0.20393974474424928, \"gen_b2\": 0.6490512100839003, \"gen_lr\": 0.0004491464075393307, \"sup_criterion\": nn.MSELoss()}\n",
    "\n",
    "## Defaults ##\n",
    "train_type='SUP'\n",
    "train_SI=True\n",
    "image_array = np.load(image_path, mmap_mode='r')       # self.image_tensor.shape=(#examples x1x71x71)\n",
    "sino_array = np.load(sino_path, mmap_mode='r')     # self.sinogram_tensor.shape=(#examples x3x101x180)\n",
    "\n",
    "## Build Image & Sino Tensors ##\n",
    "def BuildImageSinoTensors(image_array, sino_array, config, indexes):\n",
    "    '''\n",
    "    Build image and sinogram tensors with images and sinograms determined by the indexes list.\n",
    "    '''\n",
    "    first=True\n",
    "    i=0\n",
    "    for idx in indexes:\n",
    "        sino_ground, sino_scaled, image_ground, image_scaled = NpArrayDataLoader(image_array, sino_array, config,\n",
    "                                                                                image_size = image_size, sino_size=sino_size,\n",
    "                                                                                image_channels=image_channels,\n",
    "                                                                                sino_channels=sino_channels,\n",
    "                                                                                augment=False, index=idx)\n",
    "        # If first time through the loop, create blank tensors (for sino & image) with the correct shape\n",
    "        if first==True:\n",
    "            image_tensor = torch.zeros(len(indexes), image_scaled.shape[0], image_scaled.shape[1], image_scaled.shape[2]).to(device)\n",
    "            sino_tensor  = torch.zeros(len(indexes), sino_scaled.shape[0],  sino_scaled.shape[1],  sino_scaled.shape[2]).to(device)\n",
    "            first=False\n",
    "        # Fill the tensors with images & sinograms\n",
    "        image_tensor[i,:] = image_scaled\n",
    "        sino_tensor[i,:]  = sino_scaled\n",
    "        i+=1\n",
    "    return image_tensor, sino_tensor\n",
    "\n",
    "## CNN Outputs ##\n",
    "def CNN_reconstruct(sino_tensor, config, checkpoint_file_name, input_size=90, input_channels=1, output_channels=1):\n",
    "    '''\n",
    "    Construct CNN reconstructions of images of a sinogram tensor.\n",
    "    '''\n",
    "    gen =  Generator(config=config, gen_SI=True, input_size=input_size, input_channels=input_channels, output_channels=output_channels).to(device)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file_name)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    gen.load_state_dict(checkpoint['gen_state_dict'])\n",
    "    gen.eval()\n",
    "    return gen(sino_tensor).detach()\n",
    "\n",
    "## Outputs ###\n",
    "image_tensor, sino_tensor = BuildImageSinoTensors(image_array, sino_array, config_MSE, indexes)\n",
    "\n",
    "MLEM_output = reconstruct(sino_tensor, config_MSE, image_size=image_size, recon_type='MLEM', circle=True)\n",
    "CNN_output_MSE = CNN_reconstruct(sino_tensor, config_MSE, checkpoint_file_MSE)\n",
    "CNN_output_SSIM = CNN_reconstruct(sino_tensor, config_SSIM, checkpoint_file_SSIM)\n",
    "CNN_output_MAE = CNN_reconstruct(sino_tensor, config_MAE, checkpoint_file_MAE)\n",
    "CNN_output_LDM = CNN_reconstruct(sino_tensor, config_LDM, checkpoint_file_LDM)\n",
    "CNN_output_LDM_batch = CNN_reconstruct(sino_tensor, config_LDM_batch, checkpoint_file_LDM_batch)\n",
    "\n",
    "#MLEM_output2 = reconstruct(sino_tensor, config1, image_size=image_size, recon_type='MLEM', circle=False)\n",
    "#FBP_output =  reconstruct(sino_tensor, config1, image_size=image_size, recon_type='FBP')\n",
    "\n",
    "#############\n",
    "## Metrics ##\n",
    "#############\n",
    "\n",
    "'''\n",
    "frame_SSIM_MLEM, placeholder = calculate_metric(MLEM_output, image_tensor, SSIM, dataframe = True, label='MLEM, SSIM')\n",
    "frame_MSE_MLEM, placeholder =  calculate_metric(MLEM_output, image_tensor, MSE, dataframe = True, label='MLEM, MSE')\n",
    "print('################### MLEM ###################')\n",
    "print(frame_SSIM_MLEM.T)\n",
    "print(frame_MSE_MLEM.T)\n",
    "\n",
    "\n",
    "frame_SSIM_tunedMSE, placeholder = calculate_metric(CNN_output_MSE, image_tensor, SSIM, dataframe = True, label='TunedMSE, SSIM')\n",
    "frame_MSE_tunedMSE, placeholder =  calculate_metric(CNN_output_MSE, image_tensor, MSE, dataframe = True, label='TunedMSE, MSE')\n",
    "print('################### CNN (MSE) ##################')\n",
    "print(frame_SSIM_tunedMSE.T)\n",
    "print(frame_MSE_tunedMSE.T)\n",
    "\n",
    "frame_SSIM_tunedSSIM, placeholder = calculate_metric(CNN_output_SSIM, image_tensor, SSIM, dataframe = True, label='TunedSSIM, SSIM')\n",
    "frame_MSE_tunedSSIM, placeholder =  calculate_metric(CNN_output_SSIM, image_tensor, MSE, dataframe = True, label='TunedSSIM, MSE')\n",
    "frame_LDM_tunedSSIM, placeholder =  calculate_metric(CNN_output_SSIM, image_tensor, custom_metric, dataframe = True, label='TunedSSIM, LDM')\n",
    "print('################### CNN (SSIM) ##################')\n",
    "print(frame_SSIM_tunedSSIM.T)\n",
    "print(frame_MSE_tunedSSIM.T)\n",
    "print(frame_LDM_tunedSSIM.T)\n",
    "\n",
    "frame_SSIM_tunedLDM, placeholder = calculate_metric(CNN_output_LDM, image_tensor, SSIM, dataframe = True, label='TunedLDM, SSIM')\n",
    "frame_MSE_tunedLDM, placeholder =  calculate_metric(CNN_output_LDM, image_tensor, MSE, dataframe = True, label='TunedLDM, MSE')\n",
    "frame_LDM_tunedLDM, placeholder =  calculate_metric(CNN_output_LDM, image_tensor, custom_metric, dataframe = True, label='TunedLDM, LDM')\n",
    "print('################### CNN (LDM) ##################')\n",
    "print(frame_SSIM_tunedLDM.T)\n",
    "print(frame_MSE_tunedLDM.T)\n",
    "print(frame_LDM_tunedLDM.T)\n",
    "'''\n",
    "\n",
    "####################\n",
    "## Display Images ##\n",
    "####################\n",
    "\n",
    "\n",
    "#show_multiple_matched_tensors(image_tensor, CNN_output_MSE, CNN_output_LDM, CNN_output_MAE, MLEM_output, CNN_output_SSIM, fig_size=1.0)\n",
    "#show_multiple_matched_tensors(image_tensor, MLEM_output, CNN_output_MSE, CNN_output_MAE, CNN_output_SSIM, CNN_output_LDM_batch, CNN_output_LDM, fig_size=1.0)\n",
    "#show_multiple_matched_tensors(image_tensor, fig_size=1.0)\n",
    "\n",
    "#print('Ground Truth/MLEM/CNN1/CNN2')\n",
    "show_single_unmatched_tensor(sino_tensor)\n",
    "show_multiple_matched_tensors(image_tensor, MLEM_output, CNN_output_MSE, CNN_output_MAE, fig_size=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-tDENcFyMMC"
   },
   "source": [
    "# Sort: Dataset by Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "k2q2L7hFyQ1d"
   },
   "outputs": [],
   "source": [
    "def sort_DataSet(config, load_image_path, load_sino_path, save_image_path, save_sino_path, max_save_index, metric_function, threshold, threshold_min_max, num_examples=-1, visualize=False):\n",
    "    '''\n",
    "    '''\n",
    "    # Variables #\n",
    "    scale=config['SI_scale'] if train_SI==True else config['IS_scale']\n",
    "\n",
    "    # Dataloader #\n",
    "    dataloader = DataLoader(\n",
    "        NpArrayDataSet(image_path=load_image_path, sino_path=load_sino_path, config=config, image_size=image_size, image_channels=image_channels,\n",
    "                       sino_size=sino_size, sino_channels=sino_channels, num_examples=num_examples),\n",
    "        batch_size=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    ### Loop Over Batches ###\n",
    "    first = True\n",
    "    saved_idx = 0\n",
    "    for sino_ground, sino_scaled, image_ground, image_scaled in iter(dataloader): # Dataloader returns the batches. Loop over batches within epochs.\n",
    "\n",
    "        # Open memory map if first time through the loop #\n",
    "        if first==True:\n",
    "            save_image_array_shape = (max_save_index, image_scaled.shape[1], image_scaled.shape[2], image_scaled.shape[3])\n",
    "            save_sino_array_shape = (max_save_index, sino_scaled.shape[1], sino_scaled.shape[2], sino_scaled.shape[3])\n",
    "            print('save_image_array_shape: ', save_image_array_shape)\n",
    "            print('save_sino_array_shape: ', save_sino_array_shape)\n",
    "\n",
    "            save_image_array = np.lib.format.open_memmap(save_image_path, mode='w+', shape=save_image_array_shape, dtype=np.float32)\n",
    "            save_sino_array =  np.lib.format.open_memmap(save_sino_path , mode='w+', shape=save_sino_array_shape,  dtype=np.float32)\n",
    "            first=False\n",
    "\n",
    "        # Test the image to see if fits the criteria #\n",
    "        FBP_output =  reconstruct(sino_scaled, config, image_size=image_size, recon_type='FBP')\n",
    "        image_metric = metric_function(image_scaled, FBP_output)\n",
    "\n",
    "        if threshold_min_max == 'min':\n",
    "            keep = True if (image_metric > threshold) else False\n",
    "        else:\n",
    "            keep = True if (image_metric < threshold) else False\n",
    "\n",
    "        if keep==True:\n",
    "            save_sino_array[saved_idx] = sino_scaled.cpu().numpy()\n",
    "            save_image_array[saved_idx] = image_scaled.cpu().numpy()\n",
    "            saved_idx += 1\n",
    "            print('Current index (for next image): ', saved_idx)\n",
    "\n",
    "        if visualize==True:\n",
    "            # Visualize the rejected or accepted sample #\n",
    "            print('==================================')\n",
    "            print('Image Metric: ', image_metric)\n",
    "            print('Threshold: ', threshold)\n",
    "            print('Keep?: ', keep)\n",
    "            print('Current index (for next image): ', saved_idx)\n",
    "            print('Saved Arrays:')\n",
    "            print('image_scaled / FBP_output / sino_scaled')\n",
    "            show_multiple_matched_tensors(image_scaled, FBP_output)\n",
    "            show_multiple_matched_tensors(sino_scaled)\n",
    "            show_multiple_matched_tensors(torch.from_numpy(save_sino_array[0:9]))\n",
    "            show_multiple_matched_tensors(torch.from_numpy(save_image_array[0:9]))\n",
    "\n",
    "    return save_sino_array, save_image_array\n",
    "\n",
    "## Changeable Variables ##\n",
    "\n",
    "load_sino_path = '/content/drive/MyDrive/Repository/PET_Data/train_sino-70k.npy'\n",
    "load_image_path = '/content/drive/MyDrive/Repository/PET_Data/train_image-70k.npy'\n",
    "save_sino_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_sino-lowSSIM-17500.npy'\n",
    "save_image_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_image-lowSSIM-17500.npy'\n",
    "'''\n",
    "metric_function = MSE\n",
    "max_save_index = 17500\n",
    "threshold = 0.330922\n",
    "threshold_min_max = 'min'\n",
    "'''\n",
    "metric_function = SSIM\n",
    "max_save_index = 17500\n",
    "threshold = 0.837850 #0.837850  # MSE (min): 0.330922, SSIM (max): 0.837850\n",
    "threshold_min_max = 'max'\n",
    "\n",
    "## Run & Verify Result ##\n",
    "save_sino_array, save_image_array = sort_DataSet(config, load_image_path, load_sino_path, save_image_path, save_sino_path, max_save_index,\n",
    "                                                   metric_function, threshold, threshold_min_max=threshold_min_max, visualize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwU5ioxR6bHg"
   },
   "source": [
    "### Save Datasets & Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQCMCuDx6bEp"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PTrqg3MHNl1x"
   },
   "outputs": [],
   "source": [
    "#save_sino_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_sino-lowSSIM-17500.npy'\n",
    "#save_image_path = '/content/drive/MyDrive/Repository/PET_Data/quartile_data/train_sino-lowSSIM-17500.npy'\n",
    "\n",
    "# Print sorted array shape & display a few images #\n",
    "print('save_sino_array.shape: ', save_sino_array.shape)\n",
    "print('save_image_array.shape: ', save_image_array.shape)\n",
    "\n",
    "print('save_sino_array sample images')\n",
    "print('save_image_array sample images')\n",
    "show_multiple_matched_tensors(torch.from_numpy(save_sino_array[500:509]))\n",
    "show_multiple_matched_tensors(torch.from_numpy(save_image_array[500:509]))\n",
    "\n",
    "\n",
    "# Save the sorted array to disk #\n",
    "save_sino_array.flush()\n",
    "save_image_array.flush()\n",
    "#np.save(save_sino_path, save_sino_array)\n",
    "#np.save(save_image_path, save_image_array)\n",
    "\n",
    "# Load the saved array and make sure it's the same size/has the same images #\n",
    "load_sino_array = np.load(save_sino_path, mmap_mode='r')\n",
    "load_image_array = np.load(save_image_path, mmap_mode='r')\n",
    "print('load_sino_array.shape: ', load_sino_array.shape)\n",
    "print('load_image_array.shape: ', load_image_array.shape)\n",
    "\n",
    "print('load_sino_array sample images')\n",
    "print('load_image_array sample images')\n",
    "show_multiple_matched_tensors(torch.from_numpy(load_sino_array[500:509]))\n",
    "show_multiple_matched_tensors(torch.from_numpy(load_image_array[500:509]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0S2M7PagXsv"
   },
   "source": [
    "# Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QIf7aS5gfAGp"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import sys\n",
    "print('a')\n",
    "sys.exit()\n",
    "print('b')\n",
    "'''\n",
    "'''\n",
    "print(train_image_path)\n",
    "print(train_sino_path)\n",
    "print(test_image_path)\n",
    "print(test_sino_path)\n",
    "'''\n",
    "print(shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3anZSZqZgJs"
   },
   "source": [
    "# Notes:\n",
    "\n",
    "For high/low MSE experiments\n",
    "============================\n",
    "-Tuned networks for 180 minutes each.\n",
    "\n",
    "-Trained for 100 epochs using on-the-fly augmentation\n",
    "\n",
    "-See notes in checkpoint folder\n",
    "\n",
    "\n",
    "For LDM, window = 5, stride = 2\n",
    "===============================\n",
    "tune_max_t = 20            \n",
    "\n",
    "tune_minutes = 180      \n",
    "\n",
    "tune_display_step=12    \n",
    "\n",
    "tune_augment=False\n",
    "\n",
    "\n",
    "GPUs\n",
    "====\n",
    "From best to worst: V100, A100, T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AHpRBHQKzo9"
   },
   "source": [
    "Tensor board works for all experiments except the last one.\n",
    "My plotting function no longer works for any of the experiments."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNkGrkCIa3GG2reJ7rWhWOw",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1IKZAHG3MkebzxT6rbCXh6UyK1A_-pgjW",
     "timestamp": 1662946787019
    },
    {
     "file_id": "1rhTeO9VU1bsXALEA91XBWt5RmysWxoO_",
     "timestamp": 1657063751190
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
